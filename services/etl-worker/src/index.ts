/**
 * Clixer - ETL Worker
 * Background job processor for ETL operations
 * Ayrƒ± process - Ana uygulama etkilenmez!
 * 
 * MEMORY OPTƒ∞Mƒ∞ZASYONU:
 * - B√ºy√ºk veri setleri batch'ler halinde i≈ülenir
 * - Garbage collector manual tetikleme
 * - Memory limit kontrol
 */

import dotenv from 'dotenv';
dotenv.config({ path: '../../.env' });

// SSL sertifika kontrol√ºn√º devre dƒ±≈üƒ± bƒ±rak (self-signed veya expired sertifikalar i√ßin)
process.env.NODE_TLS_REJECT_UNAUTHORIZED = '0';

import {
  createLogger,
  db,
  clickhouse,
  cache
} from '@clixer/shared';

const logger = createLogger({ service: 'etl-worker' });

// Memory Optimizasyonu Sabitleri
const MAX_MEMORY_MB = 1024; // 1GB limit
const BATCH_SIZE = 5000; // K√º√ß√ºk batch'ler = daha az memory
const GC_INTERVAL = 10000; // Her 10 batch'te bir GC tetikle

// ============================================
// AKILLI TARƒ∞H D√ñN√ú≈ûT√úR√úC√ú
// Farklƒ± kaynaklardan gelen formatlarƒ± ClickHouse'a uygun hale getirir
// ============================================
function toClickHouseDateTime(val: any): string | null {
  if (val === null || val === undefined || val === '') return null;
  
  // Date objesi
  if (val instanceof Date) {
    if (isNaN(val.getTime())) return null;
    return val.toISOString().replace('T', ' ').replace('Z', '').split('.')[0];
  }
  
  // String deƒüer
  if (typeof val === 'string') {
    const str = val.trim();
    
    // 1. ISO 8601: 2025-12-13T20:33:42.722Z veya 2025-12-13T20:33:42
    if (str.match(/^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}/)) {
      return str.replace('T', ' ').replace('Z', '').split('.')[0];
    }
    
    // 2. ClickHouse formatƒ± zaten: 2025-12-13 20:33:42
    if (str.match(/^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}/)) {
      return str.split('.')[0]; // Milisaniye varsa at
    }
    
    // 3. Sadece tarih: 2025-12-13
    if (str.match(/^\d{4}-\d{2}-\d{2}$/)) {
      return str + ' 00:00:00';
    }
    
    // 4. Avrupa formatƒ±: DD-MM-YYYY veya DD/MM/YYYY (g√ºn > 12 ise kesin bu)
    const euMatch = str.match(/^(\d{2})[-\/](\d{2})[-\/](\d{4})(?:\s+(\d{2}:\d{2}:\d{2}))?/);
    if (euMatch) {
      const day = parseInt(euMatch[1]);
      const month = parseInt(euMatch[2]);
      const year = euMatch[3];
      const time = euMatch[4] || '00:00:00';
      
      // G√ºn 12'den b√ºy√ºkse kesinlikle DD-MM-YYYY
      if (day > 12 && month <= 12) {
        return `${year}-${euMatch[2]}-${euMatch[1]} ${time}`;
      }
      // Ay 12'den b√ºy√ºkse kesinlikle MM-DD-YYYY (ABD)
      if (month > 12 && day <= 12) {
        return `${year}-${euMatch[1]}-${euMatch[2]} ${time}`;
      }
      // ƒ∞kisi de <=12 ise varsayƒ±lan olarak DD-MM-YYYY kabul et (T√ºrkiye i√ßin)
      return `${year}-${euMatch[2]}-${euMatch[1]} ${time}`;
    }
    
    // 5. ABD formatƒ± a√ßƒ±k: MM/DD/YYYY
    const usMatch = str.match(/^(\d{1,2})\/(\d{1,2})\/(\d{4})(?:\s+(\d{2}:\d{2}:\d{2}))?/);
    if (usMatch) {
      const month = usMatch[1].padStart(2, '0');
      const day = usMatch[2].padStart(2, '0');
      const year = usMatch[3];
      const time = usMatch[4] || '00:00:00';
      return `${year}-${month}-${day} ${time}`;
    }
    
    // 6. PostgreSQL timestamp without time zone: 2025-12-13 20:33:42.123456
    if (str.match(/^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+/)) {
      return str.split('.')[0];
    }
    
    // 7. Son √ßare: Date.parse dene
    const parsed = new Date(str);
    if (!isNaN(parsed.getTime())) {
      return parsed.toISOString().replace('T', ' ').replace('Z', '').split('.')[0];
    }
    
    // Tanƒ±namadƒ± - null d√∂n, log at
    logger.warn('Unknown date format, returning NULL', { value: val });
    return null;
  }
  
  // Number (timestamp)
  if (typeof val === 'number') {
    const date = new Date(val);
    if (!isNaN(date.getTime())) {
      return date.toISOString().replace('T', ' ').replace('Z', '').split('.')[0];
    }
  }
  
  return null;
}

// ============================================
// KAPSAMLI TARƒ∞H FORMAT D√ñN√ú≈ûT√úR√úC√ú
// T√ºm SQL tarih formatlarƒ±nƒ± ClickHouse DateTime'a √ßevirir
// ============================================

/**
 * T√úM SQL TARƒ∞H FORMATLARINI ClickHouse DateTime'a √ßevirir
 * 
 * Desteklenen formatlar:
 * - ISO 8601: 2025-12-23T16:00:00.000Z, 2025-12-23T16:00:00Z
 * - ISO with offset: 2025-12-23T16:00:00+03:00
 * - SQL Server: 2025-12-23 16:00:00.0000000
 * - MySQL: 2025-12-23 16:00:00
 * - PostgreSQL: 2025-12-23 16:00:00.123456
 * - Date only: 2025-12-23
 * - European: 23/12/2025, 23.12.2025
 * - US: 12/23/2025
 * - JavaScript Date object
 * - Unix timestamp (number)
 * 
 * ClickHouse √ßƒ±ktƒ± formatƒ±: 'YYYY-MM-DD HH:mm:ss'
 */
function convertToClickHouseDateTime(value: any): string | null {
  if (value === null || value === undefined || value === '') {
    return null;
  }
  
  // 1. JavaScript Date objesi
  if (value instanceof Date) {
    if (isNaN(value.getTime())) return null;
    const pad = (n: number) => n.toString().padStart(2, '0');
    return `${value.getFullYear()}-${pad(value.getMonth() + 1)}-${pad(value.getDate())} ${pad(value.getHours())}:${pad(value.getMinutes())}:${pad(value.getSeconds())}`;
  }
  
  // 2. Unix timestamp (number - milliseconds)
  if (typeof value === 'number') {
    const date = new Date(value);
    if (!isNaN(date.getTime())) {
      const pad = (n: number) => n.toString().padStart(2, '0');
      return `${date.getFullYear()}-${pad(date.getMonth() + 1)}-${pad(date.getDate())} ${pad(date.getHours())}:${pad(date.getMinutes())}:${pad(date.getSeconds())}`;
    }
    return null;
  }
  
  // 3. String formatlarƒ±
  if (typeof value === 'string') {
    const val = value.trim();
    
    // A. ISO 8601 with T and optional timezone: 2025-12-23T16:00:00.000Z
    let match = val.match(/^(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):(\d{2})(?:\.\d+)?(?:Z|[+-]\d{2}:?\d{2})?$/);
    if (match) {
      return `${match[1]}-${match[2]}-${match[3]} ${match[4]}:${match[5]}:${match[6]}`;
    }
    
    // B. SQL Server format: 2025-12-23 16:00:00.0000000
    match = val.match(/^(\d{4})-(\d{2})-(\d{2})\s+(\d{2}):(\d{2}):(\d{2})(?:\.\d+)?$/);
    if (match) {
      return `${match[1]}-${match[2]}-${match[3]} ${match[4]}:${match[5]}:${match[6]}`;
    }
    
    // C. MySQL/PostgreSQL: 2025-12-23 16:00:00
    match = val.match(/^(\d{4})-(\d{2})-(\d{2})\s+(\d{2}):(\d{2}):(\d{2})$/);
    if (match) {
      return val; // Zaten doƒüru format
    }
    
    // D. Date only: 2025-12-23
    match = val.match(/^(\d{4})-(\d{2})-(\d{2})$/);
    if (match) {
      return `${val} 00:00:00`;
    }
    
    // E. European format: 23/12/2025 veya 23.12.2025
    match = val.match(/^(\d{2})[\/\.](\d{2})[\/\.](\d{4})(?:\s+(\d{2}):(\d{2}):(\d{2}))?$/);
    if (match) {
      const time = match[4] ? ` ${match[4]}:${match[5]}:${match[6]}` : ' 00:00:00';
      return `${match[3]}-${match[2]}-${match[1]}${time}`;
    }
    
    // F. US format: 12/23/2025 (MM/DD/YYYY)
    match = val.match(/^(\d{2})\/(\d{2})\/(\d{4})(?:\s+(\d{2}):(\d{2}):(\d{2}))?$/);
    if (match) {
      const time = match[4] ? ` ${match[4]}:${match[5]}:${match[6]}` : ' 00:00:00';
      return `${match[3]}-${match[1]}-${match[2]}${time}`;
    }
    
    // G. YYYYMMDD format: 20251223
    match = val.match(/^(\d{4})(\d{2})(\d{2})$/);
    if (match) {
      return `${match[1]}-${match[2]}-${match[3]} 00:00:00`;
    }
    
    // H. SQL Server short: Dec 23 2025 4:00PM
    match = val.match(/^([A-Za-z]{3})\s+(\d{1,2})\s+(\d{4})\s+(\d{1,2}):(\d{2})([AP]M)?$/i);
    if (match) {
      const months: Record<string, string> = { jan: '01', feb: '02', mar: '03', apr: '04', may: '05', jun: '06', jul: '07', aug: '08', sep: '09', oct: '10', nov: '11', dec: '12' };
      const mon = months[match[1].toLowerCase()] || '01';
      let hour = parseInt(match[4]);
      if (match[6]?.toUpperCase() === 'PM' && hour < 12) hour += 12;
      if (match[6]?.toUpperCase() === 'AM' && hour === 12) hour = 0;
      return `${match[3]}-${mon}-${match[2].padStart(2, '0')} ${hour.toString().padStart(2, '0')}:${match[5]}:00`;
    }
    
    // Tanƒ±namadƒ± - null d√∂n
    logger.warn('Unknown date format, returning null', { value: val });
    return null;
  }
  
  return null;
}

/**
 * Bir deƒüerin tarih olup olmadƒ±ƒüƒ±nƒ± kontrol et
 */
function isDateLikeValue(value: any): boolean {
  if (value === null || value === undefined) return false;
  if (value instanceof Date) return true;
  if (typeof value !== 'string') return false;
  
  const val = value.trim();
  
  // ISO format check
  if (/^\d{4}-\d{2}-\d{2}[T\s]/.test(val)) return true;
  // Date only
  if (/^\d{4}-\d{2}-\d{2}$/.test(val)) return true;
  // European/US format
  if (/^\d{2}[\/\.]\d{2}[\/\.]\d{4}/.test(val)) return true;
  
  return false;
}

/**
 * Veri satƒ±rƒ±nƒ± ClickHouse i√ßin d√∂n√º≈üt√ºr
 * T√ºm tarih benzeri deƒüerleri ClickHouse DateTime formatƒ±na √ßevirir
 */
function transformRowForClickHouse(row: any): any {
  const transformed: any = {};
  
  for (const key in row) {
    const value = row[key];
    
    if (value === null || value === undefined) {
      transformed[key] = null;
      continue;
    }
    
    // Date objesi ise d√∂n√º≈üt√ºr
    if (value instanceof Date) {
      transformed[key] = convertToClickHouseDateTime(value);
      continue;
    }
    
    // Tarih benzeri string ise d√∂n√º≈üt√ºr
    if (typeof value === 'string' && isDateLikeValue(value)) {
      const converted = convertToClickHouseDateTime(value);
      transformed[key] = converted !== null ? converted : value;
      continue;
    }
    
    // Diƒüer deƒüerler olduƒüu gibi
    transformed[key] = value;
  }
  
  return transformed;
}

/**
 * Veri batch'ini ClickHouse i√ßin d√∂n√º≈üt√ºr
 */
function transformBatchForClickHouse(rows: any[]): any[] {
  return rows.map(row => transformRowForClickHouse(row));
}

// ============================================
// TABLO YOKSA OLU≈ûTUR (SELF-HEALING)
// ============================================
// KAPSAMLI Tƒ∞P D√ñN√ú≈û√úM HARƒ∞TASI
// PostgreSQL/MySQL/MSSQL/Oracle ‚Üí ClickHouse
// ============================================
const SQL_TO_CLICKHOUSE_TYPE: Record<string, string> = {
  // ============ INTEGER TYPES ============
  'int': 'Int32', 'int4': 'Int32', 'integer': 'Int32',
  'int2': 'Int16', 'smallint': 'Int16',
  'int8': 'Int64', 'bigint': 'Int64',
  'serial': 'Int32', 'bigserial': 'Int64', 'smallserial': 'Int16',
  'oid': 'UInt32', 'tinyint': 'Int8', 'mediumint': 'Int32', 'year': 'Int16',
  'number': 'Float64', 'pls_integer': 'Int32', 'binary_integer': 'Int32',
  
  // ============ FLOAT TYPES ============
  'float': 'Float64', 'float4': 'Float32', 'float8': 'Float64',
  'real': 'Float32', 'double': 'Float64', 'double precision': 'Float64',
  'decimal': 'Float64', 'numeric': 'Float64', 'money': 'Float64',
  'newdecimal': 'Float64', 'smallmoney': 'Float64',
  'binary_float': 'Float32', 'binary_double': 'Float64',
  
  // ============ STRING TYPES ============
  'text': 'String', 'varchar': 'String', 'char': 'String',
  'character varying': 'String', 'character': 'String', 'bpchar': 'String',
  'name': 'String', 'uuid': 'String', 'json': 'String', 'jsonb': 'String',
  'xml': 'String', 'citext': 'String', 'inet': 'String', 'cidr': 'String', 'macaddr': 'String',
  'tinytext': 'String', 'mediumtext': 'String', 'longtext': 'String',
  'enum': 'String', 'set': 'String',
  'nvarchar': 'String', 'nchar': 'String', 'ntext': 'String',
  'uniqueidentifier': 'String', 'sql_variant': 'String', 'sysname': 'String',
  'varchar2': 'String', 'nvarchar2': 'String', 'clob': 'String', 'nclob': 'String',
  'long': 'String', 'rowid': 'String',
  
  // ============ DATE/TIME TYPES ============
  'date': 'Date', 'time': 'String', 'timetz': 'String', 'interval': 'String',
  'timestamp': 'DateTime', 'timestamptz': 'DateTime',
  'timestamp without time zone': 'DateTime', 'timestamp with time zone': 'DateTime',
  'datetime': 'DateTime', 'newdate': 'Date',
  'datetime2': 'DateTime', 'smalldatetime': 'DateTime', 'datetimeoffset': 'DateTime',
  'timestamp with local time zone': 'DateTime',
  
  // ============ BOOLEAN TYPES ============
  'boolean': 'UInt8', 'bool': 'UInt8', 'bit': 'UInt8',
  
  // ============ BINARY TYPES ============
  'bytea': 'String', 'blob': 'String', 'tinyblob': 'String', 
  'mediumblob': 'String', 'longblob': 'String',
  'binary': 'String', 'varbinary': 'String', 'image': 'String',
  'raw': 'String', 'long raw': 'String', 'bfile': 'String',
  
  // ============ GEOMETRY TYPES ============
  'geometry': 'String', 'geography': 'String', 'point': 'String',
  'linestring': 'String', 'polygon': 'String',
};

function mapSourceTypeToClickHouse(sourceType: string): string {
  if (!sourceType) return 'String';
  const normalized = sourceType.toLowerCase().replace(/\s+/g, ' ').trim();
  
  // Parantez i√ßindeki deƒüerleri temizle: varchar(255) ‚Üí varchar
  const baseType = normalized.split('(')[0].trim();
  
  return SQL_TO_CLICKHOUSE_TYPE[baseType] || SQL_TO_CLICKHOUSE_TYPE[normalized] || 'String';
}

// ============================================
// UI'dan sync tetiklendiƒüinde tablo yoksa otomatik olu≈üturur
// ============================================
async function ensureTableExists(dataset: any): Promise<void> {
  const tableName = dataset.clickhouse_table;
  
  try {
    // Tablo var mƒ± kontrol et
    const result = await clickhouse.query(`
      SELECT count() as cnt FROM system.tables 
      WHERE database = 'clixer_analytics' AND name = '${tableName}'
    `);
    
    const tableExists = result && result[0] && parseInt(result[0].cnt) > 0;
    
    if (tableExists) {
      logger.debug('Table already exists', { tableName });
      return;
    }
    
    logger.info('Table does not exist, creating automatically', { tableName, datasetId: dataset.id });
    
    // column_mapping'den tablo yapƒ±sƒ±nƒ± olu≈ütur
    const columnMapping = dataset.column_mapping || [];
    
    if (columnMapping.length === 0) {
      throw new Error(`Dataset ${dataset.name} i√ßin column_mapping bulunamadƒ±. Tablo olu≈üturulamƒ±yor.`);
    }
    
    // Kolonlarƒ± olu≈ütur - KAPSAMLI Tƒ∞P D√ñN√ú≈û√úM
    const columns = columnMapping.map((col: any) => {
      const targetName = col.target || col.source;
      
      // 1. √ñnce UI'dan gelen clickhouseType'ƒ± kullan
      let chType = col.clickhouseType;
      
      // 2. clickhouseType yoksa, kaynak tipten d√∂n√º≈üt√ºr
      if (!chType || chType === 'String') {
        const sourceType = col.sourceType || col.type || '';
        chType = mapSourceTypeToClickHouse(sourceType);
      }
      
      // 3. Decimal tipi Float64 olarak sakla (SUM, AVG √ßalƒ±≈üsƒ±n!)
      if (chType && chType.startsWith('Decimal')) {
        chType = 'Float64';
      }
      
      // 4. Sayƒ±sal kolonlarƒ± algƒ±la (kolon adƒ±ndan - fallback)
      const numericPatterns = [
        'amount', 'tutar', 'price', 'fiyat', 'total', 'toplam', 
        'adet', 'quantity', 'miktar', 'indirim', 'discount', 
        'brut', 'net', 'gross', 'ucret', 'maas', 'salary', 'fee',
        'cost', 'maliyet', 'borc', 'alacak', 'bakiye', 'balance'
      ];
      if (chType === 'String' && numericPatterns.some(p => targetName.toLowerCase().includes(p))) {
        chType = 'Float64';
        logger.debug('Auto-detected numeric column from name', { column: targetName, type: chType });
      }
      
      return `${targetName} ${chType}`;
    });
    
    // ORDER BY i√ßin uygun kolon bul
    const orderByCol = columnMapping.find((c: any) => {
      const name = c.target || c.source;
      return name === 'id' || name === 'kod' || name === 'code' ||
        c.clickhouseType === 'Date' || c.clickhouseType === 'DateTime';
    });
    const orderByColumn = orderByCol ? (orderByCol.target || orderByCol.source) : '_synced_at';
    
    const createSql = `
      CREATE TABLE IF NOT EXISTS clixer_analytics.${tableName} (
        ${columns.join(',\n        ')},
        _synced_at DateTime DEFAULT now()
      )
      ENGINE = MergeTree()
      ORDER BY (${orderByColumn})
    `;
    
    logger.info('Auto-creating ClickHouse table', { tableName, sql: createSql.substring(0, 500) });
    await clickhouse.execute(createSql);
    logger.info('Table created successfully', { tableName });
    
  } catch (error: any) {
    logger.error('Failed to ensure table exists', { tableName, error: error.message });
    throw new Error(`ClickHouse tablo olu≈üturulamadƒ±: ${error.message}`);
  }
}

// ============================================
// SQL SORGUDAN TABLO ADI √áIKARICI
// ============================================
function extractTableFromQuery(query: string): string | null {
  if (!query) return null;
  
  // MSSQL formatƒ±: [schema].[table] veya [table]
  // Standard format: schema.table veya table
  
  // √ñnce MSSQL k√∂≈üeli parantez formatƒ±nƒ± dene: [dbo].[transaction_items]
  const mssqlMatch = query.match(/\bFROM\s+\[?([a-zA-Z_][a-zA-Z0-9_]*)\]?\.\[?([a-zA-Z_][a-zA-Z0-9_]*)\]?/i);
  if (mssqlMatch) {
    // schema.table formatƒ±nda d√∂nd√ºr (k√∂≈üeli parantezler olmadan)
    return `${mssqlMatch[1]}.${mssqlMatch[2]}`;
  }
  
  // Sadece tablo adƒ± (k√∂≈üeli parantezli veya deƒüil): [transaction_items] veya transaction_items
  const tableMatch = query.match(/\bFROM\s+\[?([a-zA-Z_][a-zA-Z0-9_]*)\]?(?:\s|$|;|,|\))/i);
  if (tableMatch) {
    return tableMatch[1];
  }
  
  return null;
}

// ============================================
// Tƒ∞P UYUMLULUK KONTROL√ú (MUST!)
// ClickHouse tablo tipleri ile kaynak DB tipleri UYUMLU OLMAK ZORUNDA
// ============================================

interface TypeMismatch {
  column: string;
  sourceType: string;
  clickhouseType: string;
  compatible: boolean;
}

/**
 * SQL tipini ClickHouse tipine d√∂n√º≈üt√ºr
 */
function sqlToClickHouseType(sqlType: string): string {
  if (!sqlType) return 'String';
  
  const normalized = sqlType.toLowerCase().trim();
  
  const typeMap: Record<string, string> = {
    // Integer types
    'int': 'Int32', 'int4': 'Int32', 'integer': 'Int32',
    'int2': 'Int16', 'smallint': 'Int16',
    'int8': 'Int64', 'bigint': 'Int64',
    'tinyint': 'Int8',
    'serial': 'Int32', 'bigserial': 'Int64',
    
    // Float types
    'float': 'Float64', 'float4': 'Float32', 'float8': 'Float64',
    'real': 'Float32', 'double': 'Float64', 'double precision': 'Float64',
    'decimal': 'Float64', 'numeric': 'Float64', 'money': 'Float64',
    
    // String types
    'text': 'String', 'varchar': 'String', 'char': 'String',
    'nvarchar': 'String', 'nchar': 'String', 'ntext': 'String',
    'uuid': 'String', 'uniqueidentifier': 'String',
    
    // Date/Time types
    'date': 'Date',
    'datetime': 'DateTime', 'datetime2': 'DateTime',
    'timestamp': 'DateTime', 'timestamptz': 'DateTime',
    
    // Boolean
    'boolean': 'UInt8', 'bool': 'UInt8', 'bit': 'UInt8',
  };
  
  // Parantez i√ßini kaldƒ±r (varchar(255) -> varchar)
  const baseType = normalized.replace(/\(.*\)/, '').trim();
  
  return typeMap[baseType] || typeMap[normalized] || 'String';
}

/**
 * ƒ∞ki ClickHouse tipi uyumlu mu kontrol et
 */
function areTypesCompatible(expected: string, actual: string): boolean {
  if (expected === actual) return true;
  
  // Int32 ‚Üî Int64 uyumlu (upcast)
  if (expected.startsWith('Int') && actual.startsWith('Int')) return true;
  
  // Float32 ‚Üî Float64 uyumlu
  if (expected.startsWith('Float') && actual.startsWith('Float')) return true;
  
  // String her ≈üeyle uyumlu (en geni≈ü tip)
  if (actual === 'String') return true;
  
  return false;
}

/**
 * Kaynak tablo ve ClickHouse tablo tiplerini kar≈üƒ±la≈ütƒ±r
 * MUST: Sync ba≈ülamadan √∂nce √ßaƒürƒ±lmalƒ±!
 */
async function validateTypeCompatibility(
  dataset: any, 
  connection: any
): Promise<{ valid: boolean; mismatches: TypeMismatch[]; warning: string | null }> {
  const mismatches: TypeMismatch[] = [];
  
  try {
    // 1. Kaynak tablo tiplerini al (column_mapping'den)
    const columnMapping = typeof dataset.column_mapping === 'string' 
      ? JSON.parse(dataset.column_mapping) 
      : dataset.column_mapping;
    
    if (!columnMapping || columnMapping.length === 0) {
      logger.warn('No column mapping found, skipping type validation');
      return { valid: true, mismatches: [], warning: null };
    }
    
    // 2. ClickHouse tablo var mƒ±?
    if (!dataset.clickhouse_table) {
      return { valid: true, mismatches: [], warning: null };
    }
    
    // 3. ClickHouse tablo tiplerini al
    const chColumns = await clickhouse.query(`
      DESCRIBE TABLE clixer_analytics.${dataset.clickhouse_table}
    `);
    
    if (!chColumns || chColumns.length === 0) {
      return { valid: true, mismatches: [], warning: null };
    }
    
    const chTypeMap: Record<string, string> = {};
    for (const col of chColumns) {
      chTypeMap[col.name] = col.type;
    }
    
    // 4. Her kolon i√ßin tip kar≈üƒ±la≈ütƒ±r
    for (const col of columnMapping) {
      const colName = col.target || col.targetName || col.source || col.sourceName;
      const sqlType = col.sqlType || col.sourceType || col.type || '';
      const expectedChType = col.clickhouseType || sqlToClickHouseType(sqlType);
      const actualChType = chTypeMap[colName];
      
      if (actualChType && !areTypesCompatible(expectedChType, actualChType)) {
        mismatches.push({
          column: colName,
          sourceType: expectedChType,
          clickhouseType: actualChType,
          compatible: false
        });
      }
    }
    
    if (mismatches.length > 0) {
      const warning = `Tip uyumsuzluƒüu: ${mismatches.map(m => `${m.column}(${m.sourceType}‚Üí${m.clickhouseType})`).join(', ')}`;
      logger.error('üö® TYPE MISMATCH DETECTED!', { mismatches, dataset: dataset.name });
      return { valid: false, mismatches, warning };
    }
    
    return { valid: true, mismatches: [], warning: null };
    
  } catch (error: any) {
    logger.warn('Type validation failed, proceeding with caution', { error: error.message });
    return { valid: true, mismatches: [], warning: `Tip kontrol√º yapƒ±lamadƒ±: ${error.message}` };
  }
}

// ============================================
// VERƒ∞ TUTARLILIK VE DUPLICATE √ñNLEME Sƒ∞STEMƒ∞
// ============================================

interface DataValidationResult {
  sourceCount: number;
  targetCount: number;
  isConsistent: boolean;
  duplicateCount: number;
  message: string;
}

/**
 * ReplacingMergeTree i√ßin OPTIMIZE √ßalƒ±≈ütƒ±r - Duplicate'larƒ± temizler
 * CRITICAL: Her sync sonrasƒ± √ßaƒürƒ±lmalƒ±!
 */
async function optimizeTable(tableName: string): Promise<number> {
  try {
    // √ñnce duplicate sayƒ±sƒ±nƒ± bul
    const beforeCount = await clickhouse.queryOne(
      `SELECT count() as cnt FROM clixer_analytics.${tableName}`
    );
    
    // OPTIMIZE FINAL - ReplacingMergeTree duplicate'larƒ± temizler
    await clickhouse.execute(`OPTIMIZE TABLE clixer_analytics.${tableName} FINAL`);
    
    // Sonraki sayƒ±yƒ± al
    const afterCount = await clickhouse.queryOne(
      `SELECT count() as cnt FROM clixer_analytics.${tableName}`
    );
    
    const removedDuplicates = (beforeCount?.cnt || 0) - (afterCount?.cnt || 0);
    
    if (removedDuplicates > 0) {
      logger.info('Duplicates removed by OPTIMIZE', { 
        tableName, 
        before: beforeCount?.cnt, 
        after: afterCount?.cnt, 
        removed: removedDuplicates 
      });
    }
    
    return removedDuplicates;
  } catch (error: any) {
    logger.warn('OPTIMIZE failed (non-critical)', { tableName, error: error.message });
    return 0;
  }
}

/**
 * Veri tutarlƒ±lƒ±k kontrol√º
 * Kaynak ve hedef satƒ±r sayƒ±sƒ±nƒ± kar≈üƒ±la≈ütƒ±rƒ±r
 */
async function validateDataConsistency(
  dataset: any, 
  sourceClient: any, 
  expectedRows: number
): Promise<DataValidationResult> {
  try {
    // ClickHouse'daki satƒ±r sayƒ±sƒ±
    const targetResult = await clickhouse.queryOne(
      `SELECT count() as cnt FROM clixer_analytics.${dataset.clickhouse_table}`
    );
    const targetCount = targetResult?.cnt || 0;
    
    // Duplicate kontrol√º (ORDER BY key'e g√∂re grup sayƒ±sƒ±)
    let duplicateCount = 0;
    try {
      // ReplacingMergeTree i√ßin OPTIMIZE √ßalƒ±≈ütƒ±r
      duplicateCount = await optimizeTable(dataset.clickhouse_table);
    } catch (e) {
      // Ignore
    }
    
    // Tolerans: %1 fark kabul edilebilir (bazƒ± kayƒ±tlar filtrelenmi≈ü olabilir)
    const tolerance = Math.ceil(expectedRows * 0.01);
    const isConsistent = Math.abs(targetCount - expectedRows) <= tolerance;
    
    const result: DataValidationResult = {
      sourceCount: expectedRows,
      targetCount,
      isConsistent,
      duplicateCount,
      message: isConsistent 
        ? `‚úÖ Veri tutarlƒ±: ${targetCount} satƒ±r (beklenen: ${expectedRows})`
        : `‚ö†Ô∏è Veri uyumsuzluƒüu: ${targetCount} satƒ±r (beklenen: ${expectedRows}, fark: ${Math.abs(targetCount - expectedRows)})`
    };
    
    logger.info('Data validation completed', result);
    return result;
  } catch (error: any) {
    return {
      sourceCount: expectedRows,
      targetCount: 0,
      isConsistent: false,
      duplicateCount: 0,
      message: `‚ùå Doƒürulama hatasƒ±: ${error.message}`
    };
  }
}

/**
 * Partition bazlƒ± duplicate kontrol√º
 * Date partition'larda aynƒ± g√ºn√ºn verileri tekrar etmemeli
 */
async function checkPartitionDuplicates(
  tableName: string, 
  partitionColumn: string,
  dateValue: string
): Promise<number> {
  try {
    // Aynƒ± tarih i√ßin ka√ß satƒ±r var?
    const result = await clickhouse.queryOne(`
      SELECT count() as cnt 
      FROM clixer_analytics.${tableName} 
      WHERE toDate(${partitionColumn}) = '${dateValue}'
    `);
    return result?.cnt || 0;
  } catch (error) {
    return 0;
  }
}

// ============================================
// DATASET LOCK MEKANƒ∞ZMASI
// Aynƒ± dataset i√ßin aynƒ± anda sadece bir job √ßalƒ±≈üabilir!
// ============================================
const LOCK_TTL = 3600; // 1 saat lock timeout (sonsuz √ßalƒ±≈ümayƒ± √∂nler)

/**
 * Dataset i√ßin lock al
 * @returns true eƒüer lock alƒ±ndƒ±ysa, false eƒüer zaten kilitliyse
 */
async function acquireDatasetLock(datasetId: string): Promise<boolean> {
  const lockKey = `etl:lock:${datasetId}`;
  try {
    // SETNX equivalent - sadece key yoksa set et
    const result = await cache.setNX(lockKey, JSON.stringify({
      pid: process.pid,
      startedAt: new Date().toISOString()
    }), LOCK_TTL);
    
    if (result) {
      logger.info('Dataset lock acquired', { datasetId });
      return true;
    } else {
      // Lock zaten var - kim tarafƒ±ndan alƒ±nmƒ±≈ü kontrol et
      const existing = await cache.get(lockKey);
      logger.warn('Dataset already locked (duplicate job prevention)', { 
        datasetId, 
        existingLock: existing 
      });
      return false;
    }
  } catch (error) {
    logger.error('Failed to acquire dataset lock', { datasetId, error });
    return false;
  }
}

/**
 * Dataset lock'ƒ±nƒ± serbest bƒ±rak
 */
async function releaseDatasetLock(datasetId: string): Promise<void> {
  const lockKey = `etl:lock:${datasetId}`;
  try {
    await cache.del(lockKey);
    logger.info('Dataset lock released', { datasetId });
  } catch (error) {
    logger.error('Failed to release dataset lock', { datasetId, error });
  }
}

/**
 * Job iptal edildi mi kontrol et
 */
async function isJobCancelled(jobId: string): Promise<boolean> {
  const cancelKey = `etl:cancel:${jobId}`;
  const cancelled = await cache.get(cancelKey);
  return cancelled === 'true';
}

/**
 * Kill signal g√∂nder
 */
async function sendKillSignal(jobId: string): Promise<void> {
  const cancelKey = `etl:cancel:${jobId}`;
  await cache.set(cancelKey, 'true', 3600);
  logger.info('Kill signal sent', { jobId });
}

// Memory kullanƒ±mƒ±nƒ± kontrol et
function checkMemory(): { usedMB: number; ok: boolean } {
  const used = process.memoryUsage();
  const usedMB = Math.round(used.heapUsed / 1024 / 1024);
  const ok = usedMB < MAX_MEMORY_MB;
  if (!ok) {
    logger.warn('High memory usage detected', { usedMB, maxMB: MAX_MEMORY_MB });
  }
  return { usedMB, ok };
}

// Garbage collector'ƒ± manual tetikle (varsa)
function forceGC() {
  if (global.gc) {
    global.gc();
    logger.debug('Manual garbage collection triggered');
  }
}

// ============================================
// ETL JOB PROCESSOR
// ============================================

interface ETLJob {
  datasetId: string;
  jobId?: string;
  action: 'initial_sync' | 'incremental_sync' | 'full_refresh' | 'manual_sync' | 'partial_refresh' | 'missing_sync' | 'new_records_sync';
  triggeredBy?: string;
  days?: number; // Partial refresh i√ßin g√ºn sayƒ±sƒ±
  ranges?: Array<{start: number; end: number; missing_count?: number}>; // missing_sync i√ßin eksik ID aralƒ±klarƒ±
  pkColumn?: string; // ‚ö†Ô∏è KULLANICI SE√áTƒ∞ƒûƒ∞ PK KOLONU - hardcoded deƒüil!
  afterId?: number; // new_records_sync i√ßin: Bu ID'den sonraki kayƒ±tlarƒ± √ßek
  limit?: number; // Opsiyonel satƒ±r limiti
}

async function processETLJob(job: ETLJob): Promise<void> {
  const startTime = Date.now();
  logger.info('Starting ETL job', job);

  try {
    // Dataset bilgisini al
    const dataset = await db.queryOne(
      'SELECT * FROM datasets WHERE id = $1',
      [job.datasetId]
    );

    if (!dataset) {
      throw new Error(`Dataset not found: ${job.datasetId}`);
    }

    // Connection bilgisini al
    const connection = await db.queryOne(
      'SELECT * FROM data_connections WHERE id = $1',
      [dataset.connection_id]
    );

    if (!connection) {
      throw new Error(`Connection not found: ${dataset.connection_id}`);
    }

    // ============================================
    // MUST: Tƒ∞P UYUMLULUK KONTROL√ú
    // Sync ba≈ülamadan √∂nce kaynak ve hedef tipleri kar≈üƒ±la≈ütƒ±r
    // ============================================
    const typeValidation = await validateTypeCompatibility(dataset, connection);
    if (!typeValidation.valid) {
      const errorMsg = `Tƒ∞P UYUMSUZLUƒûU: ${typeValidation.warning}. Tabloyu silip yeniden olu≈üturun.`;
      logger.error('üö® Sync blocked due to type mismatch', { 
        dataset: dataset.name, 
        mismatches: typeValidation.mismatches 
      });
      
      // Job'ƒ± failed olarak i≈üaretle
      if (job.jobId) {
        await db.query(
          `UPDATE etl_jobs SET status = 'failed', completed_at = NOW(), error_message = $1 WHERE id = $2`,
          [errorMsg, job.jobId]
        );
      }
      throw new Error(errorMsg);
    }
    
    if (typeValidation.warning) {
      logger.warn('Type validation warning', { warning: typeValidation.warning });
    }

    // ETL job kaydƒ±: Eƒüer jobId varsa (data-service'ten geldiyse) onu kullan, yoksa yeni olu≈ütur
    let etlJobId: string;
    
    if (job.jobId) {
      // data-service zaten job olu≈üturdu, sadece running yap
      await db.query(
        `UPDATE etl_jobs SET status = 'running', started_at = NOW() WHERE id = $1`,
        [job.jobId]
      );
      etlJobId = job.jobId;
      logger.info('Using existing job from data-service', { jobId: job.jobId });
    } else {
      // Eski stil trigger veya initial_sync - yeni job olu≈ütur
      // Ama √∂nce duplicate kontrol√º yap
      const existingJob = await db.queryOne(
        `SELECT id FROM etl_jobs WHERE dataset_id = $1 AND status IN ('pending', 'running') LIMIT 1`,
        [job.datasetId]
      );
      
      if (existingJob) {
        logger.warn('Duplicate job prevented in ETL Worker', { datasetId: job.datasetId, existingJobId: existingJob.id });
        return; // Duplicate job, atla
      }
      
      const etlJob = await db.queryOne(
        `INSERT INTO etl_jobs (tenant_id, dataset_id, action, status, started_at)
         VALUES ($1, $2, $3, 'running', NOW())
         RETURNING id`,
        [dataset.tenant_id, job.datasetId, job.action]
      );
      etlJobId = etlJob.id;
    }
    
    const etlJob = { id: etlJobId };

    let rowsProcessed = 0;

    try {
      // ============================================
      // INITIAL_SYNC - ƒ∞lk olu≈üturmada SADECE test verisi yaz!
      // LIMIT'i KORUYARAK source_query'yi √ßalƒ±≈ütƒ±r (10 satƒ±r)
      // Bu sayede ID-Based/Time-Based sync i√ßin referans noktasƒ± olur
      // ============================================
      if (job.action === 'initial_sync') {
        logger.info('üß™ INITIAL SYNC - Test verisi yazƒ±lƒ±yor (LIMIT korunuyor)', { 
          datasetId: job.datasetId,
          sourceQuery: dataset.source_query?.substring(0, 100)
        });
        rowsProcessed = await initialTestSync(dataset, connection, etlJobId);
      }
      // ============================================
      // PARTIAL REFRESH - √ñzel aksiyon (UI'dan gelen)
      // ============================================
      else if (job.action === 'partial_refresh') {
        logger.info('Executing partial refresh', { 
          datasetId: job.datasetId, 
          days: dataset.refresh_window_days,
          partitionColumn: dataset.partition_column 
        });
        rowsProcessed = await syncByDatePartition(dataset, connection, etlJobId);
      }
      // ============================================
      // MISSING SYNC - Sadece eksik ID aralƒ±klarƒ±nƒ± √ßek
      // ============================================
      else if (job.action === 'missing_sync') {
        logger.info('üîç MISSING SYNC - Eksik ID aralƒ±klarƒ± √ßekiliyor', { 
          datasetId: job.datasetId,
          rangesCount: job.ranges?.length || 0,
          pkColumn: job.pkColumn || 'id'
        });
        rowsProcessed = await syncMissingRanges(dataset, connection, etlJobId, job.ranges || [], job.pkColumn || 'id');
      }
      // ============================================
      // üöÄ NEW RECORDS SYNC - Max ID'den sonraki yeni kayƒ±tlarƒ± √ßek
      // 100M+ tablolar i√ßin en verimli y√∂ntem!
      // ============================================
      else if (job.action === 'new_records_sync') {
        const pkColumn = job.pkColumn || 'id';
        const afterId = job.afterId || 0;
        const limit = job.limit;
        
        logger.info('üöÄ NEW RECORDS SYNC - Sadece yeni kayƒ±tlar √ßekiliyor', { 
          datasetId: job.datasetId,
          pkColumn,
          afterId,
          limit: limit || 'UNLIMITED'
        });
        
        rowsProcessed = await syncNewRecordsAfterMaxId(dataset, connection, etlJobId, pkColumn, afterId, limit);
      } else {
        // Sync stratejisine g√∂re veri √ßek - jobId'yi ge√ßir progress i√ßin
        switch (dataset.sync_strategy) {
          case 'timestamp':
            rowsProcessed = await syncByTimestamp(dataset, connection, etlJobId);
            break;
          case 'id':
            rowsProcessed = await syncById(dataset, connection, etlJobId);
            break;
          case 'date_partition':
            rowsProcessed = await syncByDatePartition(dataset, connection, etlJobId);
            break;
          case 'date_delete_insert':
            rowsProcessed = await syncByDateDeleteInsert(dataset, connection, etlJobId);
            break;
          case 'full_refresh':
          default:
            rowsProcessed = await fullRefresh(dataset, connection, etlJobId);
        }
      }

      // Job ba≈üarƒ±lƒ±
      await db.query(
        `UPDATE etl_jobs SET status = 'completed', completed_at = NOW(), rows_processed = $1 WHERE id = $2`,
        [rowsProcessed, etlJob.id]
      );

      // Dataset son sync zamanƒ±nƒ± ve satƒ±r sayƒ±sƒ±nƒ± g√ºncelle
      // ClickHouse'dan ger√ßek satƒ±r sayƒ±sƒ±nƒ± al
      let totalRows = rowsProcessed;
      try {
        if (dataset.clickhouse_table) {
          const countResult = await clickhouse.query(`SELECT count() as cnt FROM clixer_analytics.${dataset.clickhouse_table}`);
          totalRows = countResult[0]?.cnt || rowsProcessed;
        }
      } catch (e) {
        logger.warn('Could not get total rows from ClickHouse', { error: e });
      }
      
      await db.query(
        `UPDATE datasets SET last_sync_at = NOW(), last_sync_rows = $1, total_rows = $2, status = 'active' WHERE id = $3`,
        [rowsProcessed, totalRows, job.datasetId]
      );

      // Cache invalidate
      await cache.invalidate(`kpi:${dataset.clickhouse_table}:*`, 'etl');

      // Event yayƒ±nla
      await cache.publish('etl:completed', {
        datasetId: job.datasetId,
        rowsProcessed,
        duration: Date.now() - startTime
      });

      logger.info('ETL job completed', {
        datasetId: job.datasetId,
        rowsProcessed,
        duration: `${Date.now() - startTime}ms`
      });

    } catch (error: any) {
      // Job ba≈üarƒ±sƒ±z
      await db.query(
        `UPDATE etl_jobs SET status = 'failed', completed_at = NOW(), error_message = $1 WHERE id = $2`,
        [error.message, etlJob.id]
      );
      throw error;
    }

  } catch (error) {
    logger.error('ETL job failed', { job, error });
    throw error;
  }
}

// ============================================
// STREAMING POSTGRESQL SYNC
// 200M+ satƒ±r sorunsuz i≈ülenir - bellek sabit kalƒ±r
// ============================================

const STREAM_BATCH_SIZE = 10000; // Cursor'dan bir seferde okunacak satƒ±r
const INSERT_BATCH_SIZE = 5000;  // ClickHouse'a yazƒ±lacak batch boyutu

async function streamingPostgreSQLSync(
  dataset: any, 
  connection: any, 
  columnMapping: any[],
  jobId?: string
): Promise<number> {
  const Cursor = require('pg-cursor');
  const { Client } = require('pg');
  
  const client = new Client({
    host: connection.host,
    port: connection.port || 5432,
    database: connection.database_name,
    user: connection.username,
    password: connection.password_encrypted,
  });
  
  await client.connect();
  
  // Source query olu≈ütur - LIMIT YOK! Streaming ile hepsini alacaƒüƒ±z
  let query = dataset.source_query || `SELECT * FROM ${dataset.source_table}`;
  
  // ‚ùó KRITIK: Dataset olu≈üturulurken LIMIT 10 ile test edilmi≈ü olabilir!
  // Full Refresh'te T√úM veriyi √ßekmek i√ßin LIMIT'i kaldƒ±r
  query = query.replace(/\s+LIMIT\s+\d+\s*/gi, ' ').trim();
  
  // Custom WHERE ko≈üulu varsa ekle (Full Refresh i√ßin kullanƒ±cƒ± tanƒ±mlƒ± filtre)
  const customWhere = dataset.custom_where;
  if (customWhere && customWhere.trim()) {
    // Sorgunun zaten WHERE i√ßerip i√ßermediƒüini kontrol et
    if (query.toUpperCase().includes(' WHERE ')) {
      // Varsa AND ile ekle
      query = `${query} AND (${customWhere})`;
    } else {
      // Yoksa WHERE ekle
      query = `${query} WHERE ${customWhere}`;
    }
    logger.info('Custom WHERE applied', { customWhere });
  }
  
  // Ama row_limit varsa ona uyalƒ±m
  const rowLimit = dataset.row_limit || null; // null = sƒ±nƒ±rsƒ±z
  
  logger.info('üöÄ STREAMING ETL starting', { 
    datasetId: dataset.id,
    table: dataset.clickhouse_table,
    query: query.substring(0, 100),
    customWhere: customWhere || 'none',
    rowLimit: rowLimit || 'UNLIMITED',
    streamBatchSize: STREAM_BATCH_SIZE
  });
  
  // Cursor olu≈ütur
  const cursor = client.query(new Cursor(query));
  
  let totalInserted = 0;
  let batchNumber = 0;
  let insertBuffer: any[] = [];
  let columns: string[] = [];
  
  // ClickHouse tablosunu truncate et
  try {
    await clickhouse.execute(`TRUNCATE TABLE clixer_analytics.${dataset.clickhouse_table}`);
    logger.info('Truncated ClickHouse table', { table: dataset.clickhouse_table });
  } catch (truncErr: any) {
    logger.warn('Could not truncate, trying ALTER DELETE', { error: truncErr.message });
    await clickhouse.execute(`ALTER TABLE clixer_analytics.${dataset.clickhouse_table} DELETE WHERE 1=1`);
  }
  
  // Streaming okuma d√∂ng√ºs√º
  const readBatch = (): Promise<any[]> => {
    return new Promise((resolve, reject) => {
      cursor.read(STREAM_BATCH_SIZE, (err: any, rows: any[]) => {
        if (err) reject(err);
        else resolve(rows);
      });
    });
  };
  
  try {
    while (true) {
      // Kill check
      if (jobId && await isJobCancelled(jobId)) {
        logger.info('Job cancelled during streaming', { jobId, totalInserted });
        break;
      }
      
      // Batch oku
      const rows = await readBatch();
      
      if (rows.length === 0) {
        // Veri bitti
        break;
      }
      
      batchNumber++;
      
      // ƒ∞lk batch'te column mapping olu≈ütur (eƒüer yoksa)
      if (batchNumber === 1 && columnMapping.length === 0 && rows.length > 0) {
        const firstRow = rows[0];
        for (const key of Object.keys(firstRow)) {
          const value = firstRow[key];
          let clickhouseType = 'String';
          if (typeof value === 'number') {
            clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
          } else if (typeof value === 'boolean') {
            clickhouseType = 'UInt8';
          }
          columnMapping.push({
            source: key,
            target: key.replace(/[^a-zA-Z0-9_]/g, '_'),
            clickhouseType
          });
        }
        columns = columnMapping.map((m: any) => m.target || m.targetName);
      }
      
      if (columns.length === 0) {
        columns = columnMapping.map((m: any) => m.target || m.targetName);
      }
      
      // Satƒ±rlarƒ± d√∂n√º≈üt√ºr ve buffer'a ekle
      for (const row of rows) {
        const transformed: any = {};
        for (const mapping of columnMapping) {
          const sourceCol = mapping.source || mapping.sourceName;
          const targetCol = mapping.target || mapping.targetName;
          let value = row[sourceCol];
          
          if (value === null || value === undefined) {
            if (mapping.clickhouseType?.includes('Int') || mapping.clickhouseType?.includes('Decimal')) {
              value = 0;
            } else if (mapping.clickhouseType === 'Date') {
              value = '1970-01-01';
            } else {
              value = '';
            }
          }
          
          transformed[targetCol] = value;
        }
        insertBuffer.push(transformed);
        
        // Buffer doluysa ClickHouse'a yaz
        if (insertBuffer.length >= INSERT_BATCH_SIZE) {
          await insertToClickHouse(dataset.clickhouse_table, columns, insertBuffer);
          totalInserted += insertBuffer.length;
          insertBuffer = []; // Buffer'ƒ± temizle - bellek serbest
          
          // Her 50K satƒ±rda progress log
          if (totalInserted % 50000 === 0) {
            const memMB = Math.round(process.memoryUsage().heapUsed / 1024 / 1024);
            logger.info('üìä Streaming progress', { 
              totalInserted: totalInserted.toLocaleString(),
              memoryMB: memMB,
              batchNumber
            });
            
            // GC tetikle
            forceGC();
          }
        }
        
        // Row limit kontrol√º
        if (rowLimit && totalInserted + insertBuffer.length >= rowLimit) {
          break;
        }
      }
      
      // Row limit kontrol√º (dƒ±≈ü d√∂ng√º i√ßin)
      if (rowLimit && totalInserted + insertBuffer.length >= rowLimit) {
        break;
      }
    }
    
    // Kalan buffer'ƒ± yaz
    if (insertBuffer.length > 0) {
      await insertToClickHouse(dataset.clickhouse_table, columns, insertBuffer);
      totalInserted += insertBuffer.length;
    }
    
  } finally {
    // Cursor ve client'ƒ± kapat
    await new Promise<void>((resolve) => cursor.close(() => resolve()));
    await client.end();
    forceGC();
  }
  
  const memMB = Math.round(process.memoryUsage().heapUsed / 1024 / 1024);
  logger.info('‚úÖ STREAMING ETL completed', { 
    datasetId: dataset.id, 
    totalInserted: totalInserted.toLocaleString(),
    finalMemoryMB: memMB
  });
  
  return totalInserted;
}

/**
 * ClickHouse'a batch insert
 * DateTime formatƒ±: YYYY-MM-DD HH:MM:SS
 * Akƒ±llƒ± tarih d√∂n√º≈üt√ºr√ºc√º ile t√ºm formatlar desteklenir
 */
async function insertToClickHouse(tableName: string, columns: string[], rows: any[]): Promise<void> {
  if (rows.length === 0) return;
  
  const values = rows.map(row => {
    const vals = columns.map((col: string) => {
      const val = row[col];
      
      // Tarih/DateTime kontrol√º - √∂nce akƒ±llƒ± d√∂n√º≈üt√ºr√ºc√ºy√º dene
      if (val instanceof Date || (typeof val === 'string' && val.match(/\d{4}[-\/]\d{2}[-\/]\d{2}|\d{2}[-\/]\d{2}[-\/]\d{4}/))) {
        const converted = toClickHouseDateTime(val);
        if (converted) return `'${converted}'`;
        return 'NULL';
      }
      
      if (typeof val === 'string') {
        return `'${val.replace(/\\/g, '\\\\').replace(/'/g, "''")}'`;
      } else if (val === null || val === undefined) {
        return 'NULL';
      }
      return val;
    });
    return `(${vals.join(', ')})`;
  }).join(',\n');
  
  const sql = `INSERT INTO clixer_analytics.${tableName} (${columns.join(', ')}) VALUES ${values}`;
  await clickhouse.execute(sql);
}

// ============================================
// INITIAL TEST SYNC - ƒ∞lk olu≈üturmada LIMIT'i KORUYARAK test verisi yaz
// Bu fonksiyon sadece dataset ilk olu≈üturulduƒüunda √ßalƒ±≈üƒ±r
// LIMIT 10 ile gelen sorguyu OLDUƒûU Gƒ∞Bƒ∞ √ßalƒ±≈ütƒ±rƒ±r
// B√∂ylece ID-Based/Time-Based sync i√ßin referans noktasƒ± olu≈üur
// ============================================
async function initialTestSync(dataset: any, connection: any, jobId?: string): Promise<number> {
  logger.info('üß™ Initial Test Sync starting - LIMIT korunuyor!', { 
    datasetId: dataset.id,
    table: dataset.clickhouse_table,
    connectionType: connection.type
  });
  
  try {
    // üîß SELF-HEALING: Tablo yoksa otomatik olu≈ütur
    await ensureTableExists(dataset);
    
    const columnMapping = dataset.column_mapping || [];
    
    // ‚ùó KRƒ∞Tƒ∞K: source_query'yi OLDUƒûU Gƒ∞Bƒ∞ kullan - LIMIT KALDIRMA!
    const query = dataset.source_query || `SELECT * FROM ${dataset.source_table} LIMIT 10`;
    
    logger.info('Initial test query (LIMIT korunuyor)', { query: query.substring(0, 200) });
    
    let rows: any[] = [];
    
    // Baƒülantƒ± tipine g√∂re veri √ßek
    if (connection.type === 'postgresql') {
      const { Client } = require('pg');
      const client = new Client({
        host: connection.host,
        port: connection.port || 5432,
        database: connection.database_name,
        user: connection.username,
        password: connection.password_encrypted,
      });
      await client.connect();
      const result = await client.query(query);
      rows = result.rows;
      await client.end();
    } 
    else if (connection.type === 'mysql') {
      const mysql = require('mysql2/promise');
      const conn = await mysql.createConnection({
        host: connection.host,
        port: connection.port || 3306,
        database: connection.database_name,
        user: connection.username,
        password: connection.password_encrypted,
        charset: 'utf8mb4',
        dateStrings: true
      });
      const [result] = await conn.query(query);
      rows = result as any[];
      await conn.end();
    }
    else if (connection.type === 'mssql') {
      const mssql = require('mssql');
      const isAzure = connection.host.includes('.database.windows.net');
      const pool = await mssql.connect({
        server: connection.host,
        port: connection.port || 1433,
        database: connection.database_name,
        user: connection.username,
        password: connection.password_encrypted,
        options: { encrypt: isAzure, trustServerCertificate: !isAzure }
      });
      const result = await pool.request().query(query);
      rows = result.recordset;
      await pool.close();
    }
    
    logger.info('Initial test data fetched', { rowCount: rows.length });
    
    if (rows.length === 0) {
      logger.warn('No test data fetched - table will be empty');
      return 0;
    }
    
    // ClickHouse'a yaz (TRUNCATE YOK - tablo zaten bo≈ü)
    const insertData = rows.map((row: any) => {
      const obj: any = {};
      for (const col of columnMapping) {
        const sourceCol = col.source || col.sourceName;
        const targetCol = col.target || col.targetName;
        let val = row[sourceCol];
        
        // DateTime d√∂n√º≈ü√ºm√º
        if (col.clickhouseType === 'DateTime' || col.clickhouseType === 'Date') {
          val = toClickHouseDateTime(val) || '1970-01-01 00:00:00';
        }
        
        // Null handling
        if (val === null || val === undefined) {
          if (col.clickhouseType === 'String') val = '';
          else if (col.clickhouseType?.includes('Int') || col.clickhouseType?.includes('Float')) val = 0;
          else val = '';
        }
        
        obj[targetCol] = val;
      }
      return obj;
    });
    
    await clickhouse.insert(`clixer_analytics.${dataset.clickhouse_table}`, insertData);
    
    logger.info('‚úÖ Initial test sync completed', { 
      datasetId: dataset.id,
      rowsInserted: rows.length 
    });
    
    return rows.length;
    
  } catch (error: any) {
    logger.error('Initial test sync failed', { 
      datasetId: dataset.id, 
      error: error.message 
    });
    throw error;
  }
}

// ============================================
// SYNC STRATEGIES
// ============================================

async function syncByTimestamp(dataset: any, connection: any, jobId?: string): Promise<number> {
  // üîß SELF-HEALING: Tablo yoksa otomatik olu≈ütur
  await ensureTableExists(dataset);
  
  const {
    reference_column,
    clickhouse_table,
    source_query,
    source_table,
    row_limit,
    last_sync_value
  } = dataset;

  logger.info('Timestamp-based sync starting', { 
    datasetId: dataset.id, 
    datasetName: dataset.name,
    referenceColumn: reference_column,
    lastSyncValue: last_sync_value,
    clickhouseTable: clickhouse_table
  });

  // Referans kolon kontrol√º
  if (!reference_column) {
    logger.warn('No reference column defined, falling back to full refresh', { datasetId: dataset.id });
    return await fullRefresh(dataset, connection, jobId);
  }

  // Source kontrol√º
  const sourceTableName = extractTableFromQuery(source_query || `SELECT * FROM ${source_table}`);
  if (!sourceTableName) {
    throw new Error('Kaynak tablo bulunamadƒ±');
  }

  try {
    // Son sync deƒüerini al (yoksa t√ºm veriyi √ßek)
    let lastValue = last_sync_value;
    
    // Eƒüer last_sync_value yoksa, ClickHouse'tan en son deƒüeri al
    if (!lastValue) {
      try {
        const maxResult = await clickhouse.query(`
          SELECT max(${reference_column}) as max_val 
          FROM clixer_analytics.${clickhouse_table}
        `);
        lastValue = maxResult[0]?.max_val || null;
        logger.info('Got max timestamp from ClickHouse', { lastValue });
      } catch (e) {
        lastValue = null;
      }
    }

    let totalInserted = 0;

    // column_mapping'i parse et
    let columnMapping = dataset.column_mapping || [];
    if (typeof columnMapping === 'string') {
      try {
        columnMapping = JSON.parse(columnMapping);
      } catch (e) {
        columnMapping = [];
      }
    }

    if (connection.type === 'mssql') {
      const columns = columnMapping.map((c: any) => c.source).join(', ') || '*';
      let whereClause = '';
      
      if (lastValue) {
        // MSSQL datetime formatƒ±
        whereClause = `WHERE ${reference_column} > '${lastValue}'`;
      }
      
      const limit = row_limit || 10000000;
      const query = `SELECT TOP ${limit} ${columns} FROM ${sourceTableName} ${whereClause} ORDER BY ${reference_column}`;
      
      logger.info('Executing MSSQL timestamp query', { 
        query: query.substring(0, 300),
        lastValue,
        limit
      });

      const originalQuery = dataset.source_query;
      dataset.source_query = query;
      try {
        totalInserted = await mssqlSync(dataset, connection, columnMapping, jobId);
      } finally {
        dataset.source_query = originalQuery;
      }
      
    } else if (connection.type === 'postgresql') {
      const columns = columnMapping.map((c: any) => c.source).join(', ') || '*';
      let whereClause = '';
      
      if (lastValue) {
        whereClause = `WHERE ${reference_column} > '${lastValue}'`;
      }
      
      const limit = row_limit || 10000000;
      const query = `SELECT ${columns} FROM ${sourceTableName} ${whereClause} ORDER BY ${reference_column} LIMIT ${limit}`;
      
      logger.info('Executing PostgreSQL timestamp query', { query: query.substring(0, 300) });
      
      totalInserted = await streamingPostgreSQLSync(dataset, connection, columnMapping, jobId);
      
    } else if (connection.type === 'mysql') {
      const columns = columnMapping.map((c: any) => c.source).join(', ') || '*';
      let whereClause = '';
      
      if (lastValue) {
        whereClause = `WHERE ${reference_column} > '${lastValue}'`;
      }
      
      const limit = row_limit || 10000000;
      const query = `SELECT ${columns} FROM ${sourceTableName} ${whereClause} ORDER BY ${reference_column} LIMIT ${limit}`;
      
      logger.info('Executing MySQL timestamp query', { query: query.substring(0, 300) });
      
      totalInserted = await mysqlSync(dataset, connection, columnMapping, jobId);
      
    } else {
      logger.warn('Unsupported connection type for timestamp sync, falling back to full refresh', { 
        type: connection.type 
      });
      return await fullRefresh(dataset, connection, jobId);
    }

    // Son sync deƒüerini g√ºncelle
    if (totalInserted > 0) {
      try {
        const newMaxResult = await clickhouse.query(`
          SELECT max(${reference_column}) as max_val 
          FROM clixer_analytics.${clickhouse_table}
        `);
        const newMaxValue = newMaxResult[0]?.max_val;
        
        if (newMaxValue) {
          await db.query(
            'UPDATE datasets SET last_sync_value = $1, last_sync_at = NOW() WHERE id = $2',
            [newMaxValue, dataset.id]
          );
          logger.info('Updated last_sync_value', { newMaxValue });
        }
      } catch (e: any) {
        logger.warn('Could not update last_sync_value', { error: e.message });
      }
    }

    // OPTIMIZE √ßalƒ±≈ütƒ±r
    await clickhouse.execute(`OPTIMIZE TABLE clixer_analytics.${clickhouse_table} FINAL`);
    
    logger.info('Timestamp-based sync completed', { 
      datasetId: dataset.id,
      totalInserted,
      lastValue
    });

    return totalInserted;
    
  } catch (error: any) {
    logger.error('Timestamp-based sync failed', { 
      datasetId: dataset.id,
      error: error.message 
    });
    throw error;
  }
}

async function syncById(dataset: any, connection: any, jobId?: string): Promise<number> {
  // üîß SELF-HEALING: Tablo yoksa otomatik olu≈ütur
  await ensureTableExists(dataset);
  
  const { clickhouse_table, source_table, source_query, reference_column, row_limit } = dataset;
  
  // Referans kolon kontrol√º
  if (!reference_column) {
    logger.warn('No reference column for ID-based sync, falling back to full refresh', { datasetId: dataset.id });
    return await fullRefresh(dataset, connection, jobId);
  }
  
  // Kaynak tablo veya sorgu kontrol√º
  const sourceTableName = source_table || (source_query ? extractTableFromQuery(source_query) : null);
  if (!sourceTableName) {
    logger.warn('No source table or query for ID-based sync, falling back to full refresh', { datasetId: dataset.id });
    return await fullRefresh(dataset, connection, jobId);
  }
  
  logger.info('ID-Based incremental sync starting', { 
    datasetId: dataset.id, 
    table: clickhouse_table,
    sourceTable: sourceTableName,
    referenceColumn: reference_column
  });
  
  try {
    // 1. ClickHouse'tan mevcut max ID'yi al
    // NOT: ID kolonu String olabilir, bu y√ºzden toInt64OrZero ile sayƒ±ya √ßeviriyoruz!
    let maxId = 0;
    try {
      const maxResult = await clickhouse.queryOne(`
        SELECT max(toInt64OrZero(toString(${reference_column}))) as max_id FROM clixer_analytics.${clickhouse_table}
      `);
      maxId = parseInt(maxResult?.max_id || '0') || 0;
      logger.info('Current max ID in ClickHouse (converted to Int64)', { maxId, table: clickhouse_table });
    } catch (e: any) {
      logger.warn('Could not get max ID, will fetch all', { error: e.message });
    }
    
    // 2. Kaynaktan sadece yeni kayƒ±tlarƒ± √ßek (WHERE id > maxId)
    // ‚ö†Ô∏è row_limit string olarak gelebilir, integer'a √ßevir!
    const limit = parseInt(String(row_limit)) || 10000000;
    let totalInserted = 0;
    
    if (connection.type === 'mssql') {
      const sql = require('mssql');
      const isAzure = connection.host?.includes('.database.windows.net');
      
      const config = {
        user: connection.username,
        password: connection.password_encrypted,
        server: connection.host,
        database: connection.database_name,
        port: connection.port || 1433,
        options: {
          encrypt: isAzure,
          trustServerCertificate: !isAzure
        },
        requestTimeout: 600000 // 10 dakika
      };
      
      const pool = await sql.connect(config);
      
      // ============================================
      // CURSOR/PAGINATION MANTIƒûI - 5000'erlik par√ßalar
      // Bellek dolmaz, milyonlarca satƒ±r √ßekilebilir!
      // ============================================
      const BATCH_SIZE = 5000;
      let currentMaxId = maxId;
      let lastFetchedId = maxId;
      let hasMoreData = true;
      
      logger.info('MSSQL ID-based sync with cursor starting', { 
        startMaxId: maxId, 
        batchSize: BATCH_SIZE,
        rowLimit: limit 
      });
      
      while (hasMoreData && (limit === null || totalInserted < limit)) {
        const remainingLimit = limit ? Math.min(BATCH_SIZE, limit - totalInserted) : BATCH_SIZE;
        const query = `SELECT TOP ${remainingLimit} * FROM ${sourceTableName} WHERE ${reference_column} > ${currentMaxId} ORDER BY ${reference_column}`;
        
        const result = await pool.request().query(query);
        const rows = result.recordset;
        
        if (rows.length === 0) {
          hasMoreData = false;
          logger.info('No more rows to fetch', { currentMaxId, totalInserted });
          break;
        }
        
        // ClickHouse'a yaz
        const transformedBatch = transformBatchForClickHouse(rows);
        await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedBatch);
        totalInserted += rows.length;
        
        // Sonraki par√ßa i√ßin max ID g√ºncelle
        lastFetchedId = rows[rows.length - 1][reference_column];
        currentMaxId = lastFetchedId;
        
        // Progress g√ºncelle
        if (jobId) {
          await db.query(
            `UPDATE etl_jobs SET rows_processed = $1 WHERE id = $2`,
            [totalInserted, jobId]
          );
        }
        
        logger.info('MSSQL cursor batch inserted', { 
          batchSize: rows.length, 
          totalInserted, 
          currentMaxId
        });
        
        if (limit && totalInserted >= limit) {
          hasMoreData = false;
        }
      }
      
      await pool.close();
      
      if (totalInserted > 0) {
        await db.query(
          `UPDATE datasets SET last_sync_value = $1, last_sync_at = NOW() WHERE id = $2`,
          [String(lastFetchedId), dataset.id]
        );
        logger.info('Updated last_sync_value', { newMaxId: lastFetchedId, totalInserted });
      }
      
    } else if (connection.type === 'mysql') {
      const mysql = require('mysql2/promise');
      
      const conn = await mysql.createConnection({
        host: connection.host,
        port: connection.port || 3306,
        user: connection.username,
        password: connection.password_encrypted,
        database: connection.database_name,
        charset: 'utf8mb4',
        dateStrings: true
      });
      
      // ============================================
      // CURSOR/PAGINATION MANTIƒûI - 5000'erlik par√ßalar
      // Bellek dolmaz, milyonlarca satƒ±r √ßekilebilir!
      // ============================================
      const BATCH_SIZE = 5000;
      // ‚ö†Ô∏è maxId integer olmalƒ± - MySQL prepared statement i√ßin!
      let currentMaxId: number = parseInt(String(maxId)) || 0;
      let lastFetchedId: number = currentMaxId;
      let hasMoreData = true;
      
      logger.info('MySQL ID-based sync with cursor starting', { 
        startMaxId: currentMaxId, 
        batchSize: BATCH_SIZE,
        rowLimit: limit 
      });
      
      while (hasMoreData && totalInserted < limit) {
        // Her seferinde sadece 5000 satƒ±r √ßek
        const remainingLimit: number = Math.min(BATCH_SIZE, limit - totalInserted);
        
        // ‚ö†Ô∏è MySQL prepared statement sorunlarƒ± nedeniyle deƒüerleri direkt sorguya yazƒ±yoruz
        const query = `SELECT * FROM ${sourceTableName} WHERE ${reference_column} > ${currentMaxId} ORDER BY ${reference_column} LIMIT ${remainingLimit}`;
        logger.info('MySQL ID-based query', { query: query.substring(0, 200), currentMaxId, remainingLimit });
        const [rows] = await conn.query(query); // execute yerine query kullan!
        
        if ((rows as any[]).length === 0) {
          hasMoreData = false;
          logger.info('No more rows to fetch', { currentMaxId, totalInserted });
          break;
        }
        
        // ClickHouse'a yaz
        const transformedBatch = transformBatchForClickHouse(rows as any[]);
        await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedBatch);
        totalInserted += (rows as any[]).length;
        
        // Sonraki par√ßa i√ßin max ID g√ºncelle
        // ‚ö†Ô∏è Integer'a √ßevir - MySQL prepared statement i√ßin!
        lastFetchedId = parseInt(String((rows as any[])[(rows as any[]).length - 1][reference_column])) || 0;
        currentMaxId = lastFetchedId;
        
        // Progress g√ºncelle
        if (jobId) {
          await db.query(
            `UPDATE etl_jobs SET rows_processed = $1 WHERE id = $2`,
            [totalInserted, jobId]
          );
        }
        
        logger.info('MySQL cursor batch inserted', { 
          batchSize: (rows as any[]).length, 
          totalInserted, 
          currentMaxId,
          hasMoreData: (rows as any[]).length === remainingLimit
        });
        
        // Eƒüer limit'e ula≈ütƒ±ysak dur
        if (limit && totalInserted >= limit) {
          hasMoreData = false;
        }
      }
      
      await conn.end();
      
      if (totalInserted > 0) {
        await db.query(
          `UPDATE datasets SET last_sync_value = $1, last_sync_at = NOW() WHERE id = $2`,
          [String(lastFetchedId), dataset.id]
        );
        logger.info('Updated last_sync_value', { newMaxId: lastFetchedId, totalInserted });
      }
      
    } else if (connection.type === 'postgresql') {
      const { Client } = require('pg');
      const client = new Client({
        host: connection.host,
        port: connection.port || 5432,
        user: connection.username,
        password: connection.password_encrypted,
        database: connection.database_name
      });
      
      await client.connect();
      
      // ============================================
      // CURSOR/PAGINATION MANTIƒûI - 5000'erlik par√ßalar
      // Bellek dolmaz, milyonlarca satƒ±r √ßekilebilir!
      // ============================================
      const BATCH_SIZE = 5000;
      let currentMaxId = maxId;
      let lastFetchedId = maxId;
      let hasMoreData = true;
      
      logger.info('PostgreSQL ID-based sync with cursor starting', { 
        startMaxId: maxId, 
        batchSize: BATCH_SIZE,
        rowLimit: limit 
      });
      
      while (hasMoreData && (limit === null || totalInserted < limit)) {
        const remainingLimit = limit ? Math.min(BATCH_SIZE, limit - totalInserted) : BATCH_SIZE;
        const query = `SELECT * FROM ${sourceTableName} WHERE ${reference_column} > $1 ORDER BY ${reference_column} LIMIT $2`;
        const result = await client.query(query, [currentMaxId, remainingLimit]);
        
        if (result.rows.length === 0) {
          hasMoreData = false;
          logger.info('No more rows to fetch', { currentMaxId, totalInserted });
          break;
        }
        
        // ClickHouse'a yaz
        const transformedBatch = transformBatchForClickHouse(result.rows);
        await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedBatch);
        totalInserted += result.rows.length;
        
        // Sonraki par√ßa i√ßin max ID g√ºncelle
        lastFetchedId = result.rows[result.rows.length - 1][reference_column];
        currentMaxId = lastFetchedId;
        
        // Progress g√ºncelle
        if (jobId) {
          await db.query(
            `UPDATE etl_jobs SET rows_processed = $1 WHERE id = $2`,
            [totalInserted, jobId]
          );
        }
        
        logger.info('PostgreSQL cursor batch inserted', { 
          batchSize: result.rows.length, 
          totalInserted, 
          currentMaxId
        });
        
        if (limit && totalInserted >= limit) {
          hasMoreData = false;
        }
      }
      
      await client.end();
      
      if (totalInserted > 0) {
        await db.query(
          `UPDATE datasets SET last_sync_value = $1, last_sync_at = NOW() WHERE id = $2`,
          [String(lastFetchedId), dataset.id]
        );
        logger.info('Updated last_sync_value', { newMaxId: lastFetchedId, totalInserted });
      }
    }
    
    // ============================================
    // COUNT VERIFICATION - Kaynak vs Hedef Kar≈üƒ±la≈ütƒ±rma
    // ============================================
    let verificationWarning: string | null = null;
    try {
      // ClickHouse'taki toplam satƒ±r sayƒ±sƒ±
      const chCountResult = await clickhouse.queryOne(`
        SELECT count() as total FROM clixer_analytics.${clickhouse_table}
      `);
      const clickhouseCount = parseInt(chCountResult?.total || '0');
      
      // Kaynak DB'deki toplam satƒ±r sayƒ±sƒ± (sadece count, hƒ±zlƒ±)
      let sourceCount = 0;
      const sourceTableName = dataset.source_table || (dataset.source_query ? extractTableFromQuery(dataset.source_query) : null);
      
      if (sourceTableName && connection.type === 'mssql') {
        const sql = require('mssql');
        const isAzure = connection.host?.includes('.database.windows.net');
        const config = {
          user: connection.username,
          password: connection.password_encrypted,
          server: connection.host,
          database: connection.database_name,
          port: connection.port || 1433,
          options: { encrypt: isAzure, trustServerCertificate: !isAzure },
          requestTimeout: 60000
        };
        const pool = await sql.connect(config);
        const countResult = await pool.request().query(`SELECT COUNT(*) as cnt FROM ${sourceTableName}`);
        sourceCount = countResult.recordset[0].cnt;
        await pool.close();
      } else if (sourceTableName && connection.type === 'mysql') {
        const mysql = require('mysql2/promise');
        const conn = await mysql.createConnection({
          host: connection.host,
          port: connection.port || 3306,
          user: connection.username,
          password: connection.password_encrypted,
          database: connection.database_name
        });
        const [rows] = await conn.execute(`SELECT COUNT(*) as cnt FROM ${sourceTableName}`);
        sourceCount = (rows as any[])[0].cnt;
        await conn.end();
      } else if (sourceTableName && connection.type === 'postgresql') {
        const { Client } = require('pg');
        const client = new Client({
          host: connection.host,
          port: connection.port || 5432,
          user: connection.username,
          password: connection.password_encrypted,
          database: connection.database_name
        });
        await client.connect();
        const result = await client.query(`SELECT COUNT(*) as cnt FROM ${sourceTableName}`);
        sourceCount = parseInt(result.rows[0].cnt);
        await client.end();
      }
      
      // Kar≈üƒ±la≈ütƒ±r
      const diff = sourceCount - clickhouseCount;
      const diffPercent = sourceCount > 0 ? Math.round((diff / sourceCount) * 100) : 0;
      
      logger.info('üìä Count Verification', { 
        source: sourceCount, 
        clickhouse: clickhouseCount, 
        diff, 
        diffPercent: `${diffPercent}%` 
      });
      
      // %1'den fazla fark varsa uyarƒ±
      if (Math.abs(diffPercent) > 1 && Math.abs(diff) > 1000) {
        verificationWarning = `Kaynak: ${sourceCount.toLocaleString()}, Hedef: ${clickhouseCount.toLocaleString()} - ${Math.abs(diff).toLocaleString()} satƒ±r fark (${diffPercent}%)`;
        logger.warn('‚ö†Ô∏è Count mismatch detected!', { 
          source: sourceCount, 
          clickhouse: clickhouseCount, 
          diff,
          warning: verificationWarning
        });
      }
    } catch (verifyError: any) {
      logger.warn('Count verification failed', { error: verifyError.message });
    }
    
    // Job tamamla (verification sonucu ile)
    if (jobId) {
      await db.query(
        `UPDATE etl_jobs SET 
          status = 'completed', 
          completed_at = NOW(), 
          rows_processed = $1,
          error_message = $2
        WHERE id = $3`,
        [totalInserted, verificationWarning, jobId]
      );
    }
    
    // Cache invalidate
    await cache.del(`kpi:${clickhouse_table}:*`);
    
    logger.info('ID-Based incremental sync completed', { 
      datasetId: dataset.id, 
      totalInserted,
      previousMaxId: maxId,
      verification: verificationWarning || 'OK'
    });
    
    return totalInserted;
    
  } catch (error: any) {
    logger.error('ID-Based sync failed', { error: error.message, datasetId: dataset.id });
    throw error;
  }
}

/**
 * EKSƒ∞K ID ARALIKLARI SYNC (Missing Ranges Sync)
 * 
 * Sadece eksik olan ID aralƒ±klarƒ±nƒ± kaynak DB'den √ßeker.
 * Truncate yapmaz, mevcut verilere ekler.
 * 
 * Kullanƒ±m: Veri doƒürulamada tespit edilen eksik aralƒ±klar i√ßin.
 */
async function syncMissingRanges(
  dataset: any, 
  connection: any, 
  jobId: string,
  ranges: Array<{start: number; end: number; missing_count?: number}>,
  pkColumn: string = 'id' // ‚ö†Ô∏è KULLANICI SE√áTƒ∞ƒûƒ∞ PK KOLONU
): Promise<number> {
  const { clickhouse_table, source_query } = dataset;
  
  if (!ranges || ranges.length === 0) {
    logger.warn('No ranges provided for missing sync', { datasetId: dataset.id });
    return 0;
  }
  
  // Kaynak sorgudan tablo adƒ±nƒ± √ßƒ±kar
  let sourceTable = dataset.source_table;
  if (!sourceTable && source_query) {
    const match = source_query.match(/FROM\s+\[?(\w+)\]?\.\[?(\w+)\]?/i) ||
                  source_query.match(/FROM\s+\[?(\w+)\]?/i);
    if (match) {
      sourceTable = match[2] || match[1];
    }
  }
  
  if (!sourceTable) {
    throw new Error('Kaynak tablo adƒ± bulunamadƒ±');
  }
  
  // ‚ö†Ô∏è Dƒ∞NAMƒ∞K PK KOLONU - kullanƒ±cƒ± se√ßiyor!
  const idColumn = pkColumn;
  let totalInserted = 0;
  
  logger.info('üîç Starting missing ranges sync', { 
    datasetId: dataset.id, 
    ranges: ranges.length,
    totalMissing: ranges.reduce((sum, r) => sum + (r.missing_count || (r.end - r.start + 1)), 0)
  });
  
  // column_mapping'i parse et
  let columnMapping = dataset.column_mapping || [];
  if (typeof columnMapping === 'string') {
    try { columnMapping = JSON.parse(columnMapping); } catch (e) { columnMapping = []; }
  }
  
  if (connection.type === 'mssql') {
    const mssql = require('mssql');
    const connStr = connection.connection_string || 
      `Server=${connection.host},${connection.port || 1433};Database=${connection.database_name};User Id=${connection.username};Password=${connection.password_encrypted};Encrypt=${connection.host?.includes('.database.windows.net')};TrustServerCertificate=true;Connection Timeout=30;Request Timeout=120000`;
    
    const pool = await mssql.connect(connStr);
    
    for (let i = 0; i < ranges.length; i++) {
      const range = ranges[i];
      logger.info(`Processing range ${i + 1}/${ranges.length}`, { start: range.start, end: range.end });
      
      // Bu aralƒ±ktaki verileri √ßek
      const query = `SELECT * FROM [${sourceTable}] WITH (NOLOCK) WHERE [${idColumn}] >= ${range.start} AND [${idColumn}] <= ${range.end}`;
      const result = await pool.request().query(query);
      
      if (result.recordset.length > 0) {
        // Column mapping yoksa olu≈ütur
        if (columnMapping.length === 0) {
          for (const key of Object.keys(result.recordset[0])) {
            const value = result.recordset[0][key];
            let clickhouseType = 'String';
            if (typeof value === 'number') {
              clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
            } else if (value instanceof Date) {
              clickhouseType = 'DateTime';
            }
            columnMapping.push({ sourceName: key, targetName: key, type: clickhouseType });
          }
        }
        
        // Veriyi ClickHouse formatƒ±na d√∂n√º≈üt√ºr
        const transformedData = transformBatchForClickHouse(result.recordset, columnMapping);
        
        // ClickHouse'a yaz
        await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedData);
        totalInserted += result.recordset.length;
        
        logger.info(`Range ${i + 1} completed`, { 
          inserted: result.recordset.length, 
          totalSoFar: totalInserted 
        });
      }
      
      // Progress g√ºncelle
      if (jobId) {
        const progress = Math.round(((i + 1) / ranges.length) * 100);
        await db.query(
          `UPDATE etl_jobs SET rows_processed = $1, error_message = $2 WHERE id = $3`,
          [totalInserted, `Range ${i + 1}/${ranges.length} - ${progress}%`, jobId]
        );
      }
    }
    
    await pool.close();
  } else {
    throw new Error(`Missing sync hen√ºz sadece MSSQL destekliyor. Mevcut: ${connection.type}`);
  }
  
  // OPTIMIZE √ßalƒ±≈ütƒ±r
  try {
    await clickhouse.execute(`OPTIMIZE TABLE clixer_analytics.${clickhouse_table} FINAL`);
    logger.info('OPTIMIZE completed after missing sync');
  } catch (optErr: any) {
    logger.warn('OPTIMIZE failed (non-critical)', { error: optErr.message });
  }
  
  logger.info('‚úÖ Missing ranges sync completed', { 
    datasetId: dataset.id, 
    totalInserted,
    rangesProcessed: ranges.length
  });
  
  return totalInserted;
}

/**
 * üöÄ SADECE YENƒ∞ KAYITLARI √áEK (New Records Sync)
 * 
 * 100M+ tablolar i√ßin EN VERƒ∞MLƒ∞ y√∂ntem!
 * ClickHouse'daki max ID'den sonraki t√ºm kayƒ±tlarƒ± batch batch √ßeker.
 * 
 * Avantajlar:
 * - RAM t√ºketimi minimum (5000'lik batch)
 * - Eksik ID aralƒ±klarƒ±nƒ± taramaya gerek yok
 * - Kaynakta yeni ne varsa hepsini alƒ±r
 * - Progress bar ile takip edilebilir
 */
async function syncNewRecordsAfterMaxId(
  dataset: any,
  connection: any,
  jobId: string,
  pkColumn: string = 'id',
  afterId: number = 0,
  limit?: number
): Promise<number> {
  const { clickhouse_table, source_query } = dataset;
  const BATCH_SIZE = 5000; // Her batch'te 5000 satƒ±r
  
  // Kaynak sorgudan tablo adƒ±nƒ± √ßƒ±kar
  let sourceTable = dataset.source_table;
  if (!sourceTable && source_query) {
    const match = source_query.match(/FROM\s+\[?(\w+)\]?\.\[?(\w+)\]?/i) ||
                  source_query.match(/FROM\s+\[?(\w+)\]?/i);
    if (match) {
      sourceTable = match[2] || match[1];
    }
  }
  
  if (!sourceTable) {
    throw new Error('Kaynak tablo adƒ± bulunamadƒ±');
  }
  
  logger.info('üöÄ NEW RECORDS SYNC ba≈ülƒ±yor', { 
    datasetId: dataset.id, 
    pkColumn, 
    afterId,
    limit: limit || 'UNLIMITED',
    batchSize: BATCH_SIZE
  });
  
  // column_mapping'i parse et
  let columnMapping = dataset.column_mapping || [];
  if (typeof columnMapping === 'string') {
    try { columnMapping = JSON.parse(columnMapping); } catch (e) { columnMapping = []; }
  }
  
  let totalInserted = 0;
  let currentMaxId = afterId;
  let hasMore = true;
  let batchNum = 0;
  
  if (connection.type === 'mssql') {
    const mssql = require('mssql');
    const connStr = connection.connection_string || 
      `Server=${connection.host},${connection.port || 1433};Database=${connection.database_name};User Id=${connection.username};Password=${connection.password_encrypted};Encrypt=${connection.host?.includes('.database.windows.net')};TrustServerCertificate=true;Connection Timeout=30;Request Timeout=120000`;
    
    const pool = await mssql.connect(connStr);
    
    // Kaynaktaki max ID'yi al (ilerleme hesabƒ± i√ßin)
    const sourceMaxResult = await pool.request().query(`SELECT MAX([${pkColumn}]) as max_id FROM [${sourceTable}] WITH (NOLOCK)`);
    const sourceMaxId = sourceMaxResult.recordset[0]?.max_id || 0;
    const expectedTotal = Math.max(0, sourceMaxId - afterId);
    
    logger.info('üìä Kaynak max ID bulundu', { sourceMaxId, afterId, expectedTotal });
    
    while (hasMore) {
      batchNum++;
      
      // Limit kontrol√º
      const remainingLimit = limit ? Math.max(0, limit - totalInserted) : BATCH_SIZE;
      const batchLimit = Math.min(BATCH_SIZE, remainingLimit);
      
      if (batchLimit === 0) {
        logger.info('Limite ulasildi, sync durduruluyor', { totalInserted, limit });
        break;
      }
      
      // Cursor/Pagination ile veri √ßek
      // MSSQL i√ßin TOP kullan, ORDER BY ile sƒ±rala
      const query = `
        SELECT TOP ${batchLimit} * 
        FROM [${sourceTable}] WITH (NOLOCK) 
        WHERE [${pkColumn}] > ${currentMaxId} 
        ORDER BY [${pkColumn}] ASC
      `;
      
      const result = await pool.request().query(query);
      
      if (result.recordset.length === 0) {
        hasMore = false;
        logger.info('√áekilecek veri kalmadƒ±, sync tamamlandƒ±');
        break;
      }
      
      // Column mapping yoksa olu≈ütur
      if (columnMapping.length === 0 && result.recordset.length > 0) {
        for (const key of Object.keys(result.recordset[0])) {
          const value = result.recordset[0][key];
          let clickhouseType = 'String';
          if (typeof value === 'number') {
            clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
          } else if (value instanceof Date) {
            clickhouseType = 'DateTime';
          }
          columnMapping.push({ sourceName: key, targetName: key, type: clickhouseType });
        }
      }
      
      // Veriyi ClickHouse formatƒ±na d√∂n√º≈üt√ºr
      const transformedData = transformBatchForClickHouse(result.recordset, columnMapping);
      
      // ClickHouse'a yaz
      await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedData);
      totalInserted += result.recordset.length;
      
      // Cursor'u g√ºncelle (son satƒ±rƒ±n ID'si)
      currentMaxId = result.recordset[result.recordset.length - 1][pkColumn];
      
      // Progress g√ºncelle
      const progress = expectedTotal > 0 ? Math.min(100, Math.round((totalInserted / expectedTotal) * 100)) : 0;
      await db.query(
        `UPDATE etl_jobs SET rows_processed = $1, error_message = $2 WHERE id = $3`,
        [totalInserted, `Batch ${batchNum}: ${totalInserted} satƒ±r (${progress}%)`, jobId]
      );
      
      logger.info(`Batch ${batchNum} tamamlandƒ±`, { 
        batchSize: result.recordset.length,
        totalSoFar: totalInserted,
        currentMaxId,
        progress: `${progress}%`
      });
      
      // Batch tam dolmadƒ±ysa veri bitmi≈ü demektir
      if (result.recordset.length < batchLimit) {
        hasMore = false;
      }
    }
    
    await pool.close();
  } else if (connection.type === 'postgresql') {
    const { Pool } = require('pg');
    const pool = new Pool({
      host: connection.host,
      port: connection.port || 5432,
      database: connection.database_name,
      user: connection.username,
      password: connection.password_encrypted,
      max: 5
    });
    
    while (hasMore) {
      batchNum++;
      const batchLimit = limit ? Math.min(BATCH_SIZE, limit - totalInserted) : BATCH_SIZE;
      if (batchLimit <= 0) break;
      
      const query = `SELECT * FROM ${sourceTable} WHERE ${pkColumn} > $1 ORDER BY ${pkColumn} ASC LIMIT $2`;
      const result = await pool.query(query, [currentMaxId, batchLimit]);
      
      if (result.rows.length === 0) {
        hasMore = false;
        break;
      }
      
      if (columnMapping.length === 0 && result.rows.length > 0) {
        for (const key of Object.keys(result.rows[0])) {
          const value = result.rows[0][key];
          let clickhouseType = 'String';
          if (typeof value === 'number') clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
          else if (value instanceof Date) clickhouseType = 'DateTime';
          columnMapping.push({ sourceName: key, targetName: key, type: clickhouseType });
        }
      }
      
      const transformedData = transformBatchForClickHouse(result.rows, columnMapping);
      await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedData);
      totalInserted += result.rows.length;
      currentMaxId = result.rows[result.rows.length - 1][pkColumn];
      
      await db.query(
        `UPDATE etl_jobs SET rows_processed = $1, error_message = $2 WHERE id = $3`,
        [totalInserted, `Batch ${batchNum}: ${totalInserted} satƒ±r`, jobId]
      );
      
      if (result.rows.length < batchLimit) hasMore = false;
    }
    
    await pool.end();
  } else if (connection.type === 'mysql') {
    const mysql = require('mysql2/promise');
    const conn = await mysql.createConnection({
      host: connection.host,
      port: connection.port || 3306,
      database: connection.database_name,
      user: connection.username,
      password: connection.password_encrypted,
      charset: 'utf8mb4'
    });
    
    while (hasMore) {
      batchNum++;
      const batchLimit = limit ? Math.min(BATCH_SIZE, limit - totalInserted) : BATCH_SIZE;
      if (batchLimit <= 0) break;
      
      // MySQL i√ßin inline values kullan (prepared statement LIMIT sorunu!)
      const query = `SELECT * FROM ${sourceTable} WHERE ${pkColumn} > ${currentMaxId} ORDER BY ${pkColumn} ASC LIMIT ${batchLimit}`;
      const [rows] = await conn.query(query);
      
      if (!rows || (rows as any[]).length === 0) {
        hasMore = false;
        break;
      }
      
      const rowsArray = rows as any[];
      
      if (columnMapping.length === 0 && rowsArray.length > 0) {
        for (const key of Object.keys(rowsArray[0])) {
          const value = rowsArray[0][key];
          let clickhouseType = 'String';
          if (typeof value === 'number') clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
          else if (value instanceof Date) clickhouseType = 'DateTime';
          columnMapping.push({ sourceName: key, targetName: key, type: clickhouseType });
        }
      }
      
      const transformedData = transformBatchForClickHouse(rowsArray, columnMapping);
      await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedData);
      totalInserted += rowsArray.length;
      currentMaxId = rowsArray[rowsArray.length - 1][pkColumn];
      
      await db.query(
        `UPDATE etl_jobs SET rows_processed = $1, error_message = $2 WHERE id = $3`,
        [totalInserted, `Batch ${batchNum}: ${totalInserted} satƒ±r`, jobId]
      );
      
      if (rowsArray.length < batchLimit) hasMore = false;
    }
    
    await conn.end();
  } else {
    throw new Error(`new_records_sync hen√ºz ${connection.type} desteklemiyor`);
  }
  
  // OPTIMIZE √ßalƒ±≈ütƒ±r (opsiyonel - b√ºy√ºk tablolarda zaman alƒ±r!)
  if (totalInserted > 0 && totalInserted < 1000000) { // 1M altƒ±nda optimize et
    try {
      await clickhouse.execute(`OPTIMIZE TABLE clixer_analytics.${clickhouse_table} FINAL`);
      logger.info('OPTIMIZE completed after new records sync');
    } catch (optErr: any) {
      logger.warn('OPTIMIZE skipped or failed (non-critical for large tables)', { error: optErr.message });
    }
  } else {
    logger.info('OPTIMIZE skipped for large table (>1M rows inserted)', { totalInserted });
  }
  
  logger.info('‚úÖ NEW RECORDS SYNC tamamlandƒ±', { 
    datasetId: dataset.id, 
    totalInserted,
    batchesProcessed: batchNum,
    finalMaxId: currentMaxId
  });
  
  return totalInserted;
}

/**
 * TARƒ∞H BAZLI Sƒ∞L-YAZ (Date Delete & Insert)
 * 
 * Partition gerektirmez! Basit ve etkili:
 * 1. ClickHouse'tan son X g√ºn√º sil (reference_column'a g√∂re)
 * 2. Kaynak DB'den son X g√ºn√º √ßek
 * 3. ClickHouse'a yaz
 * 
 * K√º√ß√ºk-orta tablolar i√ßin ideal!
 */
async function syncByDateDeleteInsert(dataset: any, connection: any, jobId?: string): Promise<number> {
  const {
    reference_column,
    delete_days = 1,
    clickhouse_table,
    source_query,
    source_table,
    row_limit
  } = dataset;

  // column_mapping'i parse et
  let columnMapping = dataset.column_mapping || [];
  if (typeof columnMapping === 'string') {
    try {
      columnMapping = JSON.parse(columnMapping);
    } catch (e) {
      columnMapping = [];
    }
  }

  logger.info('Date Delete & Insert sync starting', { 
    datasetId: dataset.id, 
    datasetName: dataset.name,
    referenceColumn: reference_column,
    deleteDays: delete_days,
    clickhouseTable: clickhouse_table
  });

  // Referans kolon kontrol√º
  if (!reference_column) {
    logger.warn('No reference column defined, falling back to full refresh', { datasetId: dataset.id });
    return await fullRefresh(dataset, connection, jobId);
  }

  // Source kontrol√º
  const sourceTableName = extractTableFromQuery(source_query || `SELECT * FROM ${source_table}`);
  if (!sourceTableName) {
    throw new Error('Kaynak tablo bulunamadƒ±');
  }

  try {
    // 1. ClickHouse'tan son X g√ºn√º sil
    const today = new Date();
    const startDate = new Date(today);
    startDate.setDate(today.getDate() - delete_days);
    const startDateStr = startDate.toISOString().split('T')[0]; // YYYY-MM-DD
    
    logger.info('Deleting data from ClickHouse', { 
      table: clickhouse_table, 
      dateColumn: reference_column,
      fromDate: startDateStr
    });

    await clickhouse.execute(`
      ALTER TABLE clixer_analytics.${clickhouse_table} 
      DELETE WHERE toDate(${reference_column}) >= '${startDateStr}'
    `);
    
    logger.info('Deleted recent data from ClickHouse');

    // 2. Kaynak DB'den son X g√ºn√º √ßek
    let totalInserted = 0;

    if (connection.type === 'mssql') {
      // MSSQL i√ßin tarih filtreli sorgu
      const columns = (dataset.column_mapping || []).map((c: any) => c.source).join(', ') || '*';
      let dateFilter = '';
      
      // MSSQL tarih formatƒ±
      if (delete_days === 0) {
        dateFilter = `WHERE CAST(${reference_column} AS DATE) = CAST(GETDATE() AS DATE)`;
      } else {
        dateFilter = `WHERE ${reference_column} >= DATEADD(day, -${delete_days}, CAST(GETDATE() AS DATE))`;
      }
      
      const limit = row_limit || 10000000;
      const query = `SELECT TOP ${limit} ${columns} FROM ${sourceTableName} ${dateFilter} ORDER BY ${reference_column}`;
      
      logger.info('Executing MSSQL date-filtered query', { 
        query: query.substring(0, 300),
        dateFilter,
        limit
      });

      // MSSQL streaming ile veri √ßek - custom query ile
      const originalQuery = dataset.source_query;
      dataset.source_query = query;
      
      // column_mapping'i parse et
      let columnMapping = dataset.column_mapping || [];
      if (typeof columnMapping === 'string') {
        try {
          columnMapping = JSON.parse(columnMapping);
        } catch (e) {
          columnMapping = [];
        }
      }
      
      try {
        totalInserted = await mssqlSync(dataset, connection, columnMapping, jobId);
      } finally {
        dataset.source_query = originalQuery;
      }
      
    } else if (connection.type === 'postgresql') {
      // PostgreSQL i√ßin
      const columns = (dataset.column_mapping || []).map((c: any) => c.source).join(', ') || '*';
      let dateFilter = '';
      
      if (delete_days === 0) {
        dateFilter = `WHERE ${reference_column}::date = CURRENT_DATE`;
      } else {
        dateFilter = `WHERE ${reference_column} >= CURRENT_DATE - INTERVAL '${delete_days} days'`;
      }
      
      const limit = row_limit || 10000000;
      const query = `SELECT ${columns} FROM ${sourceTableName} ${dateFilter} ORDER BY ${reference_column} LIMIT ${limit}`;
      
      logger.info('Executing PostgreSQL date-filtered query', { query: query.substring(0, 300) });
      
      // source_query'yi ge√ßici olarak deƒüi≈ütir
      const originalQuery = dataset.source_query;
      dataset.source_query = query;
      try {
        totalInserted = await streamingPostgreSQLSync(dataset, connection, columnMapping, jobId);
      } finally {
        dataset.source_query = originalQuery;
      }
      
    } else if (connection.type === 'mysql') {
      // MySQL i√ßin
      const columns = (dataset.column_mapping || []).map((c: any) => c.source).join(', ') || '*';
      let dateFilter = '';
      
      if (delete_days === 0) {
        dateFilter = `WHERE DATE(${reference_column}) = CURDATE()`;
      } else {
        dateFilter = `WHERE ${reference_column} >= DATE_SUB(CURDATE(), INTERVAL ${delete_days} DAY)`;
      }
      
      const limit = row_limit || 10000000;
      const query = `SELECT ${columns} FROM ${sourceTableName} ${dateFilter} ORDER BY ${reference_column} LIMIT ${limit}`;
      
      logger.info('Executing MySQL date-filtered query', { query: query.substring(0, 300) });
      
      // source_query'yi ge√ßici olarak deƒüi≈ütir
      const originalQuery = dataset.source_query;
      dataset.source_query = query;
      try {
        totalInserted = await mysqlSync(dataset, connection, columnMapping, jobId);
      } finally {
        dataset.source_query = originalQuery;
      }
      
    } else {
      logger.warn('Unsupported connection type for date_delete_insert, falling back to full refresh', { 
        type: connection.type 
      });
      return await fullRefresh(dataset, connection, jobId);
    }

    // 3. OPTIMIZE √ßalƒ±≈ütƒ±r
    await clickhouse.execute(`OPTIMIZE TABLE clixer_analytics.${clickhouse_table} FINAL`);
    
    logger.info('Date Delete & Insert sync completed', { 
      datasetId: dataset.id,
      totalInserted,
      deleteDays: delete_days
    });

    return totalInserted;
    
  } catch (error: any) {
    logger.error('Date Delete & Insert sync failed', { 
      datasetId: dataset.id,
      error: error.message 
    });
    throw error;
  }
}

/**
 * PARTITION BAZLI Sƒ∞L-YAZ (Sliding Window + Modified Algƒ±lama)
 * 
 * Bu fonksiyon ≈üunlarƒ± yapar:
 * 1. refresh_window_days kadar geriye gider (sliding window)
 * 2. detect_modified=true ise modified_at ile deƒüi≈üen g√ºnleri bulur
 * 3. Etkilenen partition'larƒ± siler
 * 4. Sadece o g√ºnlerin verilerini yazar
 * 
 * Power BI Incremental Refresh benzeri ama daha g√º√ßl√º!
 */
async function syncByDatePartition(dataset: any, connection: any, jobId?: string): Promise<number> {
  // üîß SELF-HEALING: Tablo yoksa otomatik olu≈ütur
  await ensureTableExists(dataset);
  
  const {
    partition_column,
    partition_type = 'monthly',
    refresh_window_days = 7,
    detect_modified = false,
    modified_column,
    clickhouse_table
  } = dataset;

  logger.info('Partition-based sync starting', { 
    datasetId: dataset.id, 
    partitionColumn: partition_column,
    partitionType: partition_type,
    refreshWindowDays: refresh_window_days,
    detectModified: detect_modified,
    modifiedColumn: modified_column
  });

  // Partition kolonu yoksa full refresh'e d√º≈ü
  if (!partition_column) {
    logger.warn('No partition column defined, falling back to full refresh', { datasetId: dataset.id });
    return await fullRefresh(dataset, connection, jobId);
  }

  // Source table kontrol√º
  if (!dataset.source_table && !dataset.source_query) {
    logger.error('No source table or query defined', { datasetId: dataset.id });
    throw new Error('Kaynak tablo veya sorgu tanƒ±mlanmamƒ±≈ü');
  }

  // ============================================
  // TUTARLILIK KONTROL√ú: Dataset ayarƒ± vs ClickHouse tablo yapƒ±sƒ±
  // ============================================
  try {
    const tableInfo = await clickhouse.queryOne(`
      SELECT partition_key, engine, sorting_key
      FROM system.tables 
      WHERE database = 'clixer_analytics' AND name = '${clickhouse_table}'
    `);
    
    if (tableInfo) {
      const actualPartitionKey = tableInfo.partition_key || '';
      const isActuallyMonthly = actualPartitionKey.includes('toYYYYMM') && !actualPartitionKey.includes('toYYYYMMDD');
      const isActuallyDaily = actualPartitionKey.includes('toYYYYMMDD');
      const expectedFormat = partition_type === 'daily' ? 'YYYYMMDD (g√ºnl√ºk)' : 'YYYYMM (aylƒ±k)';
      const actualFormat = isActuallyDaily ? 'YYYYMMDD (g√ºnl√ºk)' : (isActuallyMonthly ? 'YYYYMM (aylƒ±k)' : 'bilinmiyor');
      
      // Uyumsuzluk kontrol√º
      const isConsistent = 
        (partition_type === 'daily' && isActuallyDaily) || 
        (partition_type === 'monthly' && isActuallyMonthly);
      
      if (!isConsistent && actualPartitionKey) {
        logger.warn('‚ö†Ô∏è PARTITION FORMAT UYUMSUZLUƒûU!', {
          datasetId: dataset.id,
          datasetName: dataset.name,
          clickhouseTable: clickhouse_table,
          uiAyari: {
            partitionType: partition_type,
            beklenenFormat: expectedFormat
          },
          clickhouseGercek: {
            partitionKey: actualPartitionKey,
            gercekFormat: actualFormat,
            engine: tableInfo.engine
          },
          cozum: 'Tablo partition_key formatƒ± kullanƒ±lacak (UI ayarƒ± yoksayƒ±ldƒ±)'
        });
      } else {
        logger.info('‚úÖ Partition format tutarlƒ±', {
          datasetId: dataset.id,
          format: actualFormat,
          engine: tableInfo.engine
        });
      }
    }
  } catch (consistencyError: any) {
    logger.warn('Tutarlƒ±lƒ±k kontrol√º yapƒ±lamadƒ±', { error: consistencyError.message });
  }

  try {
    const columnMapping = dataset.column_mapping || [];
    
    // Column mapping kontrol√º
    if (!columnMapping || columnMapping.length === 0) {
      logger.error('No column mapping defined', { datasetId: dataset.id });
      throw new Error('Kolon e≈üle≈ütirmesi tanƒ±mlanmamƒ±≈ü');
    }
    
    // PostgreSQL baƒülantƒ±sƒ± kur
    if (connection.type !== 'postgresql') {
      logger.warn('Partition sync only supports PostgreSQL, falling back to full refresh', { type: connection.type });
      return await fullRefresh(dataset, connection, jobId);
    }

    const { Client } = require('pg');
    const client = new Client({
      host: connection.host,
      port: connection.port,
      database: connection.database_name,
      user: connection.username,
      password: connection.password_encrypted
    });
    await client.connect();

    // Source table veya query belirle
    const sourceFrom = dataset.source_table 
      ? dataset.source_table 
      : dataset.source_query 
        ? `(${dataset.source_query}) AS sq` 
        : null;
    
    if (!sourceFrom) {
      throw new Error('Kaynak tablo veya sorgu tanƒ±mlanmamƒ±≈ü');
    }

    try {
      // 1. AFFECTED DATES: Hangi g√ºnler etkilendi?
      let affectedDates: string[] = [];
      const today = new Date();
      
      if (detect_modified && modified_column) {
        // modified_at bazlƒ± algƒ±lama - hangi g√ºnlerin verisi deƒüi≈üti?
        const lastSyncTime = dataset.last_sync_at || new Date(0).toISOString();
        
        const affectedQuery = `
          SELECT DISTINCT DATE(${partition_column}) as affected_date
          FROM ${sourceFrom}
          WHERE ${modified_column} >= $1
          ORDER BY affected_date
        `;
        
        const affectedResult = await client.query(affectedQuery, [lastSyncTime]);
        affectedDates = affectedResult.rows.map((r: any) => r.affected_date.toISOString().split('T')[0]);
        
        logger.info('Modified dates detected', { 
          count: affectedDates.length, 
          dates: affectedDates.slice(0, 10),
          since: lastSyncTime 
        });
      }
      
      // 2. SLIDING WINDOW: Son X g√ºn√º de ekle
      if (refresh_window_days > 0) {
        for (let i = 0; i < refresh_window_days; i++) {
          const date = new Date(today);
          date.setDate(date.getDate() - i);
          const dateStr = date.toISOString().split('T')[0];
          if (!affectedDates.includes(dateStr)) {
            affectedDates.push(dateStr);
          }
        }
      }

      // Tarih yoksa en az bug√ºn√º ekle
      if (affectedDates.length === 0) {
        affectedDates.push(today.toISOString().split('T')[0]);
      }

      // Tarihleri sƒ±rala
      affectedDates.sort();
      
      logger.info('Dates to refresh', { 
        count: affectedDates.length, 
        dates: affectedDates 
      });

      // 3. HER TARƒ∞H ƒ∞√áƒ∞N: Partition sil + yaz
      // ============================================
      // 3. √ñNCE: Etkilenen partition'larƒ± Bƒ∞R KEZ sil
      // ============================================
      
      // Tablonun partition formatƒ±nƒ± √∂ƒüren (d√∂ng√º dƒ±≈üƒ±nda - performans i√ßin)
      const tableInfo = await clickhouse.queryOne(`
        SELECT partition_key 
        FROM system.tables 
        WHERE database = 'clixer_analytics' AND name = '${clickhouse_table}'
      `);
      const isMonthlyPartition = tableInfo?.partition_key?.includes('toYYYYMM') && !tableInfo?.partition_key?.includes('toYYYYMMDD');
      
      // Unique partition deƒüerlerini hesapla (aynƒ± ay i√ßin tek sil)
      const uniquePartitions = new Set<string>();
      for (const dateStr of affectedDates) {
        const partitionDate = new Date(dateStr);
        const partitionValue = isMonthlyPartition 
          ? partitionDate.toISOString().slice(0, 7).replace(/-/g, '')   // YYYYMM
          : partitionDate.toISOString().slice(0, 10).replace(/-/g, ''); // YYYYMMDD
        uniquePartitions.add(partitionValue);
      }
      
      logger.info('Unique partitions to drop', { 
        count: uniquePartitions.size, 
        partitions: Array.from(uniquePartitions),
        isMonthly: isMonthlyPartition,
        partitionKey: tableInfo?.partition_key
      });
      
      // Her unique partition'ƒ± sadece Bƒ∞R KEZ sil
      for (const partitionValue of uniquePartitions) {
        try {
          logger.info('Dropping partition', { table: clickhouse_table, partition: partitionValue });
          await clickhouse.execute(
            `ALTER TABLE clixer_analytics.${clickhouse_table} DROP PARTITION '${partitionValue}'`
          );
          logger.info('Partition dropped successfully', { table: clickhouse_table, partition: partitionValue });
        } catch (dropError: any) {
          if (!dropError.message?.includes('not found') && !dropError.message?.includes('Cannot find')) {
            logger.warn('Partition drop warning', { partition: partitionValue, error: dropError.message });
          }
        }
      }
      
      // ============================================
      // 4. SONRA: T√ºm veriyi ekle (partition silme yok artƒ±k)
      // ============================================
      let totalRowsProcessed = 0;
      
      for (const dateStr of affectedDates) {
        // Job iptal edilmi≈ü mi kontrol et
        if (jobId && await isJobCancelled(jobId)) {
          logger.info('Job cancelled during partition sync', { jobId, currentDate: dateStr });
          break;
        }

        logger.info('Processing date data', { date: dateStr });

        // O tarihin verilerini √ßek ve yaz (partition silme ARTIK YOK - yukarƒ±da yapƒ±ldƒ±)
        const dateCondition = partition_type === 'daily'
          ? `DATE(${partition_column}) = '${dateStr}'`
          : `DATE_TRUNC('month', ${partition_column}) = DATE_TRUNC('month', '${dateStr}'::date)`;

        const selectColumns = columnMapping.map((col: any) => col.source).join(', ');
        
        // Source table veya source_query'den veri √ßek
        let dataQuery: string;
        if (dataset.source_query) {
          // Subquery olarak kullan
          dataQuery = `SELECT ${selectColumns} FROM (${dataset.source_query}) AS sq WHERE ${dateCondition}`;
        } else if (dataset.source_table) {
          dataQuery = `SELECT ${selectColumns} FROM ${dataset.source_table} WHERE ${dateCondition}`;
        } else {
          throw new Error('Kaynak tablo veya sorgu tanƒ±mlanmamƒ±≈ü');
        }
        
        const dataResult = await client.query(dataQuery);
        const rows = dataResult.rows;
        
        if (rows.length > 0) {
          // ClickHouse'a yaz
          const targetColumns = columnMapping.map((col: any) => col.target);
          
          // Batch insert
          const batchSize = 5000;
          for (let i = 0; i < rows.length; i += batchSize) {
            const batch = rows.slice(i, i + batchSize);
            const values = batch.map((row: any) => {
              return '(' + columnMapping.map((col: any) => {
                const val = row[col.source];
                if (val === null || val === undefined) return 'NULL';
                
                // DateTime veya Date tipinde kolon i√ßin akƒ±llƒ± d√∂n√º≈üt√ºr√ºc√º kullan
                if (col.clickhouseType === 'DateTime' || col.clickhouseType === 'DateTime64' || col.clickhouseType === 'Date') {
                  const converted = convertToClickHouseDateTime(val);
                  if (converted) {
                    // Date tipi i√ßin sadece tarih kƒ±smƒ±nƒ± al
                    if (col.clickhouseType === 'Date') {
                      return `'${converted.split(' ')[0]}'`;
                    }
                    return `'${converted}'`;
                  }
                  // Varsayƒ±lan deƒüer
                  return col.clickhouseType === 'Date' ? "'1970-01-01'" : "'1970-01-01 00:00:00'";
                }
                
                // Tarih benzeri string'ler i√ßin de kontrol et
                if (val instanceof Date || (typeof val === 'string' && val.match(/\d{4}[-\/]\d{2}[-\/]\d{2}|\d{2}[-\/]\d{2}[-\/]\d{4}/))) {
                  const converted = convertToClickHouseDateTime(val);
                  if (converted) return `'${converted}'`;
                }
                
                if (typeof val === 'string') return `'${val.replace(/'/g, "''")}'`;
                return val;
              }).join(',') + ')';
            }).join(',\n');

            const insertSql = `
              INSERT INTO clixer_analytics.${clickhouse_table} (${targetColumns.join(',')})
              VALUES ${values}
            `;
            
            await clickhouse.execute(insertSql);
          }
          
          totalRowsProcessed += rows.length;
          logger.info('Partition data written', { 
            date: dateStr, 
            rows: rows.length 
          });
        } else {
          logger.info('No data for partition', { date: dateStr });
        }
      }

      // Cache invalidate
      await cache.del(`kpi:${clickhouse_table}:*`);
      
      // ============================================
      // VERƒ∞ TUTARLILIK KONTROL√ú VE DUPLICATE TEMƒ∞ZLƒ∞ƒûƒ∞
      // ============================================
      const validation = await validateDataConsistency(dataset, client, totalRowsProcessed);
      
      if (!validation.isConsistent) {
        logger.warn('Partition sync consistency warning', {
          datasetId: dataset.id,
          expected: totalRowsProcessed,
          actual: validation.targetCount,
          duplicatesRemoved: validation.duplicateCount
        });
      }
      
      logger.info('Partition sync completed', { 
        datasetId: dataset.id, 
        rowsProcessed: totalRowsProcessed,
        finalRowCount: validation.targetCount,
        partitionsProcessed: affectedDates.length,
        duplicatesRemoved: validation.duplicateCount,
        isConsistent: validation.isConsistent
      });

      // ƒ∞≈ülenen satƒ±r sayƒ±sƒ± 0 ise, tablodaki toplam satƒ±r sayƒ±sƒ±nƒ± d√∂nd√ºr
      // (sliding window dƒ±≈üƒ±nda veri yoksa bile tablo bo≈ü deƒüil)
      return totalRowsProcessed > 0 ? totalRowsProcessed : validation.targetCount;

    } finally {
      await client.end();
    }

  } catch (error: any) {
    logger.error('Partition sync failed', { datasetId: dataset.id, error: error.message });
    throw error;
  }
}

// ============================================
// MSSQL SYNC (Azure SQL dahil) - STREAMING MODE
// ============================================
async function mssqlSync(
  dataset: any,
  connection: any,
  columnMapping: any[],
  jobId?: string
): Promise<number> {
  const mssql = require('mssql');
  
  const isAzure = connection.host.includes('.database.windows.net');
  const config = {
    server: connection.host,
    port: connection.port || 1433,
    database: connection.database_name,
    user: connection.username,
    password: connection.password_encrypted,
    options: { 
      encrypt: isAzure, 
      trustServerCertificate: !isAzure,
      requestTimeout: 600000 // 10 dakika timeout
    },
    pool: {
      max: 10,
      min: 0,
      idleTimeoutMillis: 30000
    }
  };
  
  const rowLimit = dataset.row_limit || null;
  
  logger.info('üî∑ MSSQL Streaming Sync starting', {
    datasetId: dataset.id,
    table: dataset.clickhouse_table,
    isAzure,
    host: connection.host,
    rowLimit: rowLimit || 'UNLIMITED'
  });
  
  let pool: any = null;
  
  // Date format helper - g√ºvenli versiyon
  const formatDate = (date: Date): string | null => {
    // Ge√ßersiz Date kontrol√º
    if (!date || isNaN(date.getTime())) {
      return null;
    }
    const pad = (n: number, len = 2) => n.toString().padStart(len, '0');
    const year = date.getFullYear();
    // Yƒ±l ge√ßersizse (√ßok k√º√ß√ºk veya √ßok b√ºy√ºk)
    if (year < 1900 || year > 2100) {
      return null;
    }
    const month = pad(date.getMonth() + 1);
    const day = pad(date.getDate());
    const hours = pad(date.getHours());
    const minutes = pad(date.getMinutes());
    const seconds = pad(date.getSeconds());
    return `${year}-${month}-${day} ${hours}:${minutes}:${seconds}`;
  };
  
  try {
    pool = await mssql.connect(config);
    logger.info('MSSQL connected');
    
    // Sorgu olu≈ütur
    let query = dataset.source_query || `SELECT * FROM ${dataset.source_table}`;
    
    // ‚ùó KRITIK: Dataset olu≈üturulurken LIMIT/TOP 10 ile test edilmi≈ü olabilir!
    // Full Refresh'te T√úM veriyi √ßekmek i√ßin LIMIT/TOP'ƒ± kaldƒ±r
    query = query.replace(/\s+LIMIT\s+\d+\s*/gi, ' ').trim();
    query = query.replace(/\bSELECT\s+TOP\s*\(?(\d+)\)?\s+/gi, 'SELECT ').trim();
    
    // Row limit varsa (kullanƒ±cƒ± isterse) yeniden ekle
    if (rowLimit && !query.toUpperCase().includes('TOP ') && !query.toUpperCase().includes('TOP(')) {
      query = query.replace(/^SELECT\s+/i, `SELECT TOP ${rowLimit} `);
    }
    
    logger.info('Executing MSSQL streaming query', { query: query.substring(0, 200) });
    
    // ClickHouse tablosunu truncate et (full_refresh i√ßin)
    if (dataset.sync_strategy === 'full_refresh') {
      try {
        await clickhouse.execute(`TRUNCATE TABLE clixer_analytics.${dataset.clickhouse_table}`);
        logger.info('Truncated ClickHouse table');
      } catch (truncErr: any) {
        await clickhouse.execute(`ALTER TABLE clixer_analytics.${dataset.clickhouse_table} DELETE WHERE 1=1`);
      }
    }
    
    // STREAMING MODE
    return new Promise((resolve, reject) => {
      const request = pool.request();
      request.stream = true; // ‚ö° STREAMING A√áIK!
      
      let batch: any[] = [];
      let totalInserted = 0;
      let columnMappingInitialized = columnMapping.length > 0;
      const STREAM_BATCH_SIZE = 5000; // Her 5000 satƒ±rda bir yaz
      let lastError: string | null = null; // Hata takibi i√ßin
      
      // Row event - her satƒ±r geldiƒüinde
      request.on('row', async (row: any) => {
        // ƒ∞lk satƒ±rda column mapping olu≈ütur
        if (!columnMappingInitialized) {
          for (const key of Object.keys(row)) {
            const value = row[key];
            let clickhouseType = 'String';
            if (typeof value === 'number') {
              clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
            } else if (value instanceof Date) {
              clickhouseType = 'DateTime';
            }
            columnMapping.push({ source: key, target: key, clickhouseType });
          }
          columnMappingInitialized = true;
          logger.info('Column mapping auto-generated', { columns: columnMapping.length });
        }
        
        // Date objelerini ve DateTime/Date kolonlarƒ±nƒ± d√ºzg√ºn formata √ßevir
        const processedRow: any = {};
        for (const key in row) {
          const colInfo = columnMapping.find((c: any) => c.source === key || c.target === key);
          let val = row[key];
          
          // DEBUG: ƒ∞lk satƒ±rda opening_date deƒüerini logla
          if (key === 'opening_date' && batch.length === 0) {
            logger.info('DEBUG opening_date', { 
              key, 
              val, 
              valType: typeof val, 
              isDate: val instanceof Date,
              colInfo: colInfo ? { source: colInfo.source, target: colInfo.target, clickhouseType: colInfo.clickhouseType } : null
            });
          }
          
          if (val === null || val === undefined) {
            processedRow[key] = null;
          } else if (val instanceof Date) {
            // Date nesnesi - g√ºvenli d√∂n√º≈ü√ºm
            const formatted = formatDate(val);
            if (formatted) {
              if (colInfo?.clickhouseType === 'Date') {
                processedRow[key] = formatted.split(' ')[0]; // Sadece tarih
              } else {
                processedRow[key] = formatted;
              }
            } else {
              // Ge√ßersiz Date - varsayƒ±lan deƒüer kullan
              processedRow[key] = colInfo?.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
            }
          } else if (colInfo?.clickhouseType === 'Date' || colInfo?.clickhouseType === 'DateTime') {
            // String olarak gelen ama Date/DateTime tipinde olmasƒ± gereken deƒüer
            const converted = convertToClickHouseDateTime(val);
            if (converted) {
              if (colInfo.clickhouseType === 'Date') {
                processedRow[key] = converted.split(' ')[0]; // Sadece tarih
              } else {
                processedRow[key] = converted;
              }
            } else {
              // D√∂n√º≈üt√ºr√ºlemedi - varsayƒ±lan deƒüer
              processedRow[key] = colInfo.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
            }
          } else {
            processedRow[key] = val;
          }
        }
        
        batch.push(processedRow);
        
        // Batch doldu - ClickHouse'a yaz
        if (batch.length >= STREAM_BATCH_SIZE) {
          request.pause(); // Streaming durdur
          
          try {
            const insertData = batch.map((r: any) => {
              const obj: any = {};
              for (const col of columnMapping) {
                // processedRow key olarak orijinal source adƒ±nƒ± kullanƒ±yor
                let val = r[col.source];
                
                // ‚ö†Ô∏è KRƒ∞Tƒ∞K: Date objesi ise √∂nce string'e √ßevir!
                // ClickHouse client Date objelerini JSON'a d√ºzg√ºn serialize edemiyor
                if (val instanceof Date) {
                  const formatted = formatDate(val);
                  if (formatted) {
                    val = col.clickhouseType === 'Date' ? formatted.split(' ')[0] : formatted;
                  } else {
                    val = col.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
                  }
                }
                
                // Date/DateTime tiplerini d√ºzelt
                if (col.clickhouseType === 'DateTime' || col.clickhouseType === 'Date') {
                  if (val === null || val === undefined || val === '') {
                    val = col.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
                  } else if (typeof val === 'string') {
                    // Zaten string ise kontrol et
                    const converted = convertToClickHouseDateTime(val);
                    if (converted) {
                      val = col.clickhouseType === 'Date' ? converted.split(' ')[0] : converted;
                    } else {
                      val = col.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
                    }
                  }
                }
                
                // Null deƒüerleri varsayƒ±lanlarla deƒüi≈ütir
                if (val === null || val === undefined) {
                  if (col.clickhouseType === 'String') val = '';
                  else if (col.clickhouseType?.includes('Int') || col.clickhouseType?.includes('Float')) val = 0;
                  else if (col.clickhouseType === 'Date') val = '1970-01-01';
                  else if (col.clickhouseType === 'DateTime') val = '1970-01-01 00:00:00';
                  else val = '';
                }
                obj[col.target] = val;
              }
              return obj;
            });
            
            await clickhouse.insert(`clixer_analytics.${dataset.clickhouse_table}`, insertData);
            
            totalInserted += batch.length;
            batch = []; // Batch'i temizle
            
            // Progress g√ºncelle
            if (jobId) {
              await db.query(
                'UPDATE etl_jobs SET rows_processed = $1 WHERE id = $2',
                [totalInserted, jobId]
              );
            }
            
            logger.info(`Streaming batch inserted`, { totalInserted, rowLimit });
            
          } catch (insertError: any) {
            logger.error('Batch insert failed', { error: insertError.message });
            lastError = insertError.message;
          }
          
          request.resume(); // Streaming devam
        }
      });
      
      // Done event - t√ºm satƒ±rlar alƒ±ndƒ±
      request.on('done', async () => {
        // Kalan batch'i yaz
        if (batch.length > 0) {
          try {
            const insertData = batch.map((r: any) => {
              const obj: any = {};
              for (const col of columnMapping) {
                // processedRow key olarak orijinal source adƒ±nƒ± kullanƒ±yor
                let val = r[col.source];
                
                // ‚ö†Ô∏è KRƒ∞Tƒ∞K: Date objesi ise √∂nce string'e √ßevir!
                if (val instanceof Date) {
                  const formatted = formatDate(val);
                  if (formatted) {
                    val = col.clickhouseType === 'Date' ? formatted.split(' ')[0] : formatted;
                  } else {
                    val = col.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
                  }
                }
                
                // Date/DateTime tiplerini d√ºzelt
                if (col.clickhouseType === 'DateTime' || col.clickhouseType === 'Date') {
                  if (val === null || val === undefined || val === '') {
                    val = col.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
                  } else if (typeof val === 'string') {
                    // Zaten string ise kontrol et
                    const converted = convertToClickHouseDateTime(val);
                    if (converted) {
                      val = col.clickhouseType === 'Date' ? converted.split(' ')[0] : converted;
                    } else {
                      val = col.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
                    }
                  }
                }
                
                // Null deƒüerleri varsayƒ±lanlarla deƒüi≈ütir
                if (val === null || val === undefined) {
                  if (col.clickhouseType === 'String') val = '';
                  else if (col.clickhouseType?.includes('Int') || col.clickhouseType?.includes('Float')) val = 0;
                  else if (col.clickhouseType === 'Date') val = '1970-01-01';
                  else if (col.clickhouseType === 'DateTime') val = '1970-01-01 00:00:00';
                  else val = '';
                }
                obj[col.target] = val;
              }
              return obj;
            });
            
            await clickhouse.insert(`clixer_analytics.${dataset.clickhouse_table}`, insertData);
            totalInserted += batch.length;
            
            if (jobId) {
              await db.query(
                'UPDATE etl_jobs SET rows_processed = $1 WHERE id = $2',
                [totalInserted, jobId]
              );
            }
            
            logger.info(`Final batch inserted`, { batchSize: batch.length, totalInserted });
          } catch (insertError: any) {
            logger.error('Final batch insert failed', { error: insertError.message });
            lastError = insertError.message;
          }
        }
        
        // Hata veya 0 satƒ±r kontrol√º
        if (lastError) {
          logger.error('‚ùå MSSQL Streaming Sync failed', { totalInserted, error: lastError });
          if (pool) await pool.close();
          reject(new Error(lastError));
          return;
        }
        
        if (totalInserted === 0) {
          // 0 satƒ±r = G√ºncellenecek yeni veri yok (incremental sync i√ßin normal)
          logger.info('‚ÑπÔ∏è MSSQL Streaming Sync: G√ºncellenecek yeni veri bulunamadƒ± (0 satƒ±r)', {
            query: query.substring(0, 200)
          });
          if (pool) await pool.close();
          resolve(0); // Ba≈üarƒ±lƒ±, sadece yeni veri yok
          return;
        }
        
        // COUNT VERIFICATION
        try {
          const validation = await validateDataConsistency(dataset, null, totalInserted);
          logger.info('üìä Count Verification', { 
            source: totalInserted, 
            clickhouse: validation.targetCount,
            isConsistent: validation.isConsistent,
            message: validation.message
          });
          
          // Verification sonucunu job'a kaydet (uyarƒ± olarak)
          if (!validation.isConsistent && jobId) {
            const verificationWarning = `Kaynak: ${totalInserted.toLocaleString()}, Hedef: ${validation.targetCount.toLocaleString()} - ${Math.abs(totalInserted - validation.targetCount).toLocaleString()} satƒ±r fark`;
            await db.query(
              'UPDATE etl_jobs SET error_message = $1 WHERE id = $2',
              [verificationWarning, jobId]
            );
          }
        } catch (verifyError: any) {
          logger.warn('Count verification failed', { error: verifyError.message });
        }
        
        logger.info('‚úÖ MSSQL Streaming Sync completed', { totalInserted });
        
        if (pool) {
          await pool.close();
        }
        
        resolve(totalInserted);
      });
      
      // Error event
      request.on('error', async (err: any) => {
        logger.error('MSSQL Stream error', { error: err.message });
        if (pool) {
          await pool.close();
        }
        reject(err);
      });
      
      // Sorguyu ba≈ülat
      request.query(query);
    });
    
  } catch (error: any) {
    logger.error('MSSQL Sync failed', { error: error.message, stack: error.stack });
    if (pool) {
      await pool.close();
    }
    throw error;
  }
}

// ============================================
// MYSQL SYNC
// ============================================
async function mysqlSync(
  dataset: any,
  connection: any,
  columnMapping: any[],
  jobId?: string
): Promise<number> {
  const mysql = require('mysql2/promise');
  
  logger.info('üî∂ MySQL Sync starting', {
    datasetId: dataset.id,
    table: dataset.clickhouse_table,
    host: connection.host
  });
  
  let conn: any = null;
  
  try {
    conn = await mysql.createConnection({
      host: connection.host,
      port: connection.port || 3306,
      database: connection.database_name,
      user: connection.username,
      password: connection.password_encrypted,
      charset: 'utf8mb4',
      dateStrings: true
    });
    
    // Sorgu olu≈ütur
    let query = dataset.source_query || `SELECT * FROM ${dataset.source_table}`;
    
    // ‚ùó KRITIK: Dataset olu≈üturulurken LIMIT 10 ile test edilmi≈ü olabilir!
    // Full Refresh'te T√úM veriyi √ßekmek i√ßin LIMIT'i kaldƒ±r
    query = query.replace(/\s+LIMIT\s+\d+\s*/gi, ' ').trim();
    
    // Row limit varsa (kullanƒ±cƒ± isterse) yeniden ekle
    const rowLimit = dataset.row_limit;
    if (rowLimit && !query.toUpperCase().includes('LIMIT')) {
      query += ` LIMIT ${rowLimit}`;
    }
    
    logger.info('MySQL row limit', { rowLimit: rowLimit || 'UNLIMITED' });
    logger.info('Executing MySQL query', { query: query.substring(0, 200) });
    
    const [rows] = await conn.query(query);
    
    logger.info('Fetched rows from MySQL', { count: rows.length });
    
    if (rows.length === 0) {
      logger.warn('No data fetched from MySQL');
      return 0;
    }
    
    // Column mapping yoksa otomatik olu≈ütur
    if (columnMapping.length === 0 && rows.length > 0) {
      const firstRow = rows[0];
      for (const key of Object.keys(firstRow)) {
        const value = firstRow[key];
        let clickhouseType = 'String';
        if (typeof value === 'number') {
          clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
        }
        columnMapping.push({
          source: key,
          target: key,
          clickhouseType
        });
      }
    }
    
    // ClickHouse tablosunu truncate et
    try {
      await clickhouse.execute(`TRUNCATE TABLE clixer_analytics.${dataset.clickhouse_table}`);
      logger.info('Truncated ClickHouse table');
    } catch (truncErr: any) {
      await clickhouse.execute(`ALTER TABLE clixer_analytics.${dataset.clickhouse_table} DELETE WHERE 1=1`);
    }
    
    // Batch insert
    let totalInserted = 0;
    
    for (let i = 0; i < rows.length; i += BATCH_SIZE) {
      const batch = rows.slice(i, i + BATCH_SIZE);
      
      // Her satƒ±rƒ± obje formatƒ±na d√∂n√º≈üt√ºr
      const insertData = batch.map((row: any) => {
        const obj: any = {};
        for (const col of columnMapping) {
          let val = row[col.source];
          
          // DateTime d√∂n√º≈ü√ºm√º
          if (col.clickhouseType === 'DateTime' || col.clickhouseType === 'Date') {
            val = toClickHouseDateTime(val) || '1970-01-01 00:00:00';
          }
          
          // Null handling
          if (val === null || val === undefined) {
            if (col.clickhouseType === 'String') val = '';
            else if (col.clickhouseType?.includes('Int') || col.clickhouseType?.includes('Float')) val = 0;
            else val = '';
          }
          
          obj[col.target] = val;
        }
        return obj;
      });
      
      await clickhouse.insert(
        `clixer_analytics.${dataset.clickhouse_table}`,
        insertData
      );
      
      totalInserted += batch.length;
      logger.info(`Inserted batch ${Math.floor(i/BATCH_SIZE) + 1}`, { 
        batchSize: batch.length, 
        totalInserted 
      });
    }
    
    // üîß OPTIMIZE FINAL - Duplicate'larƒ± fiziksel olarak sil
    // ReplacingMergeTree i√ßin kritik!
    try {
      await clickhouse.execute(`OPTIMIZE TABLE clixer_analytics.${dataset.clickhouse_table} FINAL`);
      logger.info('OPTIMIZE FINAL executed');
    } catch (optErr: any) {
      logger.warn('OPTIMIZE FINAL failed (will retry in background)', { error: optErr.message });
    }
    
    logger.info('‚úÖ MySQL Sync completed', { totalInserted });
    return totalInserted;
    
  } catch (error: any) {
    logger.error('MySQL Sync failed', { error: error.message, stack: error.stack });
    throw error;
  } finally {
    if (conn) {
      await conn.end();
    }
  }
}

async function fullRefresh(dataset: any, connection: any, jobId?: string): Promise<number> {
  logger.info('Full refresh starting', { datasetId: dataset.id, table: dataset.clickhouse_table, jobId });
  
  try {
    // üîß SELF-HEALING: Tablo yoksa otomatik olu≈ütur
    await ensureTableExists(dataset);
    
    const columnMapping = dataset.column_mapping || [];
    
    // PostgreSQL i√ßin STREAMING destekli ETL (200M+ satƒ±r destekler)
    if (connection.type === 'postgresql') {
      return await streamingPostgreSQLSync(dataset, connection, columnMapping, jobId);
    }
    
    // MSSQL i√ßin (Azure SQL dahil)
    if (connection.type === 'mssql') {
      return await mssqlSync(dataset, connection, columnMapping, jobId);
    }
    
    // MySQL i√ßin
    if (connection.type === 'mysql') {
      return await mysqlSync(dataset, connection, columnMapping, jobId);
    }
    
    // API ve diƒüer kaynaklar i√ßin (belleƒüe alarak i≈üle)
    let rows: any[] = [];
    
    if (connection.type === 'api') {
      // API baƒülantƒ±sƒ± i√ßin fetch yap
      logger.info('Fetching from API', { host: connection.host });
      
      // source_query JSON formatƒ±nda endpoint bilgisi i√ßeriyor olabilir
      let apiConfig: any = connection.api_config || {};
      
      // source_query JSON ise parse et
      if (dataset.source_query && typeof dataset.source_query === 'string') {
        try {
          const queryConfig = JSON.parse(dataset.source_query);
          apiConfig = { ...apiConfig, ...queryConfig };
        } catch (e) {
          // JSON deƒüilse endpoint olarak kullan
          apiConfig.endpoint = dataset.source_query;
        }
      }
      
      // URL olu≈ütur - Trim ile bo≈üluklarƒ± temizle
      let url = connection.host.trim().replace(/\/$/, '');
      
      // HTTP -> HTTPS otomatik d√∂n√º≈ü√ºm (301 redirect'i √∂nlemek i√ßin)
      if (url.startsWith('http://')) {
        const httpsUrl = url.replace('http://', 'https://');
        logger.info('Trying HTTPS first', { httpsUrl: httpsUrl.substring(0, 100) });
        
        try {
          const testRes = await fetch(httpsUrl, { method: 'HEAD' });
          if (testRes.ok || testRes.status < 400) {
            url = httpsUrl;
            logger.info('Using HTTPS', { url: url.substring(0, 100) });
          }
        } catch (e) {
          logger.debug('HTTPS not available, using HTTP', { url });
        }
      }
      
      if (apiConfig.endpoint) {
        url += '/' + apiConfig.endpoint.replace(/^\//, '');
      }
      if (apiConfig.queryParams) {
        url += (url.includes('?') ? '&' : '?') + apiConfig.queryParams;
      }
      
      logger.info('API URL', { url: url.substring(0, 100), method: apiConfig.method || 'GET' });
      
      // Headers olu≈ütur
      const fetchHeaders: Record<string, string> = {
        'Content-Type': 'application/json',
        'Accept': 'application/json',
        ...(apiConfig.headers || {})
      };
      
      // API Key varsa custom header'a ekle (connection.api_config'den)
      const connApiConfig = typeof connection.api_config === 'string' 
        ? JSON.parse(connection.api_config) 
        : (connection.api_config || {});
      
      if (connApiConfig.apiKey) {
        const headerName = connApiConfig.headerName || 'Authorization';
        if (headerName === 'Authorization') {
          fetchHeaders['Authorization'] = `Bearer ${connApiConfig.apiKey}`;
        } else {
          fetchHeaders[headerName] = connApiConfig.apiKey;
        }
        logger.info('API Key header set', { headerName });
      }
      
      // Fetch options
      const fetchOptions: RequestInit = {
        method: apiConfig.method || 'GET',
        headers: fetchHeaders,
        redirect: 'follow'
      };
      
      // POST/PUT i√ßin body ekle
      if ((apiConfig.method === 'POST' || apiConfig.method === 'PUT') && apiConfig.requestBody) {
        try {
          fetchOptions.body = typeof apiConfig.requestBody === 'string' 
            ? apiConfig.requestBody 
            : JSON.stringify(apiConfig.requestBody);
          logger.info('Request body set', { bodyLength: fetchOptions.body?.length });
        } catch (e) {
          logger.warn('Could not set request body', { error: (e as Error).message });
        }
      }
      
      const response = await fetch(url, fetchOptions);
      
      if (!response.ok) {
        throw new Error(`API error: ${response.status} ${response.statusText}`);
      }
      
      let data: any = await response.json();
      
      // responsePath ile nested data √ßƒ±kar
      if (apiConfig.responsePath) {
        const paths = apiConfig.responsePath.split('.');
        for (const p of paths) {
          if (data && (data as any)[p] !== undefined) {
            data = (data as any)[p];
          }
        }
      }
      
      rows = Array.isArray(data) ? data : [data];
      logger.info('Fetched rows from API', { count: rows.length });
    }
    
    if (rows.length === 0) {
      logger.warn('No data fetched from source');
      return 0;
    }
    
    // Column mapping yoksa otomatik olu≈ütur (API verileri i√ßin)
    if (columnMapping.length === 0 && rows.length > 0) {
      logger.info('No column mapping, creating auto mapping from first row');
      const firstRow = rows[0];
      for (const key of Object.keys(firstRow)) {
        const value = firstRow[key];
        let clickhouseType = 'String';
        if (typeof value === 'number') {
          clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
        } else if (typeof value === 'boolean') {
          clickhouseType = 'UInt8';
        }
        columnMapping.push({
          source: key,
          target: key.replace(/[^a-zA-Z0-9_]/g, '_'),
          clickhouseType
        });
      }
    }
    
    // 2. ClickHouse tablosunu truncate et
    try {
      await clickhouse.execute(`TRUNCATE TABLE clixer_analytics.${dataset.clickhouse_table}`);
      logger.info('Truncated ClickHouse table', { table: dataset.clickhouse_table });
    } catch (truncErr: any) {
      logger.warn('Could not truncate, trying ALTER DELETE', { error: truncErr.message });
      await clickhouse.execute(`ALTER TABLE clixer_analytics.${dataset.clickhouse_table} DELETE WHERE 1=1`);
    }
    
    // 3. Column mapping'e g√∂re veriyi d√∂n√º≈üt√ºr
    const transformedRows = rows.map(row => {
      const transformed: any = {};
      for (const mapping of columnMapping) {
        const sourceCol = mapping.source || mapping.sourceName;
        const targetCol = mapping.target || mapping.targetName;
        let value = row[sourceCol];
        const chType = mapping.clickhouseType || 'String';
        
        // Tip d√∂n√º≈ü√ºm√º - bo≈ü string de null gibi i≈ülenir
        const isNumericType = chType.includes('Int') || chType.includes('Float') || chType.includes('Decimal');
        
        if (value === null || value === undefined || (value === '' && isNumericType)) {
          // ClickHouse tiplerine g√∂re varsayƒ±lan deƒüer
          if (isNumericType) {
            value = 0;
          } else if (chType === 'Date') {
            value = '1970-01-01';
          } else if (chType === 'DateTime') {
            value = '1970-01-01 00:00:00';
          } else {
            value = '';
          }
        } else if (isNumericType && typeof value === 'string') {
          // String ama sayƒ±sal tip - sayƒ±ya d√∂n√º≈üt√ºr
          const numVal = parseFloat(value);
          value = isNaN(numVal) ? 0 : numVal;
        }
        
        transformed[targetCol] = value;
      }
      return transformed;
    });
    
    // 4. ClickHouse'a insert et (batch olarak - MEMORY OPTƒ∞Mƒ∞ZE)
    // BATCH_SIZE global sabiti kullanƒ±lƒ±r (5000 row = ~50MB RAM)
    let insertedCount = 0;
    let batchNumber = 0;
    const totalBatches = Math.ceil(transformedRows.length / BATCH_SIZE);
    const columns = columnMapping.map((m: any) => m.target || m.targetName);
    
    logger.info('Starting batch insert', { 
      totalRows: transformedRows.length, 
      batchSize: BATCH_SIZE, 
      totalBatches 
    });
    
    for (let i = 0; i < transformedRows.length; i += BATCH_SIZE) {
      batchNumber++;
      
      // ‚ö†Ô∏è KILL CHECK: Job iptal edilmi≈ü mi?
      if (jobId && await isJobCancelled(jobId)) {
        logger.info('Job cancelled during batch processing', { 
          jobId, 
          batchNumber, 
          processedSoFar: insertedCount 
        });
        return insertedCount; // Mevcut durumu d√∂nd√ºr
      }
      
      // Memory kontrol√º - limit a≈üƒ±lƒ±rsa GC tetikle
      const memCheck = checkMemory();
      if (!memCheck.ok) {
        logger.warn('Memory pressure, forcing GC', { usedMB: memCheck.usedMB });
        forceGC();
        // Kƒ±sa bir bekleme
        await new Promise(r => setTimeout(r, 100));
      }
      
      // Batch'i i≈üle
      const batch = transformedRows.slice(i, i + BATCH_SIZE);
      
      // VALUES olu≈ütur - Akƒ±llƒ± tarih d√∂n√º≈üt√ºr√ºc√º ile
      const values = batch.map(row => {
        const vals = columns.map((col: string) => {
          const val = row[col];
          if (val === null || val === undefined) {
            return 'NULL';
          }
          
          // Tarih/DateTime kontrol√º - akƒ±llƒ± d√∂n√º≈üt√ºr√ºc√º
          if (val instanceof Date || (typeof val === 'string' && val.match(/\d{4}[-\/]\d{2}[-\/]\d{2}|\d{2}[-\/]\d{2}[-\/]\d{4}/))) {
            const converted = toClickHouseDateTime(val);
            if (converted) return `'${converted}'`;
            return 'NULL';
          }
          
          if (typeof val === 'string') {
            // String escape - SQL injection korumasƒ±
            return `'${val.replace(/\\/g, '\\\\').replace(/'/g, "''")}'`;
          }
          return val;
        });
        return `(${vals.join(', ')})`;
      }).join(',\n');
      
      const insertSql = `INSERT INTO clixer_analytics.${dataset.clickhouse_table} (${columns.join(', ')}) VALUES ${values}`;
      
      try {
        await clickhouse.execute(insertSql);
        insertedCount += batch.length;
        
        // Progress log (her 10 batch'te bir)
        if (batchNumber % 10 === 0 || batchNumber === totalBatches) {
          const progress = Math.round((batchNumber / totalBatches) * 100);
          const memUsed = Math.round(process.memoryUsage().heapUsed / 1024 / 1024);
          logger.info('Batch progress', { 
            batch: `${batchNumber}/${totalBatches}`,
            progress: `${progress}%`,
            inserted: insertedCount,
            memoryMB: memUsed
          });
        }
        
        // Her GC_INTERVAL batch'te bir GC tetikle (memory basƒ±ncƒ±nƒ± azalt)
        if (batchNumber % (GC_INTERVAL / BATCH_SIZE) === 0) {
          forceGC();
        }
        
      } catch (insertErr: any) {
        logger.error('Insert error', { 
          error: insertErr.message, 
          batch: batchNumber,
          rowRange: `${i}-${i + batch.length}`
        });
        // Devam et, diƒüer batch'leri dene
      }
    }
    
    // Son GC
    forceGC();
    
    // ============================================
    // VERƒ∞ TUTARLILIK KONTROL√ú VE DUPLICATE TEMƒ∞ZLƒ∞ƒûƒ∞
    // ============================================
    const validation = await validateDataConsistency(dataset, null, transformedRows.length);
    
    if (!validation.isConsistent) {
      logger.warn('Data consistency warning', {
        datasetId: dataset.id,
        expected: transformedRows.length,
        actual: validation.targetCount,
        duplicatesRemoved: validation.duplicateCount
      });
    }
    
    const finalMem = Math.round(process.memoryUsage().heapUsed / 1024 / 1024);
    logger.info('Full refresh completed', { 
      datasetId: dataset.id, 
      rowsInserted: insertedCount,
      finalRowCount: validation.targetCount,
      duplicatesRemoved: validation.duplicateCount,
      isConsistent: validation.isConsistent,
      finalMemoryMB: finalMem
    });
    
    return validation.targetCount; // OPTIMIZE sonrasƒ± ger√ßek sayƒ±
    
  } catch (error: any) {
    logger.error('Full refresh error', { datasetId: dataset.id, error: error.message });
    throw error;
  }
}

// ============================================
// PENDING JOBS PROCESSOR
// ============================================

/**
 * Veritabanƒ±ndaki pending job'larƒ± i≈üle
 * Bu, ETL ba≈ülat butonuna basƒ±ldƒ±ƒüƒ±nda olu≈üturulan job'larƒ± i≈üler
 * 
 * ‚ö†Ô∏è KRƒ∞Tƒ∞K KURAL: Aynƒ± dataset i√ßin aynƒ± anda sadece bir job √ßalƒ±≈üabilir!
 * Bu kural duplicate job'larƒ± √∂nler ve birbirini bloklamayƒ± engeller.
 */
async function processPendingJobs(): Promise<void> {
  try {
    // Pending durumundaki job'larƒ± al (en eski √∂nce)
    const pendingJobs = await db.queryAll(`
      SELECT 
        e.id as job_id, 
        e.dataset_id, 
        e.action, 
        e.tenant_id,
        d.name as dataset_name,
        d.clickhouse_table
      FROM etl_jobs e
      JOIN datasets d ON e.dataset_id = d.id
      WHERE e.status = 'pending'
      ORDER BY e.started_at ASC
      LIMIT 10
    `);

    if (pendingJobs.length === 0) {
      return; // Bekleyen job yok
    }

    logger.info('Processing pending jobs', { count: pendingJobs.length });

    for (const job of pendingJobs) {
      // ‚ö†Ô∏è DUPLICATE PREVENTION: Dataset lock kontrol√º
      const lockAcquired = await acquireDatasetLock(job.dataset_id);
      
      if (!lockAcquired) {
        // Bu dataset i√ßin zaten √ßalƒ±≈üan bir job var
        logger.warn('Skipping job - dataset already has running job (duplicate prevention)', {
          jobId: job.job_id,
          datasetId: job.dataset_id,
          dataset: job.dataset_name
        });
        
        // Job'ƒ± skip olarak i≈üaretle (tekrar denenmemesi i√ßin)
        await db.query(
          `UPDATE etl_jobs SET status = 'skipped', completed_at = NOW(), 
           error_message = 'Aynƒ± dataset i√ßin zaten √ßalƒ±≈üan bir job var (duplicate prevention)' 
           WHERE id = $1`,
          [job.job_id]
        );
        continue;
      }
      
      try {
        // Job'ƒ± running olarak i≈üaretle
        await db.query(
          `UPDATE etl_jobs SET status = 'running', started_at = NOW() WHERE id = $1`,
          [job.job_id]
        );
        
        // Aktif job bilgisini Redis'e kaydet (kill i√ßin)
        await cache.set(`etl:active:${job.job_id}`, JSON.stringify({
          datasetId: job.dataset_id,
          datasetName: job.dataset_name,
          startedAt: new Date().toISOString(),
          pid: process.pid
        }), LOCK_TTL);

        logger.info('Starting pending job', { 
          jobId: job.job_id, 
          dataset: job.dataset_name, 
          action: job.action 
        });

        // Dataset bilgisini al
        const dataset = await db.queryOne(
          'SELECT * FROM datasets WHERE id = $1',
          [job.dataset_id]
        );

        if (!dataset) {
          throw new Error(`Dataset not found: ${job.dataset_id}`);
        }

        // Connection bilgisini al
        const connection = await db.queryOne(
          'SELECT * FROM data_connections WHERE id = $1',
          [dataset.connection_id]
        );

        if (!connection) {
          throw new Error(`Connection not found: ${dataset.connection_id}`);
        }

        const startTime = Date.now();
        let rowsProcessed = 0;

        // ‚úÖ √ñnce √ñZEL ACTION'larƒ± kontrol et (new_records_sync, missing_sync, partial_refresh)
        if (job.action === 'new_records_sync') {
          // üöÄ Sadece yeni kayƒ±tlarƒ± √ßek (max ID'den sonrasƒ±)
          logger.info('üöÄ NEW RECORDS SYNC starting from pending job', { jobId: job.job_id });
          
          // ClickHouse'daki max ID'yi al
          const pkColumn = dataset.reference_column || 'id';
          const chStats = await clickhouse.query(`SELECT max(${pkColumn}) as max_id FROM clixer_analytics.${dataset.clickhouse_table}`);
          const chMaxId = Number(chStats[0]?.max_id) || 0;
          
          rowsProcessed = await syncNewRecordsAfterMaxId(dataset, connection, job.job_id, pkColumn, chMaxId);
        } 
        else if (job.action === 'partial_refresh') {
          // Kƒ±smi yenileme (son X g√ºn)
          const days = dataset.delete_days || 7;
          rowsProcessed = await syncByDateDeleteInsert(dataset, connection, job.job_id);
        }
        else {
          // Normal sync - stratejisine g√∂re veri √ßek
          switch (dataset.sync_strategy) {
            case 'timestamp':
              rowsProcessed = await syncByTimestamp(dataset, connection, job.job_id);
              break;
            case 'id':
              rowsProcessed = await syncById(dataset, connection, job.job_id);
              break;
            case 'date_partition':
              rowsProcessed = await syncByDatePartition(dataset, connection, job.job_id);
              break;
            case 'date_delete_insert':
              rowsProcessed = await syncByDateDeleteInsert(dataset, connection, job.job_id);
              break;
            case 'full_refresh':
            default:
              rowsProcessed = await fullRefresh(dataset, connection, job.job_id);
          }
        }
        
        // Kill check - job iptal edilmi≈ü olabilir
        if (await isJobCancelled(job.job_id)) {
          logger.info('Job was cancelled', { jobId: job.job_id });
          await db.query(
            `UPDATE etl_jobs SET status = 'cancelled', completed_at = NOW() WHERE id = $1`,
            [job.job_id]
          );
        } else {
          // Job ba≈üarƒ±lƒ±
          await db.query(
            `UPDATE etl_jobs SET status = 'completed', completed_at = NOW(), rows_processed = $1 WHERE id = $2`,
            [rowsProcessed, job.job_id]
          );

          // Dataset g√ºncelle
          let totalRows = rowsProcessed;
          try {
            if (dataset.clickhouse_table) {
              const countResult = await clickhouse.query(`SELECT count() as cnt FROM clixer_analytics.${dataset.clickhouse_table}`);
              totalRows = countResult[0]?.cnt || rowsProcessed;
            }
          } catch (e) {
            logger.warn('Could not get total rows from ClickHouse', { error: e });
          }
          
          await db.query(
            `UPDATE datasets SET last_sync_at = NOW(), last_sync_rows = $1, total_rows = $2, status = 'active' WHERE id = $3`,
            [rowsProcessed, totalRows, job.dataset_id]
          );

          // Cache invalidate
          await cache.invalidate(`kpi:${dataset.clickhouse_table}:*`, 'etl');

          logger.info('Pending job completed', {
            jobId: job.job_id,
            dataset: job.dataset_name,
            rowsProcessed,
            duration: `${Date.now() - startTime}ms`
          });
        }

      } catch (jobError: any) {
        // Job ba≈üarƒ±sƒ±z
        logger.error('Pending job failed', { 
          jobId: job.job_id, 
          dataset: job.dataset_name, 
          error: jobError.message 
        });

        await db.query(
          `UPDATE etl_jobs SET status = 'failed', completed_at = NOW(), error_message = $1 WHERE id = $2`,
          [jobError.message, job.job_id]
        );
      } finally {
        // ‚ö†Ô∏è √ñNEMLƒ∞: Lock'ƒ± her zaman serbest bƒ±rak!
        await releaseDatasetLock(job.dataset_id);
        // Aktif job kaydƒ±nƒ± temizle
        await cache.del(`etl:active:${job.job_id}`);
      }
    }
  } catch (error) {
    logger.error('Pending jobs processor error', { error });
  }
}

// ============================================
// SCHEDULER
// ============================================

async function checkScheduledJobs(): Promise<void> {
  try {
    // etl_schedules tablosundan zamanƒ± gelen schedule'larƒ± bul
    const schedules = await db.queryAll(
      `SELECT s.*, d.name as dataset_name, d.clickhouse_table
       FROM etl_schedules s
       JOIN datasets d ON s.dataset_id = d.id
       WHERE s.is_active = true
       AND d.status = 'active'
       AND (s.next_run_at IS NULL OR s.next_run_at <= NOW())`
    );

    logger.info('Checking scheduled jobs', { count: schedules.length });

    for (const schedule of schedules) {
      logger.info('Running scheduled sync', { dataset: schedule.dataset_name, cron: schedule.cron_expression });
      
      try {
        await processETLJob({
          datasetId: schedule.dataset_id,
          action: 'incremental_sync'
        });

        // Bir sonraki √ßalƒ±≈üma zamanƒ±nƒ± hesapla (basit: cron'a g√∂re)
        let nextRun = new Date();
        const cron = schedule.cron_expression;
        
        // Basit cron parser
        if (cron === '* * * * *') nextRun.setMinutes(nextRun.getMinutes() + 1);
        else if (cron === '*/5 * * * *') nextRun.setMinutes(nextRun.getMinutes() + 5);
        else if (cron === '*/15 * * * *') nextRun.setMinutes(nextRun.getMinutes() + 15);
        else if (cron === '*/30 * * * *') nextRun.setMinutes(nextRun.getMinutes() + 30);
        else if (cron === '0 * * * *') nextRun.setHours(nextRun.getHours() + 1);
        else if (cron === '0 */6 * * *') nextRun.setHours(nextRun.getHours() + 6);
        else if (cron === '0 */12 * * *') nextRun.setHours(nextRun.getHours() + 12);
        else if (cron === '0 0 * * *') nextRun.setDate(nextRun.getDate() + 1);
        else nextRun.setMinutes(nextRun.getMinutes() + 1); // default: 1 dakika

        // Schedule'ƒ± g√ºncelle
        await db.query(
          `UPDATE etl_schedules SET next_run_at = $1, last_run_at = NOW() WHERE id = $2`,
          [nextRun, schedule.id]
        );
      } catch (jobError: any) {
        logger.error('Scheduled job failed', { dataset: schedule.dataset_name, error: jobError.message });
      }
    }
  } catch (error) {
    logger.error('Scheduler error', { error });
  }
}

// ============================================
// START WORKER
// ============================================

async function start() {
  try {
    db.createPool();
    await db.checkHealth();
    logger.info('PostgreSQL connected');

    clickhouse.createClickHouseClient();
    await clickhouse.checkHealth();
    logger.info('ClickHouse connected');

    cache.createRedisClient();
    await cache.checkHealth();
    logger.info('Redis connected');

    // ETL trigger event'lerini dinle
    cache.subscribe('etl:trigger', async (message: ETLJob) => {
      try {
        // Active jobs sayƒ±sƒ±nƒ± artƒ±r
        await cache.set('etl:worker:active_jobs', '1', 3600);
        await processETLJob(message);
        await cache.set('etl:worker:active_jobs', '0', 3600);
      } catch (error) {
        logger.error('ETL job processing failed', { message, error });
        await cache.set('etl:worker:active_jobs', '0', 3600);
      }
    });

    // ƒ∞lk ba≈ülangƒ±√ßta bekleyen job'larƒ± kontrol et
    logger.info('Checking for pending jobs on startup...');
    await processPendingJobs();

    // Pending job'larƒ± her 5 saniyede bir kontrol et
    setInterval(async () => {
      try {
        await processPendingJobs();
      } catch (e) {
        logger.error('Pending jobs check failed', { error: e });
      }
    }, 5 * 1000);

    // Scheduler - her dakika kontrol et
    setInterval(checkScheduledJobs, 60 * 1000);

    // Heartbeat - her 10 saniye
    const sendHeartbeat = async () => {
      try {
        await cache.set('etl:worker:heartbeat', Date.now().toString(), 60);
        await cache.set('etl:worker:status', JSON.stringify({
          startedAt: new Date().toISOString(),
          pid: process.pid,
          uptime: process.uptime()
        }), 60);
      } catch (e) {
        logger.error('Heartbeat failed', { error: e });
      }
    };
    
    // ƒ∞lk heartbeat
    await sendHeartbeat();
    // Periyodik heartbeat
    setInterval(sendHeartbeat, 10 * 1000);

    logger.info('‚öôÔ∏è ETL Worker started');

  } catch (error) {
    logger.error('Failed to start etl-worker', { error });
    process.exit(1);
  }
}

start();
