/**
 * Clixer - ETL Worker
 * Background job processor for ETL operations
 * AyrÄ± process - Ana uygulama etkilenmez!
 * 
 * MEMORY OPTÄ°MÄ°ZASYONU:
 * - BÃ¼yÃ¼k veri setleri batch'ler halinde iÅŸlenir
 * - Garbage collector manual tetikleme
 * - Memory limit kontrol
 */

import dotenv from 'dotenv';
dotenv.config({ path: '../../.env' });

// SSL sertifika kontrolÃ¼nÃ¼ devre dÄ±ÅŸÄ± bÄ±rak (self-signed veya expired sertifikalar iÃ§in)
process.env.NODE_TLS_REJECT_UNAUTHORIZED = '0';

import {
  createLogger,
  db,
  clickhouse,
  cache
} from '@clixer/shared';

const logger = createLogger({ service: 'etl-worker' });

// Memory Optimizasyonu Sabitleri
const MAX_MEMORY_MB = 1024; // 1GB limit
const BATCH_SIZE = 5000; // KÃ¼Ã§Ã¼k batch'ler = daha az memory
const GC_INTERVAL = 10000; // Her 10 batch'te bir GC tetikle

// ============================================
// AKILLI TARÄ°H DÃ–NÃœÅTÃœRÃœCÃœ
// FarklÄ± kaynaklardan gelen formatlarÄ± ClickHouse'a uygun hale getirir
// ============================================
function toClickHouseDateTime(val: any): string | null {
  if (val === null || val === undefined || val === '') return null;
  
  // Date objesi
  if (val instanceof Date) {
    if (isNaN(val.getTime())) return null;
    return val.toISOString().replace('T', ' ').replace('Z', '').split('.')[0];
  }
  
  // String deÄŸer
  if (typeof val === 'string') {
    const str = val.trim();
    
    // 1. ISO 8601: 2025-12-13T20:33:42.722Z veya 2025-12-13T20:33:42
    if (str.match(/^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}/)) {
      return str.replace('T', ' ').replace('Z', '').split('.')[0];
    }
    
    // 2. ClickHouse formatÄ± zaten: 2025-12-13 20:33:42
    if (str.match(/^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}/)) {
      return str.split('.')[0]; // Milisaniye varsa at
    }
    
    // 3. Sadece tarih: 2025-12-13
    if (str.match(/^\d{4}-\d{2}-\d{2}$/)) {
      return str + ' 00:00:00';
    }
    
    // 4. Avrupa formatÄ±: DD-MM-YYYY veya DD/MM/YYYY (gÃ¼n > 12 ise kesin bu)
    const euMatch = str.match(/^(\d{2})[-\/](\d{2})[-\/](\d{4})(?:\s+(\d{2}:\d{2}:\d{2}))?/);
    if (euMatch) {
      const day = parseInt(euMatch[1]);
      const month = parseInt(euMatch[2]);
      const year = euMatch[3];
      const time = euMatch[4] || '00:00:00';
      
      // GÃ¼n 12'den bÃ¼yÃ¼kse kesinlikle DD-MM-YYYY
      if (day > 12 && month <= 12) {
        return `${year}-${euMatch[2]}-${euMatch[1]} ${time}`;
      }
      // Ay 12'den bÃ¼yÃ¼kse kesinlikle MM-DD-YYYY (ABD)
      if (month > 12 && day <= 12) {
        return `${year}-${euMatch[1]}-${euMatch[2]} ${time}`;
      }
      // Ä°kisi de <=12 ise varsayÄ±lan olarak DD-MM-YYYY kabul et (TÃ¼rkiye iÃ§in)
      return `${year}-${euMatch[2]}-${euMatch[1]} ${time}`;
    }
    
    // 5. ABD formatÄ± aÃ§Ä±k: MM/DD/YYYY
    const usMatch = str.match(/^(\d{1,2})\/(\d{1,2})\/(\d{4})(?:\s+(\d{2}:\d{2}:\d{2}))?/);
    if (usMatch) {
      const month = usMatch[1].padStart(2, '0');
      const day = usMatch[2].padStart(2, '0');
      const year = usMatch[3];
      const time = usMatch[4] || '00:00:00';
      return `${year}-${month}-${day} ${time}`;
    }
    
    // 6. PostgreSQL timestamp without time zone: 2025-12-13 20:33:42.123456
    if (str.match(/^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+/)) {
      return str.split('.')[0];
    }
    
    // 7. Son Ã§are: Date.parse dene
    const parsed = new Date(str);
    if (!isNaN(parsed.getTime())) {
      return parsed.toISOString().replace('T', ' ').replace('Z', '').split('.')[0];
    }
    
    // TanÄ±namadÄ± - null dÃ¶n, log at
    logger.warn('Unknown date format, returning NULL', { value: val });
    return null;
  }
  
  // Number (timestamp)
  if (typeof val === 'number') {
    const date = new Date(val);
    if (!isNaN(date.getTime())) {
      return date.toISOString().replace('T', ' ').replace('Z', '').split('.')[0];
    }
  }
  
  return null;
}

// ============================================
// KAPSAMLI TARÄ°H FORMAT DÃ–NÃœÅTÃœRÃœCÃœ
// TÃ¼m SQL tarih formatlarÄ±nÄ± ClickHouse DateTime'a Ã§evirir
// ============================================

/**
 * TÃœM SQL TARÄ°H FORMATLARINI ClickHouse DateTime'a Ã§evirir
 * 
 * Desteklenen formatlar:
 * - ISO 8601: 2025-12-23T16:00:00.000Z, 2025-12-23T16:00:00Z
 * - ISO with offset: 2025-12-23T16:00:00+03:00
 * - SQL Server: 2025-12-23 16:00:00.0000000
 * - MySQL: 2025-12-23 16:00:00
 * - PostgreSQL: 2025-12-23 16:00:00.123456
 * - Date only: 2025-12-23
 * - European: 23/12/2025, 23.12.2025
 * - US: 12/23/2025
 * - JavaScript Date object
 * - Unix timestamp (number)
 * 
 * ClickHouse Ã§Ä±ktÄ± formatÄ±: 'YYYY-MM-DD HH:mm:ss'
 */
function convertToClickHouseDateTime(value: any): string | null {
  if (value === null || value === undefined || value === '') {
    return null;
  }
  
  // 1. JavaScript Date objesi
  if (value instanceof Date) {
    if (isNaN(value.getTime())) return null;
    const pad = (n: number) => n.toString().padStart(2, '0');
    return `${value.getFullYear()}-${pad(value.getMonth() + 1)}-${pad(value.getDate())} ${pad(value.getHours())}:${pad(value.getMinutes())}:${pad(value.getSeconds())}`;
  }
  
  // 2. Unix timestamp (number - milliseconds)
  if (typeof value === 'number') {
    const date = new Date(value);
    if (!isNaN(date.getTime())) {
      const pad = (n: number) => n.toString().padStart(2, '0');
      return `${date.getFullYear()}-${pad(date.getMonth() + 1)}-${pad(date.getDate())} ${pad(date.getHours())}:${pad(date.getMinutes())}:${pad(date.getSeconds())}`;
    }
    return null;
  }
  
  // 3. String formatlarÄ±
  if (typeof value === 'string') {
    const val = value.trim();
    
    // A. ISO 8601 with T and optional timezone: 2025-12-23T16:00:00.000Z
    let match = val.match(/^(\d{4})-(\d{2})-(\d{2})T(\d{2}):(\d{2}):(\d{2})(?:\.\d+)?(?:Z|[+-]\d{2}:?\d{2})?$/);
    if (match) {
      return `${match[1]}-${match[2]}-${match[3]} ${match[4]}:${match[5]}:${match[6]}`;
    }
    
    // B. SQL Server format: 2025-12-23 16:00:00.0000000
    match = val.match(/^(\d{4})-(\d{2})-(\d{2})\s+(\d{2}):(\d{2}):(\d{2})(?:\.\d+)?$/);
    if (match) {
      return `${match[1]}-${match[2]}-${match[3]} ${match[4]}:${match[5]}:${match[6]}`;
    }
    
    // C. MySQL/PostgreSQL: 2025-12-23 16:00:00
    match = val.match(/^(\d{4})-(\d{2})-(\d{2})\s+(\d{2}):(\d{2}):(\d{2})$/);
    if (match) {
      return val; // Zaten doÄŸru format
    }
    
    // D. Date only: 2025-12-23
    match = val.match(/^(\d{4})-(\d{2})-(\d{2})$/);
    if (match) {
      return `${val} 00:00:00`;
    }
    
    // E. European format: 23/12/2025 veya 23.12.2025
    match = val.match(/^(\d{2})[\/\.](\d{2})[\/\.](\d{4})(?:\s+(\d{2}):(\d{2}):(\d{2}))?$/);
    if (match) {
      const time = match[4] ? ` ${match[4]}:${match[5]}:${match[6]}` : ' 00:00:00';
      return `${match[3]}-${match[2]}-${match[1]}${time}`;
    }
    
    // F. US format: 12/23/2025 (MM/DD/YYYY)
    match = val.match(/^(\d{2})\/(\d{2})\/(\d{4})(?:\s+(\d{2}):(\d{2}):(\d{2}))?$/);
    if (match) {
      const time = match[4] ? ` ${match[4]}:${match[5]}:${match[6]}` : ' 00:00:00';
      return `${match[3]}-${match[1]}-${match[2]}${time}`;
    }
    
    // G. YYYYMMDD format: 20251223
    match = val.match(/^(\d{4})(\d{2})(\d{2})$/);
    if (match) {
      return `${match[1]}-${match[2]}-${match[3]} 00:00:00`;
    }
    
    // H. SQL Server short: Dec 23 2025 4:00PM
    match = val.match(/^([A-Za-z]{3})\s+(\d{1,2})\s+(\d{4})\s+(\d{1,2}):(\d{2})([AP]M)?$/i);
    if (match) {
      const months: Record<string, string> = { jan: '01', feb: '02', mar: '03', apr: '04', may: '05', jun: '06', jul: '07', aug: '08', sep: '09', oct: '10', nov: '11', dec: '12' };
      const mon = months[match[1].toLowerCase()] || '01';
      let hour = parseInt(match[4]);
      if (match[6]?.toUpperCase() === 'PM' && hour < 12) hour += 12;
      if (match[6]?.toUpperCase() === 'AM' && hour === 12) hour = 0;
      return `${match[3]}-${mon}-${match[2].padStart(2, '0')} ${hour.toString().padStart(2, '0')}:${match[5]}:00`;
    }
    
    // TanÄ±namadÄ± - null dÃ¶n
    logger.warn('Unknown date format, returning null', { value: val });
    return null;
  }
  
  return null;
}

/**
 * Bir deÄŸerin tarih olup olmadÄ±ÄŸÄ±nÄ± kontrol et
 */
function isDateLikeValue(value: any): boolean {
  if (value === null || value === undefined) return false;
  if (value instanceof Date) return true;
  if (typeof value !== 'string') return false;
  
  const val = value.trim();
  
  // ISO format check
  if (/^\d{4}-\d{2}-\d{2}[T\s]/.test(val)) return true;
  // Date only
  if (/^\d{4}-\d{2}-\d{2}$/.test(val)) return true;
  // European/US format
  if (/^\d{2}[\/\.]\d{2}[\/\.]\d{4}/.test(val)) return true;
  
  return false;
}

/**
 * Veri satÄ±rÄ±nÄ± ClickHouse iÃ§in dÃ¶nÃ¼ÅŸtÃ¼r
 * TÃ¼m tarih benzeri deÄŸerleri ClickHouse DateTime formatÄ±na Ã§evirir
 */
function transformRowForClickHouse(row: any): any {
  const transformed: any = {};
  
  for (const key in row) {
    const value = row[key];
    
    if (value === null || value === undefined) {
      transformed[key] = null;
      continue;
    }
    
    // Date objesi ise dÃ¶nÃ¼ÅŸtÃ¼r
    if (value instanceof Date) {
      transformed[key] = convertToClickHouseDateTime(value);
      continue;
    }
    
    // Tarih benzeri string ise dÃ¶nÃ¼ÅŸtÃ¼r
    if (typeof value === 'string' && isDateLikeValue(value)) {
      const converted = convertToClickHouseDateTime(value);
      transformed[key] = converted !== null ? converted : value;
      continue;
    }
    
    // DiÄŸer deÄŸerler olduÄŸu gibi
    transformed[key] = value;
  }
  
  return transformed;
}

/**
 * Veri batch'ini ClickHouse iÃ§in dÃ¶nÃ¼ÅŸtÃ¼r
 */
function transformBatchForClickHouse(rows: any[]): any[] {
  return rows.map(row => transformRowForClickHouse(row));
}

// ============================================
// TABLO YOKSA OLUÅTUR (SELF-HEALING)
// ============================================
// KAPSAMLI TÄ°P DÃ–NÃœÅÃœM HARÄ°TASI
// PostgreSQL/MySQL/MSSQL/Oracle â†’ ClickHouse
// ============================================
const SQL_TO_CLICKHOUSE_TYPE: Record<string, string> = {
  // ============ INTEGER TYPES ============
  'int': 'Int32', 'int4': 'Int32', 'integer': 'Int32',
  'int2': 'Int16', 'smallint': 'Int16',
  'int8': 'Int64', 'bigint': 'Int64',
  'serial': 'Int32', 'bigserial': 'Int64', 'smallserial': 'Int16',
  'oid': 'UInt32', 'tinyint': 'Int8', 'mediumint': 'Int32', 'year': 'Int16',
  'number': 'Float64', 'pls_integer': 'Int32', 'binary_integer': 'Int32',
  
  // ============ FLOAT TYPES ============
  'float': 'Float64', 'float4': 'Float32', 'float8': 'Float64',
  'real': 'Float32', 'double': 'Float64', 'double precision': 'Float64',
  'decimal': 'Float64', 'numeric': 'Float64', 'money': 'Float64',
  'newdecimal': 'Float64', 'smallmoney': 'Float64',
  'binary_float': 'Float32', 'binary_double': 'Float64',
  
  // ============ STRING TYPES ============
  'text': 'String', 'varchar': 'String', 'char': 'String',
  'character varying': 'String', 'character': 'String', 'bpchar': 'String',
  'name': 'String', 'uuid': 'String', 'json': 'String', 'jsonb': 'String',
  'xml': 'String', 'citext': 'String', 'inet': 'String', 'cidr': 'String', 'macaddr': 'String',
  'tinytext': 'String', 'mediumtext': 'String', 'longtext': 'String',
  'enum': 'String', 'set': 'String',
  'nvarchar': 'String', 'nchar': 'String', 'ntext': 'String',
  'uniqueidentifier': 'String', 'sql_variant': 'String', 'sysname': 'String',
  'varchar2': 'String', 'nvarchar2': 'String', 'clob': 'String', 'nclob': 'String',
  'long': 'String', 'rowid': 'String',
  
  // ============ DATE/TIME TYPES ============
  'date': 'Date', 'time': 'String', 'timetz': 'String', 'interval': 'String',
  'timestamp': 'DateTime', 'timestamptz': 'DateTime',
  'timestamp without time zone': 'DateTime', 'timestamp with time zone': 'DateTime',
  'datetime': 'DateTime', 'newdate': 'Date',
  'datetime2': 'DateTime', 'smalldatetime': 'DateTime', 'datetimeoffset': 'DateTime',
  'timestamp with local time zone': 'DateTime',
  
  // ============ BOOLEAN TYPES ============
  'boolean': 'UInt8', 'bool': 'UInt8', 'bit': 'UInt8',
  
  // ============ BINARY TYPES ============
  'bytea': 'String', 'blob': 'String', 'tinyblob': 'String', 
  'mediumblob': 'String', 'longblob': 'String',
  'binary': 'String', 'varbinary': 'String', 'image': 'String',
  'raw': 'String', 'long raw': 'String', 'bfile': 'String',
  
  // ============ GEOMETRY TYPES ============
  'geometry': 'String', 'geography': 'String', 'point': 'String',
  'linestring': 'String', 'polygon': 'String',
};

function mapSourceTypeToClickHouse(sourceType: string): string {
  if (!sourceType) return 'String';
  const normalized = sourceType.toLowerCase().replace(/\s+/g, ' ').trim();
  
  // Parantez iÃ§indeki deÄŸerleri temizle: varchar(255) â†’ varchar
  const baseType = normalized.split('(')[0].trim();
  
  return SQL_TO_CLICKHOUSE_TYPE[baseType] || SQL_TO_CLICKHOUSE_TYPE[normalized] || 'String';
}

// ============================================
// UI'dan sync tetiklendiÄŸinde tablo yoksa otomatik oluÅŸturur
// ============================================
async function ensureTableExists(dataset: any): Promise<void> {
  const tableName = dataset.clickhouse_table;
  
  try {
    // Tablo var mÄ± kontrol et
    const result = await clickhouse.query(`
      SELECT count() as cnt FROM system.tables 
      WHERE database = 'clixer_analytics' AND name = '${tableName}'
    `);
    
    const tableExists = result && result[0] && parseInt(result[0].cnt) > 0;
    
    if (tableExists) {
      logger.debug('Table already exists', { tableName });
      return;
    }
    
    logger.info('Table does not exist, creating automatically', { tableName, datasetId: dataset.id });
    
    // column_mapping'den tablo yapÄ±sÄ±nÄ± oluÅŸtur
    const columnMapping = dataset.column_mapping || [];
    
    if (columnMapping.length === 0) {
      throw new Error(`Dataset ${dataset.name} iÃ§in column_mapping bulunamadÄ±. Tablo oluÅŸturulamÄ±yor.`);
    }
    
    // KolonlarÄ± oluÅŸtur - KAPSAMLI TÄ°P DÃ–NÃœÅÃœM
    const columns = columnMapping.map((col: any) => {
      const targetName = col.target || col.source;
      
      // 1. Ã–nce UI'dan gelen clickhouseType'Ä± kullan
      let chType = col.clickhouseType;
      
      // 2. clickhouseType yoksa, kaynak tipten dÃ¶nÃ¼ÅŸtÃ¼r
      if (!chType || chType === 'String') {
        const sourceType = col.sourceType || col.type || '';
        chType = mapSourceTypeToClickHouse(sourceType);
      }
      
      // 3. Decimal tipi Float64 olarak sakla (SUM, AVG Ã§alÄ±ÅŸsÄ±n!)
      if (chType && chType.startsWith('Decimal')) {
        chType = 'Float64';
      }
      
      // 4. SayÄ±sal kolonlarÄ± algÄ±la (kolon adÄ±ndan - fallback)
      const numericPatterns = [
        'amount', 'tutar', 'price', 'fiyat', 'total', 'toplam', 
        'adet', 'quantity', 'miktar', 'indirim', 'discount', 
        'brut', 'net', 'gross', 'ucret', 'maas', 'salary', 'fee',
        'cost', 'maliyet', 'borc', 'alacak', 'bakiye', 'balance'
      ];
      if (chType === 'String' && numericPatterns.some(p => targetName.toLowerCase().includes(p))) {
        chType = 'Float64';
        logger.debug('Auto-detected numeric column from name', { column: targetName, type: chType });
      }
      
      return `${targetName} ${chType}`;
    });
    
    // ORDER BY iÃ§in uygun kolon bul
    // KRÄ°TÄ°K: Tarih kolonu (partition_column, reference_column) MUTLAKA dahil edilmeli!
    // Aksi halde ReplacingMergeTree farklÄ± tarihleri merge eder (1M satÄ±r â†’ 3500 satÄ±r!)
    
    // 1. Ã–nce unique kolon ara (id, code, pk vb.)
    const uniqueCandidates = ['id', 'code', 'kod', 'uuid', 'pk', 'primary_key', '_id'];
    const uniqueCol = columnMapping.find((c: any) => {
      const name = (c.target || c.source).toLowerCase();
      return uniqueCandidates.includes(name);
    });
    
    // 2. Tarih kolonu ara - partition_column, reference_column veya Date/DateTime tipi
    let dateColumn: string | null = null;
    
    // Dataset'ten partition_column veya reference_column al
    if (dataset.partition_column) {
      const partCol = columnMapping.find((c: any) => 
        c.source === dataset.partition_column || c.target === dataset.partition_column
      );
      if (partCol) dateColumn = partCol.target || partCol.source;
    }
    
    if (!dateColumn && dataset.reference_column) {
      const refCol = columnMapping.find((c: any) => 
        c.source === dataset.reference_column || c.target === dataset.reference_column
      );
      if (refCol) dateColumn = refCol.target || refCol.source;
    }
    
    // Date/DateTime tipi kolon ara
    if (!dateColumn) {
      const dateCol = columnMapping.find((c: any) => 
        c.clickhouseType === 'Date' || c.clickhouseType === 'DateTime'
      );
      if (dateCol) dateColumn = dateCol.target || dateCol.source;
    }
    
    // 3. ORDER BY oluÅŸtur
    let orderByColumn: string;
    
    if (uniqueCol) {
      // Unique kolon varsa - sadece onu kullan
      orderByColumn = uniqueCol.target || uniqueCol.source;
    } else if (dateColumn) {
      // Unique kolon yok ama tarih var - tarih + ilk 4 kolon
      const otherColumns = columnMapping
        .filter((c: any) => (c.target || c.source) !== dateColumn)
        .slice(0, 4)
        .map((c: any) => c.target || c.source);
      
      orderByColumn = [dateColumn, ...otherColumns].join(', ');
      logger.warn('No unique column found - using DATE + composite ORDER BY', { 
        dateColumn,
        orderByColumn,
        note: 'Date column added to prevent merge across dates'
      });
    } else {
      // Ne unique ne tarih var - ilk 5 kolon
      const allColumns = columnMapping.slice(0, 5).map((c: any) => c.target || c.source);
      orderByColumn = allColumns.length > 0 ? allColumns.join(', ') : '_synced_at';
      logger.warn('No unique or date column found - using composite ORDER BY', { 
        orderByColumn,
        note: 'Risk of data merge!'
      });
    }
    
    // Engine: ReplacingMergeTree kullan (duplicate Ã¶nleme!)
    const createSql = `
      CREATE TABLE IF NOT EXISTS clixer_analytics.${tableName} (
        ${columns.join(',\n        ')},
        _synced_at DateTime DEFAULT now()
      )
      ENGINE = ReplacingMergeTree(_synced_at)
      ORDER BY (${orderByColumn})
    `;
    
    logger.info('Auto-creating ClickHouse table', { tableName, sql: createSql.substring(0, 500) });
    await clickhouse.execute(createSql);
    logger.info('Table created successfully', { tableName });
    
  } catch (error: any) {
    logger.error('Failed to ensure table exists', { tableName, error: error.message });
    throw new Error(`ClickHouse tablo oluÅŸturulamadÄ±: ${error.message}`);
  }
}

// ============================================
// SQL SORGUDAN TABLO ADI Ã‡IKARICI
// ============================================
function extractTableFromQuery(query: string): string | null {
  if (!query) return null;
  
  // MSSQL formatÄ±: [schema].[table] veya [table]
  // Standard format: schema.table veya table
  
  // Ã–nce MSSQL kÃ¶ÅŸeli parantez formatÄ±nÄ± dene: [dbo].[transaction_items]
  const mssqlMatch = query.match(/\bFROM\s+\[?([a-zA-Z_][a-zA-Z0-9_]*)\]?\.\[?([a-zA-Z_][a-zA-Z0-9_]*)\]?/i);
  if (mssqlMatch) {
    // schema.table formatÄ±nda dÃ¶ndÃ¼r (kÃ¶ÅŸeli parantezler olmadan)
    return `${mssqlMatch[1]}.${mssqlMatch[2]}`;
  }
  
  // Sadece tablo adÄ± (kÃ¶ÅŸeli parantezli veya deÄŸil): [transaction_items] veya transaction_items
  const tableMatch = query.match(/\bFROM\s+\[?([a-zA-Z_][a-zA-Z0-9_]*)\]?(?:\s|$|;|,|\))/i);
  if (tableMatch) {
    return tableMatch[1];
  }
  
  return null;
}

// ============================================
// TÄ°P UYUMLULUK KONTROLÃœ (MUST!)
// ClickHouse tablo tipleri ile kaynak DB tipleri UYUMLU OLMAK ZORUNDA
// ============================================

interface TypeMismatch {
  column: string;
  sourceType: string;
  clickhouseType: string;
  compatible: boolean;
}

/**
 * SQL tipini ClickHouse tipine dÃ¶nÃ¼ÅŸtÃ¼r
 */
function sqlToClickHouseType(sqlType: string): string {
  if (!sqlType) return 'String';
  
  const normalized = sqlType.toLowerCase().trim();
  
  const typeMap: Record<string, string> = {
    // Integer types
    'int': 'Int32', 'int4': 'Int32', 'integer': 'Int32',
    'int2': 'Int16', 'smallint': 'Int16',
    'int8': 'Int64', 'bigint': 'Int64',
    'tinyint': 'Int8',
    'serial': 'Int32', 'bigserial': 'Int64',
    
    // Float types
    'float': 'Float64', 'float4': 'Float32', 'float8': 'Float64',
    'real': 'Float32', 'double': 'Float64', 'double precision': 'Float64',
    'decimal': 'Float64', 'numeric': 'Float64', 'money': 'Float64',
    
    // String types
    'text': 'String', 'varchar': 'String', 'char': 'String',
    'nvarchar': 'String', 'nchar': 'String', 'ntext': 'String',
    'uuid': 'String', 'uniqueidentifier': 'String',
    
    // Date/Time types
    'date': 'Date',
    'datetime': 'DateTime', 'datetime2': 'DateTime',
    'timestamp': 'DateTime', 'timestamptz': 'DateTime',
    
    // Boolean
    'boolean': 'UInt8', 'bool': 'UInt8', 'bit': 'UInt8',
  };
  
  // Parantez iÃ§ini kaldÄ±r (varchar(255) -> varchar)
  const baseType = normalized.replace(/\(.*\)/, '').trim();
  
  return typeMap[baseType] || typeMap[normalized] || 'String';
}

/**
 * Ä°ki ClickHouse tipi uyumlu mu kontrol et
 */
function areTypesCompatible(expected: string, actual: string): boolean {
  if (expected === actual) return true;
  
  // Int32 â†” Int64 uyumlu (upcast)
  if (expected.startsWith('Int') && actual.startsWith('Int')) return true;
  
  // Float32 â†” Float64 uyumlu
  if (expected.startsWith('Float') && actual.startsWith('Float')) return true;
  
  // String her ÅŸeyle uyumlu (en geniÅŸ tip)
  if (actual === 'String') return true;
  
  return false;
}

/**
 * Kaynak tablo ve ClickHouse tablo tiplerini karÅŸÄ±laÅŸtÄ±r
 * MUST: Sync baÅŸlamadan Ã¶nce Ã§aÄŸrÄ±lmalÄ±!
 */
async function validateTypeCompatibility(
  dataset: any, 
  connection: any
): Promise<{ valid: boolean; mismatches: TypeMismatch[]; warning: string | null }> {
  const mismatches: TypeMismatch[] = [];
  
  try {
    // 1. Kaynak tablo tiplerini al (column_mapping'den)
    const columnMapping = typeof dataset.column_mapping === 'string' 
      ? JSON.parse(dataset.column_mapping) 
      : dataset.column_mapping;
    
    if (!columnMapping || columnMapping.length === 0) {
      logger.warn('No column mapping found, skipping type validation');
      return { valid: true, mismatches: [], warning: null };
    }
    
    // 2. ClickHouse tablo var mÄ±?
    if (!dataset.clickhouse_table) {
      return { valid: true, mismatches: [], warning: null };
    }
    
    // 3. ClickHouse tablo tiplerini al
    const chColumns = await clickhouse.query(`
      DESCRIBE TABLE clixer_analytics.${dataset.clickhouse_table}
    `);
    
    if (!chColumns || chColumns.length === 0) {
      return { valid: true, mismatches: [], warning: null };
    }
    
    const chTypeMap: Record<string, string> = {};
    for (const col of chColumns) {
      chTypeMap[col.name] = col.type;
    }
    
    // 4. Her kolon iÃ§in tip karÅŸÄ±laÅŸtÄ±r
    for (const col of columnMapping) {
      const colName = col.target || col.targetName || col.source || col.sourceName;
      const sqlType = col.sqlType || col.sourceType || col.type || '';
      const expectedChType = col.clickhouseType || sqlToClickHouseType(sqlType);
      const actualChType = chTypeMap[colName];
      
      if (actualChType && !areTypesCompatible(expectedChType, actualChType)) {
        mismatches.push({
          column: colName,
          sourceType: expectedChType,
          clickhouseType: actualChType,
          compatible: false
        });
      }
    }
    
    if (mismatches.length > 0) {
      const warning = `Tip uyumsuzluÄŸu: ${mismatches.map(m => `${m.column}(${m.sourceType}â†’${m.clickhouseType})`).join(', ')}`;
      logger.error('ğŸš¨ TYPE MISMATCH DETECTED!', { mismatches, dataset: dataset.name });
      return { valid: false, mismatches, warning };
    }
    
    return { valid: true, mismatches: [], warning: null };
    
  } catch (error: any) {
    logger.warn('Type validation failed, proceeding with caution', { error: error.message });
    return { valid: true, mismatches: [], warning: `Tip kontrolÃ¼ yapÄ±lamadÄ±: ${error.message}` };
  }
}

// ============================================
// VERÄ° TUTARLILIK VE DUPLICATE Ã–NLEME SÄ°STEMÄ°
// ============================================

interface DataValidationResult {
  sourceCount: number;
  targetCount: number;
  isConsistent: boolean;
  duplicateCount: number;
  message: string;
}

/**
 * ReplacingMergeTree iÃ§in OPTIMIZE Ã§alÄ±ÅŸtÄ±r - Duplicate'larÄ± temizler
 * CRITICAL: Her sync sonrasÄ± Ã§aÄŸrÄ±lmalÄ±!
 */
async function optimizeTable(tableName: string): Promise<number> {
  try {
    // Ã–nce duplicate sayÄ±sÄ±nÄ± bul
    const beforeCount = await clickhouse.queryOne(
      `SELECT count() as cnt FROM clixer_analytics.${tableName}`
    );
    
    // OPTIMIZE FINAL - ReplacingMergeTree duplicate'larÄ± temizler
    await clickhouse.execute(`OPTIMIZE TABLE clixer_analytics.${tableName} FINAL`);
    
    // Sonraki sayÄ±yÄ± al
    const afterCount = await clickhouse.queryOne(
      `SELECT count() as cnt FROM clixer_analytics.${tableName}`
    );
    
    const removedDuplicates = (beforeCount?.cnt || 0) - (afterCount?.cnt || 0);
    
    if (removedDuplicates > 0) {
      logger.info('Duplicates removed by OPTIMIZE', { 
        tableName, 
        before: beforeCount?.cnt, 
        after: afterCount?.cnt, 
        removed: removedDuplicates 
      });
    }
    
    return removedDuplicates;
  } catch (error: any) {
    logger.warn('OPTIMIZE failed (non-critical)', { tableName, error: error.message });
    return 0;
  }
}

/**
 * Veri tutarlÄ±lÄ±k kontrolÃ¼
 * Kaynak ve hedef satÄ±r sayÄ±sÄ±nÄ± karÅŸÄ±laÅŸtÄ±rÄ±r
 */
async function validateDataConsistency(
  dataset: any, 
  sourceClient: any, 
  expectedRows: number
): Promise<DataValidationResult> {
  try {
    // ClickHouse'daki satÄ±r sayÄ±sÄ±
    const targetResult = await clickhouse.queryOne(
      `SELECT count() as cnt FROM clixer_analytics.${dataset.clickhouse_table}`
    );
    const targetCount = targetResult?.cnt || 0;
    
    // Duplicate kontrolÃ¼ (ORDER BY key'e gÃ¶re grup sayÄ±sÄ±)
    let duplicateCount = 0;
    try {
      // ReplacingMergeTree iÃ§in OPTIMIZE Ã§alÄ±ÅŸtÄ±r
      duplicateCount = await optimizeTable(dataset.clickhouse_table);
    } catch (e) {
      // Ignore
    }
    
    // Tolerans: %1 fark kabul edilebilir (bazÄ± kayÄ±tlar filtrelenmiÅŸ olabilir)
    const tolerance = Math.ceil(expectedRows * 0.01);
    const isConsistent = Math.abs(targetCount - expectedRows) <= tolerance;
    
    const result: DataValidationResult = {
      sourceCount: expectedRows,
      targetCount,
      isConsistent,
      duplicateCount,
      message: isConsistent 
        ? `âœ… Veri tutarlÄ±: ${targetCount} satÄ±r (beklenen: ${expectedRows})`
        : `âš ï¸ Veri uyumsuzluÄŸu: ${targetCount} satÄ±r (beklenen: ${expectedRows}, fark: ${Math.abs(targetCount - expectedRows)})`
    };
    
    logger.info('Data validation completed', result);
    return result;
  } catch (error: any) {
    return {
      sourceCount: expectedRows,
      targetCount: 0,
      isConsistent: false,
      duplicateCount: 0,
      message: `âŒ DoÄŸrulama hatasÄ±: ${error.message}`
    };
  }
}

/**
 * Partition bazlÄ± duplicate kontrolÃ¼
 * Date partition'larda aynÄ± gÃ¼nÃ¼n verileri tekrar etmemeli
 */
async function checkPartitionDuplicates(
  tableName: string, 
  partitionColumn: string,
  dateValue: string
): Promise<number> {
  try {
    // AynÄ± tarih iÃ§in kaÃ§ satÄ±r var?
    const result = await clickhouse.queryOne(`
      SELECT count() as cnt 
      FROM clixer_analytics.${tableName} 
      WHERE toDate(${partitionColumn}) = '${dateValue}'
    `);
    return result?.cnt || 0;
  } catch (error) {
    return 0;
  }
}

// ============================================
// DATASET LOCK MEKANÄ°ZMASI
// AynÄ± dataset iÃ§in aynÄ± anda sadece bir job Ã§alÄ±ÅŸabilir!
// ============================================
const LOCK_TTL = 3600; // 1 saat lock timeout (sonsuz Ã§alÄ±ÅŸmayÄ± Ã¶nler)

/**
 * Dataset iÃ§in lock al
 * @returns true eÄŸer lock alÄ±ndÄ±ysa, false eÄŸer zaten kilitliyse
 */
async function acquireDatasetLock(datasetId: string): Promise<boolean> {
  const lockKey = `etl:lock:${datasetId}`;
  try {
    // SETNX equivalent - sadece key yoksa set et
    const result = await cache.setNX(lockKey, JSON.stringify({
      pid: process.pid,
      startedAt: new Date().toISOString()
    }), LOCK_TTL);
    
    if (result) {
      logger.info('Dataset lock acquired', { datasetId });
      return true;
    } else {
      // Lock zaten var - kim tarafÄ±ndan alÄ±nmÄ±ÅŸ kontrol et
      const existing = await cache.get(lockKey);
      logger.warn('Dataset already locked (duplicate job prevention)', { 
        datasetId, 
        existingLock: existing 
      });
      return false;
    }
  } catch (error) {
    logger.error('Failed to acquire dataset lock', { datasetId, error });
    return false;
  }
}

/**
 * Dataset lock'Ä±nÄ± serbest bÄ±rak
 */
async function releaseDatasetLock(datasetId: string): Promise<void> {
  const lockKey = `etl:lock:${datasetId}`;
  try {
    await cache.del(lockKey);
    logger.info('Dataset lock released', { datasetId });
  } catch (error) {
    logger.error('Failed to release dataset lock', { datasetId, error });
  }
}

/**
 * Job iptal edildi mi kontrol et
 */
async function isJobCancelled(jobId: string): Promise<boolean> {
  const cancelKey = `etl:cancel:${jobId}`;
  const cancelled = await cache.get(cancelKey);
  return cancelled === 'true';
}

/**
 * Kill signal gÃ¶nder
 */
async function sendKillSignal(jobId: string): Promise<void> {
  const cancelKey = `etl:cancel:${jobId}`;
  await cache.set(cancelKey, 'true', 3600);
  logger.info('Kill signal sent', { jobId });
}

// Memory kullanÄ±mÄ±nÄ± kontrol et
function checkMemory(): { usedMB: number; ok: boolean } {
  const used = process.memoryUsage();
  const usedMB = Math.round(used.heapUsed / 1024 / 1024);
  const ok = usedMB < MAX_MEMORY_MB;
  if (!ok) {
    logger.warn('High memory usage detected', { usedMB, maxMB: MAX_MEMORY_MB });
  }
  return { usedMB, ok };
}

// Garbage collector'Ä± manual tetikle (varsa)
function forceGC() {
  if (global.gc) {
    global.gc();
    logger.debug('Manual garbage collection triggered');
  }
}

// ============================================
// ETL JOB PROCESSOR
// ============================================

interface ETLJob {
  datasetId: string;
  jobId?: string;
  action: 'initial_sync' | 'incremental_sync' | 'full_refresh' | 'manual_sync' | 'partial_refresh' | 'missing_sync' | 'new_records_sync';
  triggeredBy?: string;
  days?: number; // Partial refresh iÃ§in gÃ¼n sayÄ±sÄ±
  ranges?: Array<{start: number; end: number; missing_count?: number}>; // missing_sync iÃ§in eksik ID aralÄ±klarÄ±
  pkColumn?: string; // âš ï¸ KULLANICI SEÃ‡TÄ°ÄÄ° PK KOLONU - hardcoded deÄŸil!
  afterId?: number; // new_records_sync iÃ§in: Bu ID'den sonraki kayÄ±tlarÄ± Ã§ek
  limit?: number; // Opsiyonel satÄ±r limiti
}

async function processETLJob(job: ETLJob): Promise<void> {
  const startTime = Date.now();
  logger.info('Starting ETL job', job);

  try {
    // Dataset bilgisini al
    const dataset = await db.queryOne(
      'SELECT * FROM datasets WHERE id = $1',
      [job.datasetId]
    );

    if (!dataset) {
      throw new Error(`Dataset not found: ${job.datasetId}`);
    }

    // Connection bilgisini al
    const connection = await db.queryOne(
      'SELECT * FROM data_connections WHERE id = $1',
      [dataset.connection_id]
    );

    if (!connection) {
      throw new Error(`Connection not found: ${dataset.connection_id}`);
    }

    // ============================================
    // MUST: TÄ°P UYUMLULUK KONTROLÃœ
    // Sync baÅŸlamadan Ã¶nce kaynak ve hedef tipleri karÅŸÄ±laÅŸtÄ±r
    // ============================================
    const typeValidation = await validateTypeCompatibility(dataset, connection);
    if (!typeValidation.valid) {
      const errorMsg = `TÄ°P UYUMSUZLUÄU: ${typeValidation.warning}. Tabloyu silip yeniden oluÅŸturun.`;
      logger.error('ğŸš¨ Sync blocked due to type mismatch', { 
        dataset: dataset.name, 
        mismatches: typeValidation.mismatches 
      });
      
      // Job'Ä± failed olarak iÅŸaretle
      if (job.jobId) {
        await db.query(
          `UPDATE etl_jobs SET status = 'failed', completed_at = NOW(), error_message = $1 WHERE id = $2`,
          [errorMsg, job.jobId]
        );
      }
      throw new Error(errorMsg);
    }
    
    if (typeValidation.warning) {
      logger.warn('Type validation warning', { warning: typeValidation.warning });
    }

    // ETL job kaydÄ±: EÄŸer jobId varsa (data-service'ten geldiyse) onu kullan, yoksa yeni oluÅŸtur
    let etlJobId: string;
    
    if (job.jobId) {
      // data-service zaten job oluÅŸturdu, sadece running yap
      await db.query(
        `UPDATE etl_jobs SET status = 'running', started_at = NOW() WHERE id = $1`,
        [job.jobId]
      );
      etlJobId = job.jobId;
      logger.info('Using existing job from data-service', { jobId: job.jobId });
    } else {
      // Eski stil trigger veya initial_sync - yeni job oluÅŸtur
      // Ama Ã¶nce duplicate kontrolÃ¼ yap
      const existingJob = await db.queryOne(
        `SELECT id FROM etl_jobs WHERE dataset_id = $1 AND status IN ('pending', 'running') LIMIT 1`,
        [job.datasetId]
      );
      
      if (existingJob) {
        logger.warn('Duplicate job prevented in ETL Worker', { datasetId: job.datasetId, existingJobId: existingJob.id });
        return; // Duplicate job, atla
      }
      
      const etlJob = await db.queryOne(
        `INSERT INTO etl_jobs (tenant_id, dataset_id, action, status, started_at)
         VALUES ($1, $2, $3, 'running', NOW())
         RETURNING id`,
        [dataset.tenant_id, job.datasetId, job.action]
      );
      etlJobId = etlJob.id;
    }
    
    const etlJob = { id: etlJobId };

    let rowsProcessed = 0;

    try {
      // ============================================
      // INITIAL_SYNC - Ä°lk oluÅŸturmada SADECE test verisi yaz!
      // LIMIT'i KORUYARAK source_query'yi Ã§alÄ±ÅŸtÄ±r (10 satÄ±r)
      // Bu sayede ID-Based/Time-Based sync iÃ§in referans noktasÄ± olur
      // ============================================
      if (job.action === 'initial_sync') {
        logger.info('ğŸ§ª INITIAL SYNC - Test verisi yazÄ±lÄ±yor (LIMIT korunuyor)', { 
          datasetId: job.datasetId,
          sourceQuery: dataset.source_query?.substring(0, 100)
        });
        rowsProcessed = await initialTestSync(dataset, connection, etlJobId);
      }
      // ============================================
      // PARTIAL REFRESH - Ã–zel aksiyon (UI'dan gelen)
      // ============================================
      else if (job.action === 'partial_refresh') {
        logger.info('Executing partial refresh', { 
          datasetId: job.datasetId, 
          days: dataset.refresh_window_days,
          partitionColumn: dataset.partition_column 
        });
        rowsProcessed = await syncByDatePartition(dataset, connection, etlJobId);
      }
      // ============================================
      // MISSING SYNC - Sadece eksik ID aralÄ±klarÄ±nÄ± Ã§ek
      // ============================================
      else if (job.action === 'missing_sync') {
        logger.info('ğŸ” MISSING SYNC - Eksik ID aralÄ±klarÄ± Ã§ekiliyor', { 
          datasetId: job.datasetId,
          rangesCount: job.ranges?.length || 0,
          pkColumn: job.pkColumn || 'id'
        });
        rowsProcessed = await syncMissingRanges(dataset, connection, etlJobId, job.ranges || [], job.pkColumn || 'id');
      }
      // ============================================
      // ğŸš€ NEW RECORDS SYNC - Max ID'den sonraki yeni kayÄ±tlarÄ± Ã§ek
      // 100M+ tablolar iÃ§in en verimli yÃ¶ntem!
      // ============================================
      else if (job.action === 'new_records_sync') {
        const pkColumn = job.pkColumn || 'id';
        const afterId = job.afterId || 0;
        const limit = job.limit;
        
        logger.info('ğŸš€ NEW RECORDS SYNC - Sadece yeni kayÄ±tlar Ã§ekiliyor', { 
          datasetId: job.datasetId,
          pkColumn,
          afterId,
          limit: limit || 'UNLIMITED'
        });
        
        rowsProcessed = await syncNewRecordsAfterMaxId(dataset, connection, etlJobId, pkColumn, afterId, limit);
      } else {
        // Sync stratejisine gÃ¶re veri Ã§ek - jobId'yi geÃ§ir progress iÃ§in
        switch (dataset.sync_strategy) {
          case 'timestamp':
            rowsProcessed = await syncByTimestamp(dataset, connection, etlJobId);
            break;
          case 'id':
            rowsProcessed = await syncById(dataset, connection, etlJobId);
            break;
          case 'date_partition':
            rowsProcessed = await syncByDatePartition(dataset, connection, etlJobId);
            break;
          case 'date_delete_insert':
            rowsProcessed = await syncByDateDeleteInsert(dataset, connection, etlJobId);
            break;
          case 'full_refresh':
          default:
            rowsProcessed = await fullRefresh(dataset, connection, etlJobId);
        }
      }

      // Job baÅŸarÄ±lÄ±
      await db.query(
        `UPDATE etl_jobs SET status = 'completed', completed_at = NOW(), rows_processed = $1 WHERE id = $2`,
        [rowsProcessed, etlJob.id]
      );

      // Dataset son sync zamanÄ±nÄ± ve satÄ±r sayÄ±sÄ±nÄ± gÃ¼ncelle
      // ClickHouse'dan gerÃ§ek satÄ±r sayÄ±sÄ±nÄ± al
      let totalRows = rowsProcessed;
      try {
        if (dataset.clickhouse_table) {
          const countResult = await clickhouse.query(`SELECT count() as cnt FROM clixer_analytics.${dataset.clickhouse_table}`);
          totalRows = countResult[0]?.cnt || rowsProcessed;
        }
      } catch (e) {
        logger.warn('Could not get total rows from ClickHouse', { error: e });
      }
      
      await db.query(
        `UPDATE datasets SET last_sync_at = NOW(), last_sync_rows = $1, total_rows = $2, status = 'active' WHERE id = $3`,
        [rowsProcessed, totalRows, job.datasetId]
      );

      // Cache invalidate
      await cache.invalidate(`kpi:${dataset.clickhouse_table}:*`, 'etl');

      // Event yayÄ±nla
      await cache.publish('etl:completed', {
        datasetId: job.datasetId,
        rowsProcessed,
        duration: Date.now() - startTime
      });

      logger.info('ETL job completed', {
        datasetId: job.datasetId,
        rowsProcessed,
        duration: `${Date.now() - startTime}ms`
      });

    } catch (error: any) {
      // Job baÅŸarÄ±sÄ±z
      await db.query(
        `UPDATE etl_jobs SET status = 'failed', completed_at = NOW(), error_message = $1 WHERE id = $2`,
        [error.message, etlJob.id]
      );
      throw error;
    }

  } catch (error) {
    logger.error('ETL job failed', { job, error });
    throw error;
  }
}

// ============================================
// STREAMING POSTGRESQL SYNC
// 200M+ satÄ±r sorunsuz iÅŸlenir - bellek sabit kalÄ±r
// ============================================

const STREAM_BATCH_SIZE = 10000; // Cursor'dan bir seferde okunacak satÄ±r
const INSERT_BATCH_SIZE = 5000;  // ClickHouse'a yazÄ±lacak batch boyutu

async function streamingPostgreSQLSync(
  dataset: any, 
  connection: any, 
  columnMapping: any[],
  jobId?: string
): Promise<number> {
  const Cursor = require('pg-cursor');
  const { Client } = require('pg');
  
  const client = new Client({
    host: connection.host,
    port: connection.port || 5432,
    database: connection.database_name,
    user: connection.username,
    password: connection.password_encrypted,
  });
  
  await client.connect();
  
  // Source query oluÅŸtur - LIMIT YOK! Streaming ile hepsini alacaÄŸÄ±z
  let query = dataset.source_query || `SELECT * FROM ${dataset.source_table}`;
  
  // â— KRITIK: Dataset oluÅŸturulurken LIMIT 10 ile test edilmiÅŸ olabilir!
  // Full Refresh'te TÃœM veriyi Ã§ekmek iÃ§in LIMIT'i kaldÄ±r
  query = query.replace(/\s+LIMIT\s+\d+\s*/gi, ' ').trim();
  
  // Custom WHERE koÅŸulu varsa ekle (Full Refresh iÃ§in kullanÄ±cÄ± tanÄ±mlÄ± filtre)
  const customWhere = dataset.custom_where;
  if (customWhere && customWhere.trim()) {
    // Sorgunun zaten WHERE iÃ§erip iÃ§ermediÄŸini kontrol et
    if (query.toUpperCase().includes(' WHERE ')) {
      // Varsa AND ile ekle
      query = `${query} AND (${customWhere})`;
    } else {
      // Yoksa WHERE ekle
      query = `${query} WHERE ${customWhere}`;
    }
    logger.info('Custom WHERE applied', { customWhere });
  }
  
  // Ama row_limit varsa ona uyalÄ±m
  const rowLimit = dataset.row_limit || null; // null = sÄ±nÄ±rsÄ±z
  
  logger.info('ğŸš€ STREAMING ETL starting', { 
    datasetId: dataset.id,
    table: dataset.clickhouse_table,
    query: query.substring(0, 100),
    customWhere: customWhere || 'none',
    rowLimit: rowLimit || 'UNLIMITED',
    streamBatchSize: STREAM_BATCH_SIZE
  });
  
  // Cursor oluÅŸtur
  const cursor = client.query(new Cursor(query));
  
  let totalInserted = 0;
  let batchNumber = 0;
  let insertBuffer: any[] = [];
  let columns: string[] = [];
  
  // ClickHouse tablosunu truncate et
  try {
    await clickhouse.execute(`TRUNCATE TABLE clixer_analytics.${dataset.clickhouse_table}`);
    logger.info('Truncated ClickHouse table', { table: dataset.clickhouse_table });
  } catch (truncErr: any) {
    logger.warn('Could not truncate, trying ALTER DELETE', { error: truncErr.message });
    await clickhouse.execute(`ALTER TABLE clixer_analytics.${dataset.clickhouse_table} DELETE WHERE 1=1`);
  }
  
  // Streaming okuma dÃ¶ngÃ¼sÃ¼
  const readBatch = (): Promise<any[]> => {
    return new Promise((resolve, reject) => {
      cursor.read(STREAM_BATCH_SIZE, (err: any, rows: any[]) => {
        if (err) reject(err);
        else resolve(rows);
      });
    });
  };
  
  try {
    while (true) {
      // Kill check
      if (jobId && await isJobCancelled(jobId)) {
        logger.info('Job cancelled during streaming', { jobId, totalInserted });
        break;
      }
      
      // Batch oku
      const rows = await readBatch();
      
      if (rows.length === 0) {
        // Veri bitti
        break;
      }
      
      batchNumber++;
      
      // Ä°lk batch'te column mapping oluÅŸtur (eÄŸer yoksa)
      if (batchNumber === 1 && columnMapping.length === 0 && rows.length > 0) {
        const firstRow = rows[0];
        for (const key of Object.keys(firstRow)) {
          const value = firstRow[key];
          let clickhouseType = 'String';
          if (typeof value === 'number') {
            clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
          } else if (typeof value === 'boolean') {
            clickhouseType = 'UInt8';
          }
          columnMapping.push({
            source: key,
            target: key.replace(/[^a-zA-Z0-9_]/g, '_'),
            clickhouseType
          });
        }
        columns = columnMapping.map((m: any) => m.target || m.targetName);
      }
      
      if (columns.length === 0) {
        columns = columnMapping.map((m: any) => m.target || m.targetName);
      }
      
      // SatÄ±rlarÄ± dÃ¶nÃ¼ÅŸtÃ¼r ve buffer'a ekle
      for (const row of rows) {
        const transformed: any = {};
        for (const mapping of columnMapping) {
          const sourceCol = mapping.source || mapping.sourceName;
          const targetCol = mapping.target || mapping.targetName;
          let value = row[sourceCol];
          
          if (value === null || value === undefined) {
            if (mapping.clickhouseType?.includes('Int') || mapping.clickhouseType?.includes('Decimal')) {
              value = 0;
            } else if (mapping.clickhouseType === 'Date') {
              value = '1970-01-01';
            } else {
              value = '';
            }
          }
          
          transformed[targetCol] = value;
        }
        insertBuffer.push(transformed);
        
        // Buffer doluysa ClickHouse'a yaz
        if (insertBuffer.length >= INSERT_BATCH_SIZE) {
          await insertToClickHouse(dataset.clickhouse_table, columns, insertBuffer);
          totalInserted += insertBuffer.length;
          insertBuffer = []; // Buffer'Ä± temizle - bellek serbest
          
          // Her 50K satÄ±rda progress log
          if (totalInserted % 50000 === 0) {
            const memMB = Math.round(process.memoryUsage().heapUsed / 1024 / 1024);
            logger.info('ğŸ“Š Streaming progress', { 
              totalInserted: totalInserted.toLocaleString(),
              memoryMB: memMB,
              batchNumber
            });
            
            // GC tetikle
            forceGC();
          }
        }
        
        // Row limit kontrolÃ¼
        if (rowLimit && totalInserted + insertBuffer.length >= rowLimit) {
          break;
        }
      }
      
      // Row limit kontrolÃ¼ (dÄ±ÅŸ dÃ¶ngÃ¼ iÃ§in)
      if (rowLimit && totalInserted + insertBuffer.length >= rowLimit) {
        break;
      }
    }
    
    // Kalan buffer'Ä± yaz
    if (insertBuffer.length > 0) {
      await insertToClickHouse(dataset.clickhouse_table, columns, insertBuffer);
      totalInserted += insertBuffer.length;
    }
    
  } finally {
    // Cursor ve client'Ä± kapat
    await new Promise<void>((resolve) => cursor.close(() => resolve()));
    await client.end();
    forceGC();
  }
  
  const memMB = Math.round(process.memoryUsage().heapUsed / 1024 / 1024);
  logger.info('âœ… STREAMING ETL completed', { 
    datasetId: dataset.id, 
    totalInserted: totalInserted.toLocaleString(),
    finalMemoryMB: memMB
  });
  
  return totalInserted;
}

/**
 * ClickHouse'a batch insert
 * DateTime formatÄ±: YYYY-MM-DD HH:MM:SS
 * AkÄ±llÄ± tarih dÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ ile tÃ¼m formatlar desteklenir
 */
async function insertToClickHouse(tableName: string, columns: string[], rows: any[]): Promise<void> {
  if (rows.length === 0) return;
  
  const values = rows.map(row => {
    const vals = columns.map((col: string) => {
      const val = row[col];
      
      // Tarih/DateTime kontrolÃ¼ - Ã¶nce akÄ±llÄ± dÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼yÃ¼ dene
      if (val instanceof Date || (typeof val === 'string' && val.match(/\d{4}[-\/]\d{2}[-\/]\d{2}|\d{2}[-\/]\d{2}[-\/]\d{4}/))) {
        const converted = toClickHouseDateTime(val);
        if (converted) return `'${converted}'`;
        return 'NULL';
      }
      
      if (typeof val === 'string') {
        return `'${val.replace(/\\/g, '\\\\').replace(/'/g, "''")}'`;
      } else if (val === null || val === undefined) {
        return 'NULL';
      }
      return val;
    });
    return `(${vals.join(', ')})`;
  }).join(',\n');
  
  const sql = `INSERT INTO clixer_analytics.${tableName} (${columns.join(', ')}) VALUES ${values}`;
  await clickhouse.execute(sql);
}

// ============================================
// INITIAL TEST SYNC - Ä°lk oluÅŸturmada LIMIT'i KORUYARAK test verisi yaz
// Bu fonksiyon sadece dataset ilk oluÅŸturulduÄŸunda Ã§alÄ±ÅŸÄ±r
// LIMIT 10 ile gelen sorguyu OLDUÄU GÄ°BÄ° Ã§alÄ±ÅŸtÄ±rÄ±r
// BÃ¶ylece ID-Based/Time-Based sync iÃ§in referans noktasÄ± oluÅŸur
// ============================================
async function initialTestSync(dataset: any, connection: any, jobId?: string): Promise<number> {
  logger.info('ğŸ§ª Initial Test Sync starting - LIMIT korunuyor!', { 
    datasetId: dataset.id,
    table: dataset.clickhouse_table,
    connectionType: connection.type
  });
  
  try {
    // ğŸ”§ SELF-HEALING: Tablo yoksa otomatik oluÅŸtur
    await ensureTableExists(dataset);
    
    const columnMapping = dataset.column_mapping || [];
    
    // â— KRÄ°TÄ°K: source_query'yi OLDUÄU GÄ°BÄ° kullan - LIMIT KALDIRMA!
    const query = dataset.source_query || `SELECT * FROM ${dataset.source_table} LIMIT 10`;
    
    logger.info('Initial test query (LIMIT korunuyor)', { query: query.substring(0, 200) });
    
    let rows: any[] = [];
    
    // BaÄŸlantÄ± tipine gÃ¶re veri Ã§ek
    if (connection.type === 'postgresql') {
      const { Client } = require('pg');
      const client = new Client({
        host: connection.host,
        port: connection.port || 5432,
        database: connection.database_name,
        user: connection.username,
        password: connection.password_encrypted,
      });
      await client.connect();
      const result = await client.query(query);
      rows = result.rows;
      await client.end();
    } 
    else if (connection.type === 'mysql') {
      const mysql = require('mysql2/promise');
      const conn = await mysql.createConnection({
        host: connection.host,
        port: connection.port || 3306,
        database: connection.database_name,
        user: connection.username,
        password: connection.password_encrypted,
        charset: 'utf8mb4',
        dateStrings: true
      });
      const [result] = await conn.query(query);
      rows = result as any[];
      await conn.end();
    }
    else if (connection.type === 'mssql') {
      const mssql = require('mssql');
      const isAzure = connection.host.includes('.database.windows.net');
      const pool = await mssql.connect({
        server: connection.host,
        port: connection.port || 1433,
        database: connection.database_name,
        user: connection.username,
        password: connection.password_encrypted,
        options: { encrypt: isAzure, trustServerCertificate: !isAzure }
      });
      const result = await pool.request().query(query);
      rows = result.recordset;
      await pool.close();
    }
    
    logger.info('Initial test data fetched', { rowCount: rows.length });
    
    if (rows.length === 0) {
      logger.warn('No test data fetched - table will be empty');
      return 0;
    }
    
    // ClickHouse'a yaz (TRUNCATE YOK - tablo zaten boÅŸ)
    const insertData = rows.map((row: any) => {
      const obj: any = {};
      for (const col of columnMapping) {
        const sourceCol = col.source || col.sourceName;
        const targetCol = col.target || col.targetName;
        let val = row[sourceCol];
        
        // DateTime dÃ¶nÃ¼ÅŸÃ¼mÃ¼
        if (col.clickhouseType === 'DateTime' || col.clickhouseType === 'Date') {
          val = toClickHouseDateTime(val) || '1970-01-01 00:00:00';
        }
        
        // Null handling
        if (val === null || val === undefined) {
          if (col.clickhouseType === 'String') val = '';
          else if (col.clickhouseType?.includes('Int') || col.clickhouseType?.includes('Float')) val = 0;
          else val = '';
        }
        
        obj[targetCol] = val;
      }
      return obj;
    });
    
    await clickhouse.insert(`clixer_analytics.${dataset.clickhouse_table}`, insertData);
    
    logger.info('âœ… Initial test sync completed', { 
      datasetId: dataset.id,
      rowsInserted: rows.length 
    });
    
    return rows.length;
    
  } catch (error: any) {
    logger.error('Initial test sync failed', { 
      datasetId: dataset.id, 
      error: error.message 
    });
    throw error;
  }
}

// ============================================
// SYNC STRATEGIES
// ============================================

async function syncByTimestamp(dataset: any, connection: any, jobId?: string): Promise<number> {
  // ğŸ”§ SELF-HEALING: Tablo yoksa otomatik oluÅŸtur
  await ensureTableExists(dataset);
  
  const {
    reference_column,
    clickhouse_table,
    source_query,
    source_table,
    row_limit,
    last_sync_value
  } = dataset;

  logger.info('Timestamp-based sync starting', { 
    datasetId: dataset.id, 
    datasetName: dataset.name,
    referenceColumn: reference_column,
    lastSyncValue: last_sync_value,
    clickhouseTable: clickhouse_table
  });

  // Referans kolon kontrolÃ¼
  if (!reference_column) {
    logger.warn('No reference column defined, falling back to full refresh', { datasetId: dataset.id });
    return await fullRefresh(dataset, connection, jobId);
  }

  // Source kontrolÃ¼
  const sourceTableName = extractTableFromQuery(source_query || `SELECT * FROM ${source_table}`);
  if (!sourceTableName) {
    throw new Error('Kaynak tablo bulunamadÄ±');
  }

  try {
    // Son sync deÄŸerini al (yoksa tÃ¼m veriyi Ã§ek)
    let lastValue = last_sync_value;
    
    // EÄŸer last_sync_value yoksa, ClickHouse'tan en son deÄŸeri al
    if (!lastValue) {
      try {
        const maxResult = await clickhouse.query(`
          SELECT max(${reference_column}) as max_val 
          FROM clixer_analytics.${clickhouse_table}
        `);
        lastValue = maxResult[0]?.max_val || null;
        logger.info('Got max timestamp from ClickHouse', { lastValue });
      } catch (e) {
        lastValue = null;
      }
    }

    let totalInserted = 0;

    // column_mapping'i parse et
    let columnMapping = dataset.column_mapping || [];
    if (typeof columnMapping === 'string') {
      try {
        columnMapping = JSON.parse(columnMapping);
      } catch (e) {
        columnMapping = [];
      }
    }

    if (connection.type === 'mssql') {
      const columns = columnMapping.map((c: any) => c.source).join(', ') || '*';
      let whereClause = '';
      
      if (lastValue) {
        // MSSQL datetime formatÄ±
        whereClause = `WHERE ${reference_column} > '${lastValue}'`;
      }
      
      const limit = row_limit || 10000000;
      const query = `SELECT TOP ${limit} ${columns} FROM ${sourceTableName} ${whereClause} ORDER BY ${reference_column}`;
      
      logger.info('Executing MSSQL timestamp query', { 
        query: query.substring(0, 300),
        lastValue,
        limit
      });

      const originalQuery = dataset.source_query;
      dataset.source_query = query;
      try {
        totalInserted = await mssqlSync(dataset, connection, columnMapping, jobId);
      } finally {
        dataset.source_query = originalQuery;
      }
      
    } else if (connection.type === 'postgresql') {
      const columns = columnMapping.map((c: any) => c.source).join(', ') || '*';
      let whereClause = '';
      
      if (lastValue) {
        whereClause = `WHERE ${reference_column} > '${lastValue}'`;
      }
      
      const limit = row_limit || 10000000;
      const query = `SELECT ${columns} FROM ${sourceTableName} ${whereClause} ORDER BY ${reference_column} LIMIT ${limit}`;
      
      logger.info('Executing PostgreSQL timestamp query', { query: query.substring(0, 300) });
      
      totalInserted = await streamingPostgreSQLSync(dataset, connection, columnMapping, jobId);
      
    } else if (connection.type === 'mysql') {
      const columns = columnMapping.map((c: any) => c.source).join(', ') || '*';
      let whereClause = '';
      
      if (lastValue) {
        whereClause = `WHERE ${reference_column} > '${lastValue}'`;
      }
      
      const limit = row_limit || 10000000;
      const query = `SELECT ${columns} FROM ${sourceTableName} ${whereClause} ORDER BY ${reference_column} LIMIT ${limit}`;
      
      logger.info('Executing MySQL timestamp query', { query: query.substring(0, 300) });
      
      totalInserted = await mysqlSync(dataset, connection, columnMapping, jobId);
      
    } else {
      logger.warn('Unsupported connection type for timestamp sync, falling back to full refresh', { 
        type: connection.type 
      });
      return await fullRefresh(dataset, connection, jobId);
    }

    // Son sync deÄŸerini gÃ¼ncelle
    if (totalInserted > 0) {
      try {
        const newMaxResult = await clickhouse.query(`
          SELECT max(${reference_column}) as max_val 
          FROM clixer_analytics.${clickhouse_table}
        `);
        const newMaxValue = newMaxResult[0]?.max_val;
        
        if (newMaxValue) {
          await db.query(
            'UPDATE datasets SET last_sync_value = $1, last_sync_at = NOW() WHERE id = $2',
            [newMaxValue, dataset.id]
          );
          logger.info('Updated last_sync_value', { newMaxValue });
        }
      } catch (e: any) {
        logger.warn('Could not update last_sync_value', { error: e.message });
      }
    }

    // OPTIMIZE Ã§alÄ±ÅŸtÄ±r
    await clickhouse.execute(`OPTIMIZE TABLE clixer_analytics.${clickhouse_table} FINAL`);
    
    logger.info('Timestamp-based sync completed', { 
      datasetId: dataset.id,
      totalInserted,
      lastValue
    });

    return totalInserted;
    
  } catch (error: any) {
    logger.error('Timestamp-based sync failed', { 
      datasetId: dataset.id,
      error: error.message 
    });
    throw error;
  }
}

async function syncById(dataset: any, connection: any, jobId?: string): Promise<number> {
  // ğŸ”§ SELF-HEALING: Tablo yoksa otomatik oluÅŸtur
  await ensureTableExists(dataset);
  
  const { clickhouse_table, source_table, source_query, reference_column, row_limit } = dataset;
  
  // Referans kolon kontrolÃ¼
  if (!reference_column) {
    logger.warn('No reference column for ID-based sync, falling back to full refresh', { datasetId: dataset.id });
    return await fullRefresh(dataset, connection, jobId);
  }
  
  // Kaynak tablo veya sorgu kontrolÃ¼
  const sourceTableName = source_table || (source_query ? extractTableFromQuery(source_query) : null);
  if (!sourceTableName) {
    logger.warn('No source table or query for ID-based sync, falling back to full refresh', { datasetId: dataset.id });
    return await fullRefresh(dataset, connection, jobId);
  }
  
  logger.info('ID-Based incremental sync starting', { 
    datasetId: dataset.id, 
    table: clickhouse_table,
    sourceTable: sourceTableName,
    referenceColumn: reference_column
  });
  
  try {
    // 1. ClickHouse'tan mevcut max ID'yi al
    // NOT: ID kolonu String olabilir, bu yÃ¼zden toInt64OrZero ile sayÄ±ya Ã§eviriyoruz!
    let maxId = 0;
    try {
      const maxResult = await clickhouse.queryOne(`
        SELECT max(toInt64OrZero(toString(${reference_column}))) as max_id FROM clixer_analytics.${clickhouse_table}
      `);
      maxId = parseInt(maxResult?.max_id || '0') || 0;
      logger.info('Current max ID in ClickHouse (converted to Int64)', { maxId, table: clickhouse_table });
    } catch (e: any) {
      logger.warn('Could not get max ID, will fetch all', { error: e.message });
    }
    
    // 2. Kaynaktan sadece yeni kayÄ±tlarÄ± Ã§ek (WHERE id > maxId)
    // âš ï¸ row_limit string olarak gelebilir, integer'a Ã§evir!
    const limit = parseInt(String(row_limit)) || 10000000;
    let totalInserted = 0;
    
    if (connection.type === 'mssql') {
      const sql = require('mssql');
      const isAzure = connection.host?.includes('.database.windows.net');
      
      const config = {
        user: connection.username,
        password: connection.password_encrypted,
        server: connection.host,
        database: connection.database_name,
        port: connection.port || 1433,
        options: {
          encrypt: isAzure,
          trustServerCertificate: !isAzure
        },
        requestTimeout: 600000 // 10 dakika
      };
      
      const pool = await sql.connect(config);
      
      // ============================================
      // CURSOR/PAGINATION MANTIÄI - 5000'erlik parÃ§alar
      // Bellek dolmaz, milyonlarca satÄ±r Ã§ekilebilir!
      // ============================================
      const BATCH_SIZE = 5000;
      let currentMaxId = maxId;
      let lastFetchedId = maxId;
      let hasMoreData = true;
      
      logger.info('MSSQL ID-based sync with cursor starting', { 
        startMaxId: maxId, 
        batchSize: BATCH_SIZE,
        rowLimit: limit 
      });
      
      while (hasMoreData && (limit === null || totalInserted < limit)) {
        const remainingLimit = limit ? Math.min(BATCH_SIZE, limit - totalInserted) : BATCH_SIZE;
        const query = `SELECT TOP ${remainingLimit} * FROM ${sourceTableName} WHERE ${reference_column} > ${currentMaxId} ORDER BY ${reference_column}`;
        
        const result = await pool.request().query(query);
        const rows = result.recordset;
        
        if (rows.length === 0) {
          hasMoreData = false;
          logger.info('No more rows to fetch', { currentMaxId, totalInserted });
          break;
        }
        
        // ClickHouse'a yaz
        const transformedBatch = transformBatchForClickHouse(rows);
        await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedBatch);
        totalInserted += rows.length;
        
        // Sonraki parÃ§a iÃ§in max ID gÃ¼ncelle
        lastFetchedId = rows[rows.length - 1][reference_column];
        currentMaxId = lastFetchedId;
        
        // Progress gÃ¼ncelle
        if (jobId) {
          await db.query(
            `UPDATE etl_jobs SET rows_processed = $1 WHERE id = $2`,
            [totalInserted, jobId]
          );
        }
        
        logger.info('MSSQL cursor batch inserted', { 
          batchSize: rows.length, 
          totalInserted, 
          currentMaxId
        });
        
        if (limit && totalInserted >= limit) {
          hasMoreData = false;
        }
      }
      
      await pool.close();
      
      if (totalInserted > 0) {
        await db.query(
          `UPDATE datasets SET last_sync_value = $1, last_sync_at = NOW() WHERE id = $2`,
          [String(lastFetchedId), dataset.id]
        );
        logger.info('Updated last_sync_value', { newMaxId: lastFetchedId, totalInserted });
      }
      
    } else if (connection.type === 'mysql') {
      const mysql = require('mysql2/promise');
      
      const conn = await mysql.createConnection({
        host: connection.host,
        port: connection.port || 3306,
        user: connection.username,
        password: connection.password_encrypted,
        database: connection.database_name,
        charset: 'utf8mb4',
        dateStrings: true
      });
      
      // ============================================
      // CURSOR/PAGINATION MANTIÄI - 5000'erlik parÃ§alar
      // Bellek dolmaz, milyonlarca satÄ±r Ã§ekilebilir!
      // ============================================
      const BATCH_SIZE = 5000;
      // âš ï¸ maxId integer olmalÄ± - MySQL prepared statement iÃ§in!
      let currentMaxId: number = parseInt(String(maxId)) || 0;
      let lastFetchedId: number = currentMaxId;
      let hasMoreData = true;
      
      logger.info('MySQL ID-based sync with cursor starting', { 
        startMaxId: currentMaxId, 
        batchSize: BATCH_SIZE,
        rowLimit: limit 
      });
      
      while (hasMoreData && totalInserted < limit) {
        // Her seferinde sadece 5000 satÄ±r Ã§ek
        const remainingLimit: number = Math.min(BATCH_SIZE, limit - totalInserted);
        
        // âš ï¸ MySQL prepared statement sorunlarÄ± nedeniyle deÄŸerleri direkt sorguya yazÄ±yoruz
        const query = `SELECT * FROM ${sourceTableName} WHERE ${reference_column} > ${currentMaxId} ORDER BY ${reference_column} LIMIT ${remainingLimit}`;
        logger.info('MySQL ID-based query', { query: query.substring(0, 200), currentMaxId, remainingLimit });
        const [rows] = await conn.query(query); // execute yerine query kullan!
        
        if ((rows as any[]).length === 0) {
          hasMoreData = false;
          logger.info('No more rows to fetch', { currentMaxId, totalInserted });
          break;
        }
        
        // ClickHouse'a yaz
        const transformedBatch = transformBatchForClickHouse(rows as any[]);
        await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedBatch);
        totalInserted += (rows as any[]).length;
        
        // Sonraki parÃ§a iÃ§in max ID gÃ¼ncelle
        // âš ï¸ Integer'a Ã§evir - MySQL prepared statement iÃ§in!
        lastFetchedId = parseInt(String((rows as any[])[(rows as any[]).length - 1][reference_column])) || 0;
        currentMaxId = lastFetchedId;
        
        // Progress gÃ¼ncelle
        if (jobId) {
          await db.query(
            `UPDATE etl_jobs SET rows_processed = $1 WHERE id = $2`,
            [totalInserted, jobId]
          );
        }
        
        logger.info('MySQL cursor batch inserted', { 
          batchSize: (rows as any[]).length, 
          totalInserted, 
          currentMaxId,
          hasMoreData: (rows as any[]).length === remainingLimit
        });
        
        // EÄŸer limit'e ulaÅŸtÄ±ysak dur
        if (limit && totalInserted >= limit) {
          hasMoreData = false;
        }
      }
      
      await conn.end();
      
      if (totalInserted > 0) {
        await db.query(
          `UPDATE datasets SET last_sync_value = $1, last_sync_at = NOW() WHERE id = $2`,
          [String(lastFetchedId), dataset.id]
        );
        logger.info('Updated last_sync_value', { newMaxId: lastFetchedId, totalInserted });
      }
      
    } else if (connection.type === 'postgresql') {
      const { Client } = require('pg');
      const client = new Client({
        host: connection.host,
        port: connection.port || 5432,
        user: connection.username,
        password: connection.password_encrypted,
        database: connection.database_name
      });
      
      await client.connect();
      
      // ============================================
      // CURSOR/PAGINATION MANTIÄI - 5000'erlik parÃ§alar
      // Bellek dolmaz, milyonlarca satÄ±r Ã§ekilebilir!
      // ============================================
      const BATCH_SIZE = 5000;
      let currentMaxId = maxId;
      let lastFetchedId = maxId;
      let hasMoreData = true;
      
      logger.info('PostgreSQL ID-based sync with cursor starting', { 
        startMaxId: maxId, 
        batchSize: BATCH_SIZE,
        rowLimit: limit 
      });
      
      while (hasMoreData && (limit === null || totalInserted < limit)) {
        const remainingLimit = limit ? Math.min(BATCH_SIZE, limit - totalInserted) : BATCH_SIZE;
        const query = `SELECT * FROM ${sourceTableName} WHERE ${reference_column} > $1 ORDER BY ${reference_column} LIMIT $2`;
        const result = await client.query(query, [currentMaxId, remainingLimit]);
        
        if (result.rows.length === 0) {
          hasMoreData = false;
          logger.info('No more rows to fetch', { currentMaxId, totalInserted });
          break;
        }
        
        // ClickHouse'a yaz
        const transformedBatch = transformBatchForClickHouse(result.rows);
        await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedBatch);
        totalInserted += result.rows.length;
        
        // Sonraki parÃ§a iÃ§in max ID gÃ¼ncelle
        lastFetchedId = result.rows[result.rows.length - 1][reference_column];
        currentMaxId = lastFetchedId;
        
        // Progress gÃ¼ncelle
        if (jobId) {
          await db.query(
            `UPDATE etl_jobs SET rows_processed = $1 WHERE id = $2`,
            [totalInserted, jobId]
          );
        }
        
        logger.info('PostgreSQL cursor batch inserted', { 
          batchSize: result.rows.length, 
          totalInserted, 
          currentMaxId
        });
        
        if (limit && totalInserted >= limit) {
          hasMoreData = false;
        }
      }
      
      await client.end();
      
      if (totalInserted > 0) {
        await db.query(
          `UPDATE datasets SET last_sync_value = $1, last_sync_at = NOW() WHERE id = $2`,
          [String(lastFetchedId), dataset.id]
        );
        logger.info('Updated last_sync_value', { newMaxId: lastFetchedId, totalInserted });
      }
    }
    
    // ============================================
    // COUNT VERIFICATION - Kaynak vs Hedef KarÅŸÄ±laÅŸtÄ±rma
    // ============================================
    let verificationWarning: string | null = null;
    try {
      // ClickHouse'taki toplam satÄ±r sayÄ±sÄ±
      const chCountResult = await clickhouse.queryOne(`
        SELECT count() as total FROM clixer_analytics.${clickhouse_table}
      `);
      const clickhouseCount = parseInt(chCountResult?.total || '0');
      
      // Kaynak DB'deki toplam satÄ±r sayÄ±sÄ± (sadece count, hÄ±zlÄ±)
      let sourceCount = 0;
      const sourceTableName = dataset.source_table || (dataset.source_query ? extractTableFromQuery(dataset.source_query) : null);
      
      if (sourceTableName && connection.type === 'mssql') {
        const sql = require('mssql');
        const isAzure = connection.host?.includes('.database.windows.net');
        const config = {
          user: connection.username,
          password: connection.password_encrypted,
          server: connection.host,
          database: connection.database_name,
          port: connection.port || 1433,
          options: { encrypt: isAzure, trustServerCertificate: !isAzure },
          requestTimeout: 60000
        };
        const pool = await sql.connect(config);
        const countResult = await pool.request().query(`SELECT COUNT(*) as cnt FROM ${sourceTableName}`);
        sourceCount = countResult.recordset[0].cnt;
        await pool.close();
      } else if (sourceTableName && connection.type === 'mysql') {
        const mysql = require('mysql2/promise');
        const conn = await mysql.createConnection({
          host: connection.host,
          port: connection.port || 3306,
          user: connection.username,
          password: connection.password_encrypted,
          database: connection.database_name
        });
        const [rows] = await conn.execute(`SELECT COUNT(*) as cnt FROM ${sourceTableName}`);
        sourceCount = (rows as any[])[0].cnt;
        await conn.end();
      } else if (sourceTableName && connection.type === 'postgresql') {
        const { Client } = require('pg');
        const client = new Client({
          host: connection.host,
          port: connection.port || 5432,
          user: connection.username,
          password: connection.password_encrypted,
          database: connection.database_name
        });
        await client.connect();
        const result = await client.query(`SELECT COUNT(*) as cnt FROM ${sourceTableName}`);
        sourceCount = parseInt(result.rows[0].cnt);
        await client.end();
      }
      
      // KarÅŸÄ±laÅŸtÄ±r
      const diff = sourceCount - clickhouseCount;
      const diffPercent = sourceCount > 0 ? Math.round((diff / sourceCount) * 100) : 0;
      
      logger.info('ğŸ“Š Count Verification', { 
        source: sourceCount, 
        clickhouse: clickhouseCount, 
        diff, 
        diffPercent: `${diffPercent}%` 
      });
      
      // %1'den fazla fark varsa uyarÄ±
      if (Math.abs(diffPercent) > 1 && Math.abs(diff) > 1000) {
        verificationWarning = `Kaynak: ${sourceCount.toLocaleString()}, Hedef: ${clickhouseCount.toLocaleString()} - ${Math.abs(diff).toLocaleString()} satÄ±r fark (${diffPercent}%)`;
        logger.warn('âš ï¸ Count mismatch detected!', { 
          source: sourceCount, 
          clickhouse: clickhouseCount, 
          diff,
          warning: verificationWarning
        });
      }
    } catch (verifyError: any) {
      logger.warn('Count verification failed', { error: verifyError.message });
    }
    
    // Job tamamla (verification sonucu ile)
    if (jobId) {
      await db.query(
        `UPDATE etl_jobs SET 
          status = 'completed', 
          completed_at = NOW(), 
          rows_processed = $1,
          error_message = $2
        WHERE id = $3`,
        [totalInserted, verificationWarning, jobId]
      );
    }
    
    // Cache invalidate
    await cache.del(`kpi:${clickhouse_table}:*`);
    
    logger.info('ID-Based incremental sync completed', { 
      datasetId: dataset.id, 
      totalInserted,
      previousMaxId: maxId,
      verification: verificationWarning || 'OK'
    });
    
    return totalInserted;
    
  } catch (error: any) {
    logger.error('ID-Based sync failed', { error: error.message, datasetId: dataset.id });
    throw error;
  }
}

/**
 * EKSÄ°K ID ARALIKLARI SYNC (Missing Ranges Sync)
 * 
 * Sadece eksik olan ID aralÄ±klarÄ±nÄ± kaynak DB'den Ã§eker.
 * Truncate yapmaz, mevcut verilere ekler.
 * 
 * KullanÄ±m: Veri doÄŸrulamada tespit edilen eksik aralÄ±klar iÃ§in.
 */
async function syncMissingRanges(
  dataset: any, 
  connection: any, 
  jobId: string,
  ranges: Array<{start: number; end: number; missing_count?: number}>,
  pkColumn: string = 'id' // âš ï¸ KULLANICI SEÃ‡TÄ°ÄÄ° PK KOLONU
): Promise<number> {
  const { clickhouse_table, source_query } = dataset;
  
  if (!ranges || ranges.length === 0) {
    logger.warn('No ranges provided for missing sync', { datasetId: dataset.id });
    return 0;
  }
  
  // Kaynak sorgudan tablo adÄ±nÄ± Ã§Ä±kar
  let sourceTable = dataset.source_table;
  if (!sourceTable && source_query) {
    const match = source_query.match(/FROM\s+\[?(\w+)\]?\.\[?(\w+)\]?/i) ||
                  source_query.match(/FROM\s+\[?(\w+)\]?/i);
    if (match) {
      sourceTable = match[2] || match[1];
    }
  }
  
  if (!sourceTable) {
    throw new Error('Kaynak tablo adÄ± bulunamadÄ±');
  }
  
  // âš ï¸ DÄ°NAMÄ°K PK KOLONU - kullanÄ±cÄ± seÃ§iyor!
  const idColumn = pkColumn;
  let totalInserted = 0;
  
  logger.info('ğŸ” Starting missing ranges sync', { 
    datasetId: dataset.id, 
    ranges: ranges.length,
    totalMissing: ranges.reduce((sum, r) => sum + (r.missing_count || (r.end - r.start + 1)), 0)
  });
  
  // column_mapping'i parse et
  let columnMapping = dataset.column_mapping || [];
  if (typeof columnMapping === 'string') {
    try { columnMapping = JSON.parse(columnMapping); } catch (e) { columnMapping = []; }
  }
  
  if (connection.type === 'mssql') {
    const mssql = require('mssql');
    const connStr = connection.connection_string || 
      `Server=${connection.host},${connection.port || 1433};Database=${connection.database_name};User Id=${connection.username};Password=${connection.password_encrypted};Encrypt=${connection.host?.includes('.database.windows.net')};TrustServerCertificate=true;Connection Timeout=30;Request Timeout=120000`;
    
    const pool = await mssql.connect(connStr);
    
    for (let i = 0; i < ranges.length; i++) {
      const range = ranges[i];
      logger.info(`Processing range ${i + 1}/${ranges.length}`, { start: range.start, end: range.end });
      
      // Bu aralÄ±ktaki verileri Ã§ek
      const query = `SELECT * FROM [${sourceTable}] WITH (NOLOCK) WHERE [${idColumn}] >= ${range.start} AND [${idColumn}] <= ${range.end}`;
      const result = await pool.request().query(query);
      
      if (result.recordset.length > 0) {
        // Column mapping yoksa oluÅŸtur
        if (columnMapping.length === 0) {
          for (const key of Object.keys(result.recordset[0])) {
            const value = result.recordset[0][key];
            let clickhouseType = 'String';
            if (typeof value === 'number') {
              clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
            } else if (value instanceof Date) {
              clickhouseType = 'DateTime';
            }
            columnMapping.push({ sourceName: key, targetName: key, type: clickhouseType });
          }
        }
        
        // Veriyi ClickHouse formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
        const transformedData = transformBatchForClickHouse(result.recordset, columnMapping);
        
        // ClickHouse'a yaz
        await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedData);
        totalInserted += result.recordset.length;
        
        logger.info(`Range ${i + 1} completed`, { 
          inserted: result.recordset.length, 
          totalSoFar: totalInserted 
        });
      }
      
      // Progress gÃ¼ncelle
      if (jobId) {
        const progress = Math.round(((i + 1) / ranges.length) * 100);
        await db.query(
          `UPDATE etl_jobs SET rows_processed = $1, error_message = $2 WHERE id = $3`,
          [totalInserted, `Range ${i + 1}/${ranges.length} - ${progress}%`, jobId]
        );
      }
    }
    
    await pool.close();
  } else {
    throw new Error(`Missing sync henÃ¼z sadece MSSQL destekliyor. Mevcut: ${connection.type}`);
  }
  
  // OPTIMIZE Ã§alÄ±ÅŸtÄ±r
  try {
    await clickhouse.execute(`OPTIMIZE TABLE clixer_analytics.${clickhouse_table} FINAL`);
    logger.info('OPTIMIZE completed after missing sync');
  } catch (optErr: any) {
    logger.warn('OPTIMIZE failed (non-critical)', { error: optErr.message });
  }
  
  logger.info('âœ… Missing ranges sync completed', { 
    datasetId: dataset.id, 
    totalInserted,
    rangesProcessed: ranges.length
  });
  
  return totalInserted;
}

/**
 * ğŸš€ SADECE YENÄ° KAYITLARI Ã‡EK (New Records Sync)
 * 
 * 100M+ tablolar iÃ§in EN VERÄ°MLÄ° yÃ¶ntem!
 * ClickHouse'daki max ID'den sonraki tÃ¼m kayÄ±tlarÄ± batch batch Ã§eker.
 * 
 * Avantajlar:
 * - RAM tÃ¼ketimi minimum (5000'lik batch)
 * - Eksik ID aralÄ±klarÄ±nÄ± taramaya gerek yok
 * - Kaynakta yeni ne varsa hepsini alÄ±r
 * - Progress bar ile takip edilebilir
 */
async function syncNewRecordsAfterMaxId(
  dataset: any,
  connection: any,
  jobId: string,
  pkColumn: string = 'id',
  afterId: number = 0,
  limit?: number
): Promise<number> {
  const { clickhouse_table, source_query } = dataset;
  const BATCH_SIZE = 5000; // Her batch'te 5000 satÄ±r
  
  // Kaynak sorgudan tablo adÄ±nÄ± Ã§Ä±kar
  let sourceTable = dataset.source_table;
  if (!sourceTable && source_query) {
    const match = source_query.match(/FROM\s+\[?(\w+)\]?\.\[?(\w+)\]?/i) ||
                  source_query.match(/FROM\s+\[?(\w+)\]?/i);
    if (match) {
      sourceTable = match[2] || match[1];
    }
  }
  
  if (!sourceTable) {
    throw new Error('Kaynak tablo adÄ± bulunamadÄ±');
  }
  
  logger.info('ğŸš€ NEW RECORDS SYNC baÅŸlÄ±yor', { 
    datasetId: dataset.id, 
    pkColumn, 
    afterId,
    limit: limit || 'UNLIMITED',
    batchSize: BATCH_SIZE
  });
  
  // column_mapping'i parse et
  let columnMapping = dataset.column_mapping || [];
  if (typeof columnMapping === 'string') {
    try { columnMapping = JSON.parse(columnMapping); } catch (e) { columnMapping = []; }
  }
  
  let totalInserted = 0;
  let currentMaxId = afterId;
  let hasMore = true;
  let batchNum = 0;
  
  if (connection.type === 'mssql') {
    const mssql = require('mssql');
    const connStr = connection.connection_string || 
      `Server=${connection.host},${connection.port || 1433};Database=${connection.database_name};User Id=${connection.username};Password=${connection.password_encrypted};Encrypt=${connection.host?.includes('.database.windows.net')};TrustServerCertificate=true;Connection Timeout=30;Request Timeout=120000`;
    
    const pool = await mssql.connect(connStr);
    
    // Kaynaktaki max ID'yi al (ilerleme hesabÄ± iÃ§in)
    const sourceMaxResult = await pool.request().query(`SELECT MAX([${pkColumn}]) as max_id FROM [${sourceTable}] WITH (NOLOCK)`);
    const sourceMaxId = sourceMaxResult.recordset[0]?.max_id || 0;
    const expectedTotal = Math.max(0, sourceMaxId - afterId);
    
    logger.info('ğŸ“Š Kaynak max ID bulundu', { sourceMaxId, afterId, expectedTotal });
    
    while (hasMore) {
      batchNum++;
      
      // Limit kontrolÃ¼
      const remainingLimit = limit ? Math.max(0, limit - totalInserted) : BATCH_SIZE;
      const batchLimit = Math.min(BATCH_SIZE, remainingLimit);
      
      if (batchLimit === 0) {
        logger.info('Limite ulasildi, sync durduruluyor', { totalInserted, limit });
        break;
      }
      
      // Cursor/Pagination ile veri Ã§ek
      // MSSQL iÃ§in TOP kullan, ORDER BY ile sÄ±rala
      const query = `
        SELECT TOP ${batchLimit} * 
        FROM [${sourceTable}] WITH (NOLOCK) 
        WHERE [${pkColumn}] > ${currentMaxId} 
        ORDER BY [${pkColumn}] ASC
      `;
      
      const result = await pool.request().query(query);
      
      if (result.recordset.length === 0) {
        hasMore = false;
        logger.info('Ã‡ekilecek veri kalmadÄ±, sync tamamlandÄ±');
        break;
      }
      
      // Column mapping yoksa oluÅŸtur
      if (columnMapping.length === 0 && result.recordset.length > 0) {
        for (const key of Object.keys(result.recordset[0])) {
          const value = result.recordset[0][key];
          let clickhouseType = 'String';
          if (typeof value === 'number') {
            clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
          } else if (value instanceof Date) {
            clickhouseType = 'DateTime';
          }
          columnMapping.push({ sourceName: key, targetName: key, type: clickhouseType });
        }
      }
      
      // Veriyi ClickHouse formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
      const transformedData = transformBatchForClickHouse(result.recordset, columnMapping);
      
      // ClickHouse'a yaz
      await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedData);
      totalInserted += result.recordset.length;
      
      // Cursor'u gÃ¼ncelle (son satÄ±rÄ±n ID'si)
      currentMaxId = result.recordset[result.recordset.length - 1][pkColumn];
      
      // Progress gÃ¼ncelle
      const progress = expectedTotal > 0 ? Math.min(100, Math.round((totalInserted / expectedTotal) * 100)) : 0;
      await db.query(
        `UPDATE etl_jobs SET rows_processed = $1, error_message = $2 WHERE id = $3`,
        [totalInserted, `Batch ${batchNum}: ${totalInserted} satÄ±r (${progress}%)`, jobId]
      );
      
      logger.info(`Batch ${batchNum} tamamlandÄ±`, { 
        batchSize: result.recordset.length,
        totalSoFar: totalInserted,
        currentMaxId,
        progress: `${progress}%`
      });
      
      // Batch tam dolmadÄ±ysa veri bitmiÅŸ demektir
      if (result.recordset.length < batchLimit) {
        hasMore = false;
      }
    }
    
    await pool.close();
  } else if (connection.type === 'postgresql') {
    const { Pool } = require('pg');
    const pool = new Pool({
      host: connection.host,
      port: connection.port || 5432,
      database: connection.database_name,
      user: connection.username,
      password: connection.password_encrypted,
      max: 5
    });
    
    while (hasMore) {
      batchNum++;
      const batchLimit = limit ? Math.min(BATCH_SIZE, limit - totalInserted) : BATCH_SIZE;
      if (batchLimit <= 0) break;
      
      const query = `SELECT * FROM ${sourceTable} WHERE ${pkColumn} > $1 ORDER BY ${pkColumn} ASC LIMIT $2`;
      const result = await pool.query(query, [currentMaxId, batchLimit]);
      
      if (result.rows.length === 0) {
        hasMore = false;
        break;
      }
      
      if (columnMapping.length === 0 && result.rows.length > 0) {
        for (const key of Object.keys(result.rows[0])) {
          const value = result.rows[0][key];
          let clickhouseType = 'String';
          if (typeof value === 'number') clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
          else if (value instanceof Date) clickhouseType = 'DateTime';
          columnMapping.push({ sourceName: key, targetName: key, type: clickhouseType });
        }
      }
      
      const transformedData = transformBatchForClickHouse(result.rows, columnMapping);
      await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedData);
      totalInserted += result.rows.length;
      currentMaxId = result.rows[result.rows.length - 1][pkColumn];
      
      await db.query(
        `UPDATE etl_jobs SET rows_processed = $1, error_message = $2 WHERE id = $3`,
        [totalInserted, `Batch ${batchNum}: ${totalInserted} satÄ±r`, jobId]
      );
      
      if (result.rows.length < batchLimit) hasMore = false;
    }
    
    await pool.end();
  } else if (connection.type === 'mysql') {
    const mysql = require('mysql2/promise');
    const conn = await mysql.createConnection({
      host: connection.host,
      port: connection.port || 3306,
      database: connection.database_name,
      user: connection.username,
      password: connection.password_encrypted,
      charset: 'utf8mb4'
    });
    
    while (hasMore) {
      batchNum++;
      const batchLimit = limit ? Math.min(BATCH_SIZE, limit - totalInserted) : BATCH_SIZE;
      if (batchLimit <= 0) break;
      
      // MySQL iÃ§in inline values kullan (prepared statement LIMIT sorunu!)
      const query = `SELECT * FROM ${sourceTable} WHERE ${pkColumn} > ${currentMaxId} ORDER BY ${pkColumn} ASC LIMIT ${batchLimit}`;
      const [rows] = await conn.query(query);
      
      if (!rows || (rows as any[]).length === 0) {
        hasMore = false;
        break;
      }
      
      const rowsArray = rows as any[];
      
      if (columnMapping.length === 0 && rowsArray.length > 0) {
        for (const key of Object.keys(rowsArray[0])) {
          const value = rowsArray[0][key];
          let clickhouseType = 'String';
          if (typeof value === 'number') clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
          else if (value instanceof Date) clickhouseType = 'DateTime';
          columnMapping.push({ sourceName: key, targetName: key, type: clickhouseType });
        }
      }
      
      const transformedData = transformBatchForClickHouse(rowsArray, columnMapping);
      await clickhouse.insert(`clixer_analytics.${clickhouse_table}`, transformedData);
      totalInserted += rowsArray.length;
      currentMaxId = rowsArray[rowsArray.length - 1][pkColumn];
      
      await db.query(
        `UPDATE etl_jobs SET rows_processed = $1, error_message = $2 WHERE id = $3`,
        [totalInserted, `Batch ${batchNum}: ${totalInserted} satÄ±r`, jobId]
      );
      
      if (rowsArray.length < batchLimit) hasMore = false;
    }
    
    await conn.end();
  } else {
    throw new Error(`new_records_sync henÃ¼z ${connection.type} desteklemiyor`);
  }
  
  // OPTIMIZE Ã§alÄ±ÅŸtÄ±r (opsiyonel - bÃ¼yÃ¼k tablolarda zaman alÄ±r!)
  if (totalInserted > 0 && totalInserted < 1000000) { // 1M altÄ±nda optimize et
    try {
      await clickhouse.execute(`OPTIMIZE TABLE clixer_analytics.${clickhouse_table} FINAL`);
      logger.info('OPTIMIZE completed after new records sync');
    } catch (optErr: any) {
      logger.warn('OPTIMIZE skipped or failed (non-critical for large tables)', { error: optErr.message });
    }
  } else {
    logger.info('OPTIMIZE skipped for large table (>1M rows inserted)', { totalInserted });
  }
  
  logger.info('âœ… NEW RECORDS SYNC tamamlandÄ±', { 
    datasetId: dataset.id, 
    totalInserted,
    batchesProcessed: batchNum,
    finalMaxId: currentMaxId
  });
  
  return totalInserted;
}

/**
 * TARÄ°H BAZLI SÄ°L-YAZ (Date Delete & Insert)
 * 
 * Partition gerektirmez! Basit ve etkili:
 * 1. ClickHouse'tan son X gÃ¼nÃ¼ sil (reference_column'a gÃ¶re)
 * 2. Kaynak DB'den son X gÃ¼nÃ¼ Ã§ek
 * 3. ClickHouse'a yaz
 * 
 * KÃ¼Ã§Ã¼k-orta tablolar iÃ§in ideal!
 */
async function syncByDateDeleteInsert(dataset: any, connection: any, jobId?: string): Promise<number> {
  const {
    reference_column,
    delete_days = 1,
    clickhouse_table,
    source_query,
    source_table,
    row_limit
  } = dataset;

  // column_mapping'i parse et
  let columnMapping = dataset.column_mapping || [];
  if (typeof columnMapping === 'string') {
    try {
      columnMapping = JSON.parse(columnMapping);
    } catch (e) {
      columnMapping = [];
    }
  }

  logger.info('Date Delete & Insert sync starting', { 
    datasetId: dataset.id, 
    datasetName: dataset.name,
    referenceColumn: reference_column,
    deleteDays: delete_days,
    clickhouseTable: clickhouse_table
  });

  // Referans kolon kontrolÃ¼
  if (!reference_column) {
    logger.warn('No reference column defined, falling back to full refresh', { datasetId: dataset.id });
    return await fullRefresh(dataset, connection, jobId);
  }

  // Source kontrolÃ¼
  const sourceTableName = extractTableFromQuery(source_query || `SELECT * FROM ${source_table}`);
  if (!sourceTableName) {
    throw new Error('Kaynak tablo bulunamadÄ±');
  }

  try {
    // 1. ClickHouse'tan son X gÃ¼nÃ¼ sil
    const today = new Date();
    const startDate = new Date(today);
    startDate.setDate(today.getDate() - delete_days);
    const startDateStr = startDate.toISOString().split('T')[0]; // YYYY-MM-DD
    
    logger.info('Deleting data from ClickHouse', { 
      table: clickhouse_table, 
      dateColumn: reference_column,
      fromDate: startDateStr
    });

    await clickhouse.execute(`
      ALTER TABLE clixer_analytics.${clickhouse_table} 
      DELETE WHERE toDate(${reference_column}) >= '${startDateStr}'
    `);
    
    logger.info('Deleted recent data from ClickHouse');

    // 2. Kaynak DB'den son X gÃ¼nÃ¼ Ã§ek
    let totalInserted = 0;

    if (connection.type === 'mssql') {
      // MSSQL iÃ§in tarih filtreli sorgu
      const columns = (dataset.column_mapping || []).map((c: any) => c.source).join(', ') || '*';
      let dateFilter = '';
      
      // MSSQL tarih formatÄ±
      if (delete_days === 0) {
        dateFilter = `WHERE CAST(${reference_column} AS DATE) = CAST(GETDATE() AS DATE)`;
      } else {
        dateFilter = `WHERE ${reference_column} >= DATEADD(day, -${delete_days}, CAST(GETDATE() AS DATE))`;
      }
      
      const limit = row_limit || 10000000;
      const query = `SELECT TOP ${limit} ${columns} FROM ${sourceTableName} ${dateFilter} ORDER BY ${reference_column}`;
      
      logger.info('Executing MSSQL date-filtered query', { 
        query: query.substring(0, 300),
        dateFilter,
        limit
      });

      // MSSQL streaming ile veri Ã§ek - custom query ile
      const originalQuery = dataset.source_query;
      dataset.source_query = query;
      
      // column_mapping'i parse et
      let columnMapping = dataset.column_mapping || [];
      if (typeof columnMapping === 'string') {
        try {
          columnMapping = JSON.parse(columnMapping);
        } catch (e) {
          columnMapping = [];
        }
      }
      
      try {
        totalInserted = await mssqlSync(dataset, connection, columnMapping, jobId);
      } finally {
        dataset.source_query = originalQuery;
      }
      
    } else if (connection.type === 'postgresql') {
      // PostgreSQL iÃ§in
      const columns = (dataset.column_mapping || []).map((c: any) => c.source).join(', ') || '*';
      let dateFilter = '';
      
      if (delete_days === 0) {
        dateFilter = `WHERE ${reference_column}::date = CURRENT_DATE`;
      } else {
        dateFilter = `WHERE ${reference_column} >= CURRENT_DATE - INTERVAL '${delete_days} days'`;
      }
      
      const limit = row_limit || 10000000;
      const query = `SELECT ${columns} FROM ${sourceTableName} ${dateFilter} ORDER BY ${reference_column} LIMIT ${limit}`;
      
      logger.info('Executing PostgreSQL date-filtered query', { query: query.substring(0, 300) });
      
      // source_query'yi geÃ§ici olarak deÄŸiÅŸtir
      const originalQuery = dataset.source_query;
      dataset.source_query = query;
      try {
        totalInserted = await streamingPostgreSQLSync(dataset, connection, columnMapping, jobId);
      } finally {
        dataset.source_query = originalQuery;
      }
      
    } else if (connection.type === 'mysql') {
      // MySQL iÃ§in
      const columns = (dataset.column_mapping || []).map((c: any) => c.source).join(', ') || '*';
      let dateFilter = '';
      
      if (delete_days === 0) {
        dateFilter = `WHERE DATE(${reference_column}) = CURDATE()`;
      } else {
        dateFilter = `WHERE ${reference_column} >= DATE_SUB(CURDATE(), INTERVAL ${delete_days} DAY)`;
      }
      
      const limit = row_limit || 10000000;
      const query = `SELECT ${columns} FROM ${sourceTableName} ${dateFilter} ORDER BY ${reference_column} LIMIT ${limit}`;
      
      logger.info('Executing MySQL date-filtered query', { query: query.substring(0, 300) });
      
      // source_query'yi geÃ§ici olarak deÄŸiÅŸtir
      const originalQuery = dataset.source_query;
      dataset.source_query = query;
      try {
        totalInserted = await mysqlSync(dataset, connection, columnMapping, jobId);
      } finally {
        dataset.source_query = originalQuery;
      }
      
    } else {
      logger.warn('Unsupported connection type for date_delete_insert, falling back to full refresh', { 
        type: connection.type 
      });
      return await fullRefresh(dataset, connection, jobId);
    }

    // 3. OPTIMIZE Ã§alÄ±ÅŸtÄ±r
    await clickhouse.execute(`OPTIMIZE TABLE clixer_analytics.${clickhouse_table} FINAL`);
    
    logger.info('Date Delete & Insert sync completed', { 
      datasetId: dataset.id,
      totalInserted,
      deleteDays: delete_days
    });

    return totalInserted;
    
  } catch (error: any) {
    logger.error('Date Delete & Insert sync failed', { 
      datasetId: dataset.id,
      error: error.message 
    });
    throw error;
  }
}

/**
 * PARTITION BAZLI SÄ°L-YAZ (Sliding Window + Modified AlgÄ±lama)
 * 
 * Bu fonksiyon ÅŸunlarÄ± yapar:
 * 1. refresh_window_days kadar geriye gider (sliding window)
 * 2. detect_modified=true ise modified_at ile deÄŸiÅŸen gÃ¼nleri bulur
 * 3. Etkilenen partition'larÄ± siler
 * 4. Sadece o gÃ¼nlerin verilerini yazar
 * 
 * Power BI Incremental Refresh benzeri ama daha gÃ¼Ã§lÃ¼!
 */
async function syncByDatePartition(dataset: any, connection: any, jobId?: string): Promise<number> {
  // ğŸ”§ SELF-HEALING: Tablo yoksa otomatik oluÅŸtur
  await ensureTableExists(dataset);
  
  const {
    partition_column,
    partition_type = 'monthly',
    refresh_window_days = 7,
    detect_modified = false,
    modified_column,
    clickhouse_table
  } = dataset;

  logger.info('Partition-based sync starting', { 
    datasetId: dataset.id, 
    partitionColumn: partition_column,
    partitionType: partition_type,
    refreshWindowDays: refresh_window_days,
    detectModified: detect_modified,
    modifiedColumn: modified_column
  });

  // Partition kolonu yoksa full refresh'e dÃ¼ÅŸ
  if (!partition_column) {
    logger.warn('No partition column defined, falling back to full refresh', { datasetId: dataset.id });
    return await fullRefresh(dataset, connection, jobId);
  }

  // Source table kontrolÃ¼
  if (!dataset.source_table && !dataset.source_query) {
    logger.error('No source table or query defined', { datasetId: dataset.id });
    throw new Error('Kaynak tablo veya sorgu tanÄ±mlanmamÄ±ÅŸ');
  }

  // ============================================
  // TUTARLILIK KONTROLÃœ: Dataset ayarÄ± vs ClickHouse tablo yapÄ±sÄ±
  // ============================================
  try {
    const tableInfo = await clickhouse.queryOne(`
      SELECT partition_key, engine, sorting_key
      FROM system.tables 
      WHERE database = 'clixer_analytics' AND name = '${clickhouse_table}'
    `);
    
    if (tableInfo) {
      const actualPartitionKey = tableInfo.partition_key || '';
      const isActuallyMonthly = actualPartitionKey.includes('toYYYYMM') && !actualPartitionKey.includes('toYYYYMMDD');
      const isActuallyDaily = actualPartitionKey.includes('toYYYYMMDD');
      const expectedFormat = partition_type === 'daily' ? 'YYYYMMDD (gÃ¼nlÃ¼k)' : 'YYYYMM (aylÄ±k)';
      const actualFormat = isActuallyDaily ? 'YYYYMMDD (gÃ¼nlÃ¼k)' : (isActuallyMonthly ? 'YYYYMM (aylÄ±k)' : 'bilinmiyor');
      
      // Uyumsuzluk kontrolÃ¼
      const isConsistent = 
        (partition_type === 'daily' && isActuallyDaily) || 
        (partition_type === 'monthly' && isActuallyMonthly);
      
      if (!isConsistent && actualPartitionKey) {
        logger.warn('âš ï¸ PARTITION FORMAT UYUMSUZLUÄU!', {
          datasetId: dataset.id,
          datasetName: dataset.name,
          clickhouseTable: clickhouse_table,
          uiAyari: {
            partitionType: partition_type,
            beklenenFormat: expectedFormat
          },
          clickhouseGercek: {
            partitionKey: actualPartitionKey,
            gercekFormat: actualFormat,
            engine: tableInfo.engine
          },
          cozum: 'Tablo partition_key formatÄ± kullanÄ±lacak (UI ayarÄ± yoksayÄ±ldÄ±)'
        });
      } else {
        logger.info('âœ… Partition format tutarlÄ±', {
          datasetId: dataset.id,
          format: actualFormat,
          engine: tableInfo.engine
        });
      }
    }
  } catch (consistencyError: any) {
    logger.warn('TutarlÄ±lÄ±k kontrolÃ¼ yapÄ±lamadÄ±', { error: consistencyError.message });
  }

  try {
    const columnMapping = dataset.column_mapping || [];
    
    // Column mapping kontrolÃ¼
    if (!columnMapping || columnMapping.length === 0) {
      logger.error('No column mapping defined', { datasetId: dataset.id });
      throw new Error('Kolon eÅŸleÅŸtirmesi tanÄ±mlanmamÄ±ÅŸ');
    }
    
    // PostgreSQL baÄŸlantÄ±sÄ± kur
    if (connection.type !== 'postgresql') {
      logger.warn('Partition sync only supports PostgreSQL, falling back to full refresh', { type: connection.type });
      return await fullRefresh(dataset, connection, jobId);
    }

    const { Client } = require('pg');
    const client = new Client({
      host: connection.host,
      port: connection.port,
      database: connection.database_name,
      user: connection.username,
      password: connection.password_encrypted
    });
    await client.connect();

    // Source table veya query belirle
    const sourceFrom = dataset.source_table 
      ? dataset.source_table 
      : dataset.source_query 
        ? `(${dataset.source_query}) AS sq` 
        : null;
    
    if (!sourceFrom) {
      throw new Error('Kaynak tablo veya sorgu tanÄ±mlanmamÄ±ÅŸ');
    }

    try {
      // 1. AFFECTED DATES: Hangi gÃ¼nler etkilendi?
      let affectedDates: string[] = [];
      const today = new Date();
      
      if (detect_modified && modified_column) {
        // modified_at bazlÄ± algÄ±lama - hangi gÃ¼nlerin verisi deÄŸiÅŸti?
        const lastSyncTime = dataset.last_sync_at || new Date(0).toISOString();
        
        const affectedQuery = `
          SELECT DISTINCT DATE(${partition_column}) as affected_date
          FROM ${sourceFrom}
          WHERE ${modified_column} >= $1
          ORDER BY affected_date
        `;
        
        const affectedResult = await client.query(affectedQuery, [lastSyncTime]);
        affectedDates = affectedResult.rows.map((r: any) => r.affected_date.toISOString().split('T')[0]);
        
        logger.info('Modified dates detected', { 
          count: affectedDates.length, 
          dates: affectedDates.slice(0, 10),
          since: lastSyncTime 
        });
      }
      
      // 2. SLIDING WINDOW: Son X gÃ¼nÃ¼ de ekle
      if (refresh_window_days > 0) {
        for (let i = 0; i < refresh_window_days; i++) {
          const date = new Date(today);
          date.setDate(date.getDate() - i);
          const dateStr = date.toISOString().split('T')[0];
          if (!affectedDates.includes(dateStr)) {
            affectedDates.push(dateStr);
          }
        }
      }

      // Tarih yoksa en az bugÃ¼nÃ¼ ekle
      if (affectedDates.length === 0) {
        affectedDates.push(today.toISOString().split('T')[0]);
      }

      // Tarihleri sÄ±rala
      affectedDates.sort();
      
      logger.info('Dates to refresh', { 
        count: affectedDates.length, 
        dates: affectedDates 
      });

      // 3. HER TARÄ°H Ä°Ã‡Ä°N: Partition sil + yaz
      // ============================================
      // 3. Ã–NCE: Etkilenen partition'larÄ± BÄ°R KEZ sil
      // ============================================
      
      // Tablonun partition formatÄ±nÄ± Ã¶ÄŸren (dÃ¶ngÃ¼ dÄ±ÅŸÄ±nda - performans iÃ§in)
      const tableInfo = await clickhouse.queryOne(`
        SELECT partition_key 
        FROM system.tables 
        WHERE database = 'clixer_analytics' AND name = '${clickhouse_table}'
      `);
      const isMonthlyPartition = tableInfo?.partition_key?.includes('toYYYYMM') && !tableInfo?.partition_key?.includes('toYYYYMMDD');
      
      // Unique partition deÄŸerlerini hesapla (aynÄ± ay iÃ§in tek sil)
      const uniquePartitions = new Set<string>();
      for (const dateStr of affectedDates) {
        const partitionDate = new Date(dateStr);
        const partitionValue = isMonthlyPartition 
          ? partitionDate.toISOString().slice(0, 7).replace(/-/g, '')   // YYYYMM
          : partitionDate.toISOString().slice(0, 10).replace(/-/g, ''); // YYYYMMDD
        uniquePartitions.add(partitionValue);
      }
      
      logger.info('Unique partitions to drop', { 
        count: uniquePartitions.size, 
        partitions: Array.from(uniquePartitions),
        isMonthly: isMonthlyPartition,
        partitionKey: tableInfo?.partition_key
      });
      
      // Her unique partition'Ä± sadece BÄ°R KEZ sil
      for (const partitionValue of uniquePartitions) {
        try {
          logger.info('Dropping partition', { table: clickhouse_table, partition: partitionValue });
          await clickhouse.execute(
            `ALTER TABLE clixer_analytics.${clickhouse_table} DROP PARTITION '${partitionValue}'`
          );
          logger.info('Partition dropped successfully', { table: clickhouse_table, partition: partitionValue });
        } catch (dropError: any) {
          if (!dropError.message?.includes('not found') && !dropError.message?.includes('Cannot find')) {
            logger.warn('Partition drop warning', { partition: partitionValue, error: dropError.message });
          }
        }
      }
      
      // ============================================
      // 4. SONRA: TÃ¼m veriyi ekle (partition silme yok artÄ±k)
      // ============================================
      let totalRowsProcessed = 0;
      
      for (const dateStr of affectedDates) {
        // Job iptal edilmiÅŸ mi kontrol et
        if (jobId && await isJobCancelled(jobId)) {
          logger.info('Job cancelled during partition sync', { jobId, currentDate: dateStr });
          break;
        }

        logger.info('Processing date data', { date: dateStr });

        // O tarihin verilerini Ã§ek ve yaz (partition silme ARTIK YOK - yukarÄ±da yapÄ±ldÄ±)
        const dateCondition = partition_type === 'daily'
          ? `DATE(${partition_column}) = '${dateStr}'`
          : `DATE_TRUNC('month', ${partition_column}) = DATE_TRUNC('month', '${dateStr}'::date)`;

        const selectColumns = columnMapping.map((col: any) => col.source).join(', ');
        
        // Source table veya source_query'den veri Ã§ek
        let dataQuery: string;
        if (dataset.source_query) {
          // Subquery olarak kullan
          dataQuery = `SELECT ${selectColumns} FROM (${dataset.source_query}) AS sq WHERE ${dateCondition}`;
        } else if (dataset.source_table) {
          dataQuery = `SELECT ${selectColumns} FROM ${dataset.source_table} WHERE ${dateCondition}`;
        } else {
          throw new Error('Kaynak tablo veya sorgu tanÄ±mlanmamÄ±ÅŸ');
        }
        
        const dataResult = await client.query(dataQuery);
        const rows = dataResult.rows;
        
        if (rows.length > 0) {
          // ClickHouse'a yaz
          const targetColumns = columnMapping.map((col: any) => col.target);
          
          // Batch insert
          const batchSize = 5000;
          for (let i = 0; i < rows.length; i += batchSize) {
            const batch = rows.slice(i, i + batchSize);
            const values = batch.map((row: any) => {
              return '(' + columnMapping.map((col: any) => {
                const val = row[col.source];
                if (val === null || val === undefined) return 'NULL';
                
                // DateTime veya Date tipinde kolon iÃ§in akÄ±llÄ± dÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ kullan
                if (col.clickhouseType === 'DateTime' || col.clickhouseType === 'DateTime64' || col.clickhouseType === 'Date') {
                  const converted = convertToClickHouseDateTime(val);
                  if (converted) {
                    // Date tipi iÃ§in sadece tarih kÄ±smÄ±nÄ± al
                    if (col.clickhouseType === 'Date') {
                      return `'${converted.split(' ')[0]}'`;
                    }
                    return `'${converted}'`;
                  }
                  // VarsayÄ±lan deÄŸer
                  return col.clickhouseType === 'Date' ? "'1970-01-01'" : "'1970-01-01 00:00:00'";
                }
                
                // Tarih benzeri string'ler iÃ§in de kontrol et
                if (val instanceof Date || (typeof val === 'string' && val.match(/\d{4}[-\/]\d{2}[-\/]\d{2}|\d{2}[-\/]\d{2}[-\/]\d{4}/))) {
                  const converted = convertToClickHouseDateTime(val);
                  if (converted) return `'${converted}'`;
                }
                
                if (typeof val === 'string') return `'${val.replace(/'/g, "''")}'`;
                return val;
              }).join(',') + ')';
            }).join(',\n');

            const insertSql = `
              INSERT INTO clixer_analytics.${clickhouse_table} (${targetColumns.join(',')})
              VALUES ${values}
            `;
            
            await clickhouse.execute(insertSql);
          }
          
          totalRowsProcessed += rows.length;
          logger.info('Partition data written', { 
            date: dateStr, 
            rows: rows.length 
          });
        } else {
          logger.info('No data for partition', { date: dateStr });
        }
      }

      // Cache invalidate
      await cache.del(`kpi:${clickhouse_table}:*`);
      
      // ============================================
      // VERÄ° TUTARLILIK KONTROLÃœ VE DUPLICATE TEMÄ°ZLÄ°ÄÄ°
      // ============================================
      const validation = await validateDataConsistency(dataset, client, totalRowsProcessed);
      
      if (!validation.isConsistent) {
        logger.warn('Partition sync consistency warning', {
          datasetId: dataset.id,
          expected: totalRowsProcessed,
          actual: validation.targetCount,
          duplicatesRemoved: validation.duplicateCount
        });
      }
      
      logger.info('Partition sync completed', { 
        datasetId: dataset.id, 
        rowsProcessed: totalRowsProcessed,
        finalRowCount: validation.targetCount,
        partitionsProcessed: affectedDates.length,
        duplicatesRemoved: validation.duplicateCount,
        isConsistent: validation.isConsistent
      });

      // Ä°ÅŸlenen satÄ±r sayÄ±sÄ± 0 ise, tablodaki toplam satÄ±r sayÄ±sÄ±nÄ± dÃ¶ndÃ¼r
      // (sliding window dÄ±ÅŸÄ±nda veri yoksa bile tablo boÅŸ deÄŸil)
      return totalRowsProcessed > 0 ? totalRowsProcessed : validation.targetCount;

    } finally {
      await client.end();
    }

  } catch (error: any) {
    logger.error('Partition sync failed', { datasetId: dataset.id, error: error.message });
    throw error;
  }
}

// ============================================
// MSSQL SYNC (Azure SQL dahil) - STREAMING MODE
// ============================================
async function mssqlSync(
  dataset: any,
  connection: any,
  columnMapping: any[],
  jobId?: string
): Promise<number> {
  const mssql = require('mssql');
  
  const isAzure = connection.host.includes('.database.windows.net');
  const config = {
    server: connection.host,
    port: connection.port || 1433,
    database: connection.database_name,
    user: connection.username,
    password: connection.password_encrypted,
    options: { 
      encrypt: isAzure, 
      trustServerCertificate: !isAzure,
      requestTimeout: 600000 // 10 dakika timeout
    },
    pool: {
      max: 10,
      min: 0,
      idleTimeoutMillis: 30000
    }
  };
  
  const rowLimit = dataset.row_limit || null;
  
  logger.info('ğŸ”· MSSQL Streaming Sync starting', {
    datasetId: dataset.id,
    table: dataset.clickhouse_table,
    isAzure,
    host: connection.host,
    rowLimit: rowLimit || 'UNLIMITED'
  });
  
  let pool: any = null;
  
  // Date format helper - gÃ¼venli versiyon
  const formatDate = (date: Date): string | null => {
    // GeÃ§ersiz Date kontrolÃ¼
    if (!date || isNaN(date.getTime())) {
      return null;
    }
    const pad = (n: number, len = 2) => n.toString().padStart(len, '0');
    const year = date.getFullYear();
    // YÄ±l geÃ§ersizse (Ã§ok kÃ¼Ã§Ã¼k veya Ã§ok bÃ¼yÃ¼k)
    if (year < 1900 || year > 2100) {
      return null;
    }
    const month = pad(date.getMonth() + 1);
    const day = pad(date.getDate());
    const hours = pad(date.getHours());
    const minutes = pad(date.getMinutes());
    const seconds = pad(date.getSeconds());
    return `${year}-${month}-${day} ${hours}:${minutes}:${seconds}`;
  };
  
  try {
    pool = await mssql.connect(config);
    logger.info('MSSQL connected');
    
    // Sorgu oluÅŸtur
    let query = dataset.source_query || `SELECT * FROM ${dataset.source_table}`;
    
    // â— KRITIK: Dataset oluÅŸturulurken LIMIT/TOP 10 ile test edilmiÅŸ olabilir!
    // Full Refresh'te TÃœM veriyi Ã§ekmek iÃ§in LIMIT/TOP'Ä± kaldÄ±r
    query = query.replace(/\s+LIMIT\s+\d+\s*/gi, ' ').trim();
    query = query.replace(/\bSELECT\s+TOP\s*\(?(\d+)\)?\s+/gi, 'SELECT ').trim();
    
    // Row limit varsa (kullanÄ±cÄ± isterse) yeniden ekle
    if (rowLimit && !query.toUpperCase().includes('TOP ') && !query.toUpperCase().includes('TOP(')) {
      query = query.replace(/^SELECT\s+/i, `SELECT TOP ${rowLimit} `);
    }
    
    logger.info('Executing MSSQL streaming query', { query: query.substring(0, 200) });
    
    // ClickHouse tablosunu truncate et (full_refresh iÃ§in)
    if (dataset.sync_strategy === 'full_refresh') {
      try {
        await clickhouse.execute(`TRUNCATE TABLE clixer_analytics.${dataset.clickhouse_table}`);
        logger.info('Truncated ClickHouse table');
      } catch (truncErr: any) {
        await clickhouse.execute(`ALTER TABLE clixer_analytics.${dataset.clickhouse_table} DELETE WHERE 1=1`);
      }
    }
    
    // STREAMING MODE
    return new Promise((resolve, reject) => {
      const request = pool.request();
      request.stream = true; // âš¡ STREAMING AÃ‡IK!
      
      let batch: any[] = [];
      let totalInserted = 0;
      let columnMappingInitialized = columnMapping.length > 0;
      const STREAM_BATCH_SIZE = 5000; // Her 5000 satÄ±rda bir yaz
      let lastError: string | null = null; // Hata takibi iÃ§in
      
      // Row event - her satÄ±r geldiÄŸinde
      request.on('row', async (row: any) => {
        // Ä°lk satÄ±rda column mapping oluÅŸtur
        if (!columnMappingInitialized) {
          for (const key of Object.keys(row)) {
            const value = row[key];
            let clickhouseType = 'String';
            if (typeof value === 'number') {
              clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
            } else if (value instanceof Date) {
              clickhouseType = 'DateTime';
            }
            columnMapping.push({ source: key, target: key, clickhouseType });
          }
          columnMappingInitialized = true;
          logger.info('Column mapping auto-generated', { columns: columnMapping.length });
        }
        
        // Date objelerini ve DateTime/Date kolonlarÄ±nÄ± dÃ¼zgÃ¼n formata Ã§evir
        const processedRow: any = {};
        for (const key in row) {
          const colInfo = columnMapping.find((c: any) => c.source === key || c.target === key);
          let val = row[key];
          
          // DEBUG: Ä°lk satÄ±rda opening_date deÄŸerini logla
          if (key === 'opening_date' && batch.length === 0) {
            logger.info('DEBUG opening_date', { 
              key, 
              val, 
              valType: typeof val, 
              isDate: val instanceof Date,
              colInfo: colInfo ? { source: colInfo.source, target: colInfo.target, clickhouseType: colInfo.clickhouseType } : null
            });
          }
          
          if (val === null || val === undefined) {
            processedRow[key] = null;
          } else if (val instanceof Date) {
            // Date nesnesi - gÃ¼venli dÃ¶nÃ¼ÅŸÃ¼m
            const formatted = formatDate(val);
            if (formatted) {
              if (colInfo?.clickhouseType === 'Date') {
                processedRow[key] = formatted.split(' ')[0]; // Sadece tarih
              } else {
                processedRow[key] = formatted;
              }
            } else {
              // GeÃ§ersiz Date - varsayÄ±lan deÄŸer kullan
              processedRow[key] = colInfo?.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
            }
          } else if (colInfo?.clickhouseType === 'Date' || colInfo?.clickhouseType === 'DateTime') {
            // String olarak gelen ama Date/DateTime tipinde olmasÄ± gereken deÄŸer
            const converted = convertToClickHouseDateTime(val);
            if (converted) {
              if (colInfo.clickhouseType === 'Date') {
                processedRow[key] = converted.split(' ')[0]; // Sadece tarih
              } else {
                processedRow[key] = converted;
              }
            } else {
              // DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lemedi - varsayÄ±lan deÄŸer
              processedRow[key] = colInfo.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
            }
          } else {
            processedRow[key] = val;
          }
        }
        
        batch.push(processedRow);
        
        // Batch doldu - ClickHouse'a yaz
        if (batch.length >= STREAM_BATCH_SIZE) {
          request.pause(); // Streaming durdur
          
          try {
            const insertData = batch.map((r: any) => {
              const obj: any = {};
              for (const col of columnMapping) {
                // processedRow key olarak orijinal source adÄ±nÄ± kullanÄ±yor
                let val = r[col.source];
                
                // âš ï¸ KRÄ°TÄ°K: Date objesi ise Ã¶nce string'e Ã§evir!
                // ClickHouse client Date objelerini JSON'a dÃ¼zgÃ¼n serialize edemiyor
                if (val instanceof Date) {
                  const formatted = formatDate(val);
                  if (formatted) {
                    val = col.clickhouseType === 'Date' ? formatted.split(' ')[0] : formatted;
                  } else {
                    val = col.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
                  }
                }
                
                // Date/DateTime tiplerini dÃ¼zelt
                if (col.clickhouseType === 'DateTime' || col.clickhouseType === 'Date') {
                  if (val === null || val === undefined || val === '') {
                    val = col.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
                  } else if (typeof val === 'string') {
                    // Zaten string ise kontrol et
                    const converted = convertToClickHouseDateTime(val);
                    if (converted) {
                      val = col.clickhouseType === 'Date' ? converted.split(' ')[0] : converted;
                    } else {
                      val = col.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
                    }
                  }
                }
                
                // Null deÄŸerleri varsayÄ±lanlarla deÄŸiÅŸtir
                if (val === null || val === undefined) {
                  if (col.clickhouseType === 'String') val = '';
                  else if (col.clickhouseType?.includes('Int') || col.clickhouseType?.includes('Float')) val = 0;
                  else if (col.clickhouseType === 'Date') val = '1970-01-01';
                  else if (col.clickhouseType === 'DateTime') val = '1970-01-01 00:00:00';
                  else val = '';
                }
                obj[col.target] = val;
              }
              return obj;
            });
            
            await clickhouse.insert(`clixer_analytics.${dataset.clickhouse_table}`, insertData);
            
            totalInserted += batch.length;
            batch = []; // Batch'i temizle
            
            // Progress gÃ¼ncelle
            if (jobId) {
              await db.query(
                'UPDATE etl_jobs SET rows_processed = $1 WHERE id = $2',
                [totalInserted, jobId]
              );
            }
            
            logger.info(`Streaming batch inserted`, { totalInserted, rowLimit });
            
          } catch (insertError: any) {
            logger.error('Batch insert failed', { error: insertError.message });
            lastError = insertError.message;
          }
          
          request.resume(); // Streaming devam
        }
      });
      
      // Done event - tÃ¼m satÄ±rlar alÄ±ndÄ±
      request.on('done', async () => {
        // Kalan batch'i yaz
        if (batch.length > 0) {
          try {
            const insertData = batch.map((r: any) => {
              const obj: any = {};
              for (const col of columnMapping) {
                // processedRow key olarak orijinal source adÄ±nÄ± kullanÄ±yor
                let val = r[col.source];
                
                // âš ï¸ KRÄ°TÄ°K: Date objesi ise Ã¶nce string'e Ã§evir!
                if (val instanceof Date) {
                  const formatted = formatDate(val);
                  if (formatted) {
                    val = col.clickhouseType === 'Date' ? formatted.split(' ')[0] : formatted;
                  } else {
                    val = col.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
                  }
                }
                
                // Date/DateTime tiplerini dÃ¼zelt
                if (col.clickhouseType === 'DateTime' || col.clickhouseType === 'Date') {
                  if (val === null || val === undefined || val === '') {
                    val = col.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
                  } else if (typeof val === 'string') {
                    // Zaten string ise kontrol et
                    const converted = convertToClickHouseDateTime(val);
                    if (converted) {
                      val = col.clickhouseType === 'Date' ? converted.split(' ')[0] : converted;
                    } else {
                      val = col.clickhouseType === 'Date' ? '1970-01-01' : '1970-01-01 00:00:00';
                    }
                  }
                }
                
                // Null deÄŸerleri varsayÄ±lanlarla deÄŸiÅŸtir
                if (val === null || val === undefined) {
                  if (col.clickhouseType === 'String') val = '';
                  else if (col.clickhouseType?.includes('Int') || col.clickhouseType?.includes('Float')) val = 0;
                  else if (col.clickhouseType === 'Date') val = '1970-01-01';
                  else if (col.clickhouseType === 'DateTime') val = '1970-01-01 00:00:00';
                  else val = '';
                }
                obj[col.target] = val;
              }
              return obj;
            });
            
            await clickhouse.insert(`clixer_analytics.${dataset.clickhouse_table}`, insertData);
            totalInserted += batch.length;
            
            if (jobId) {
              await db.query(
                'UPDATE etl_jobs SET rows_processed = $1 WHERE id = $2',
                [totalInserted, jobId]
              );
            }
            
            logger.info(`Final batch inserted`, { batchSize: batch.length, totalInserted });
          } catch (insertError: any) {
            logger.error('Final batch insert failed', { error: insertError.message });
            lastError = insertError.message;
          }
        }
        
        // Hata veya 0 satÄ±r kontrolÃ¼
        if (lastError) {
          logger.error('âŒ MSSQL Streaming Sync failed', { totalInserted, error: lastError });
          if (pool) await pool.close();
          reject(new Error(lastError));
          return;
        }
        
        if (totalInserted === 0) {
          // 0 satÄ±r = GÃ¼ncellenecek yeni veri yok (incremental sync iÃ§in normal)
          logger.info('â„¹ï¸ MSSQL Streaming Sync: GÃ¼ncellenecek yeni veri bulunamadÄ± (0 satÄ±r)', {
            query: query.substring(0, 200)
          });
          if (pool) await pool.close();
          resolve(0); // BaÅŸarÄ±lÄ±, sadece yeni veri yok
          return;
        }
        
        // COUNT VERIFICATION
        try {
          const validation = await validateDataConsistency(dataset, null, totalInserted);
          logger.info('ğŸ“Š Count Verification', { 
            source: totalInserted, 
            clickhouse: validation.targetCount,
            isConsistent: validation.isConsistent,
            message: validation.message
          });
          
          // Verification sonucunu job'a kaydet (uyarÄ± olarak)
          if (!validation.isConsistent && jobId) {
            const verificationWarning = `Kaynak: ${totalInserted.toLocaleString()}, Hedef: ${validation.targetCount.toLocaleString()} - ${Math.abs(totalInserted - validation.targetCount).toLocaleString()} satÄ±r fark`;
            await db.query(
              'UPDATE etl_jobs SET error_message = $1 WHERE id = $2',
              [verificationWarning, jobId]
            );
          }
        } catch (verifyError: any) {
          logger.warn('Count verification failed', { error: verifyError.message });
        }
        
        logger.info('âœ… MSSQL Streaming Sync completed', { totalInserted });
        
        if (pool) {
          await pool.close();
        }
        
        resolve(totalInserted);
      });
      
      // Error event
      request.on('error', async (err: any) => {
        logger.error('MSSQL Stream error', { error: err.message });
        if (pool) {
          await pool.close();
        }
        reject(err);
      });
      
      // Sorguyu baÅŸlat
      request.query(query);
    });
    
  } catch (error: any) {
    logger.error('MSSQL Sync failed', { error: error.message, stack: error.stack });
    if (pool) {
      await pool.close();
    }
    throw error;
  }
}

// ============================================
// MYSQL SYNC
// ============================================
async function mysqlSync(
  dataset: any,
  connection: any,
  columnMapping: any[],
  jobId?: string
): Promise<number> {
  const mysql = require('mysql2/promise');
  
  logger.info('ğŸ”¶ MySQL Sync starting', {
    datasetId: dataset.id,
    table: dataset.clickhouse_table,
    host: connection.host
  });
  
  let conn: any = null;
  
  try {
    conn = await mysql.createConnection({
      host: connection.host,
      port: connection.port || 3306,
      database: connection.database_name,
      user: connection.username,
      password: connection.password_encrypted,
      charset: 'utf8mb4',
      dateStrings: true
    });
    
    // Sorgu oluÅŸtur
    let query = dataset.source_query || `SELECT * FROM ${dataset.source_table}`;
    
    // â— KRITIK: Dataset oluÅŸturulurken LIMIT 10 ile test edilmiÅŸ olabilir!
    // Full Refresh'te TÃœM veriyi Ã§ekmek iÃ§in LIMIT'i kaldÄ±r
    query = query.replace(/\s+LIMIT\s+\d+\s*/gi, ' ').trim();
    
    // Row limit varsa (kullanÄ±cÄ± isterse) yeniden ekle
    const rowLimit = dataset.row_limit;
    if (rowLimit && !query.toUpperCase().includes('LIMIT')) {
      query += ` LIMIT ${rowLimit}`;
    }
    
    logger.info('MySQL row limit', { rowLimit: rowLimit || 'UNLIMITED' });
    logger.info('Executing MySQL query', { query: query.substring(0, 200) });
    
    const [rows] = await conn.query(query);
    
    logger.info('Fetched rows from MySQL', { count: rows.length });
    
    if (rows.length === 0) {
      logger.warn('No data fetched from MySQL');
      return 0;
    }
    
    // Column mapping yoksa otomatik oluÅŸtur
    if (columnMapping.length === 0 && rows.length > 0) {
      const firstRow = rows[0];
      for (const key of Object.keys(firstRow)) {
        const value = firstRow[key];
        let clickhouseType = 'String';
        if (typeof value === 'number') {
          clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
        }
        columnMapping.push({
          source: key,
          target: key,
          clickhouseType
        });
      }
    }
    
    // ClickHouse tablosunu truncate et
    try {
      await clickhouse.execute(`TRUNCATE TABLE clixer_analytics.${dataset.clickhouse_table}`);
      logger.info('Truncated ClickHouse table');
    } catch (truncErr: any) {
      await clickhouse.execute(`ALTER TABLE clixer_analytics.${dataset.clickhouse_table} DELETE WHERE 1=1`);
    }
    
    // Batch insert
    let totalInserted = 0;
    
    for (let i = 0; i < rows.length; i += BATCH_SIZE) {
      const batch = rows.slice(i, i + BATCH_SIZE);
      
      // Her satÄ±rÄ± obje formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
      const insertData = batch.map((row: any) => {
        const obj: any = {};
        for (const col of columnMapping) {
          let val = row[col.source];
          
          // DateTime dÃ¶nÃ¼ÅŸÃ¼mÃ¼
          if (col.clickhouseType === 'DateTime' || col.clickhouseType === 'Date') {
            val = toClickHouseDateTime(val) || '1970-01-01 00:00:00';
          }
          
          // Null handling
          if (val === null || val === undefined) {
            if (col.clickhouseType === 'String') val = '';
            else if (col.clickhouseType?.includes('Int') || col.clickhouseType?.includes('Float')) val = 0;
            else val = '';
          }
          
          obj[col.target] = val;
        }
        return obj;
      });
      
      await clickhouse.insert(
        `clixer_analytics.${dataset.clickhouse_table}`,
        insertData
      );
      
      totalInserted += batch.length;
      logger.info(`Inserted batch ${Math.floor(i/BATCH_SIZE) + 1}`, { 
        batchSize: batch.length, 
        totalInserted 
      });
    }
    
    // ğŸ”§ OPTIMIZE FINAL - Duplicate'larÄ± fiziksel olarak sil
    // ReplacingMergeTree iÃ§in kritik!
    try {
      await clickhouse.execute(`OPTIMIZE TABLE clixer_analytics.${dataset.clickhouse_table} FINAL`);
      logger.info('OPTIMIZE FINAL executed');
    } catch (optErr: any) {
      logger.warn('OPTIMIZE FINAL failed (will retry in background)', { error: optErr.message });
    }
    
    logger.info('âœ… MySQL Sync completed', { totalInserted });
    return totalInserted;
    
  } catch (error: any) {
    logger.error('MySQL Sync failed', { error: error.message, stack: error.stack });
    throw error;
  } finally {
    if (conn) {
      await conn.end();
    }
  }
}

async function fullRefresh(dataset: any, connection: any, jobId?: string): Promise<number> {
  logger.info('Full refresh starting', { datasetId: dataset.id, table: dataset.clickhouse_table, jobId });
  
  try {
    // ğŸ”§ SELF-HEALING: Tablo yoksa otomatik oluÅŸtur
    await ensureTableExists(dataset);
    
    const columnMapping = dataset.column_mapping || [];
    
    // PostgreSQL iÃ§in STREAMING destekli ETL (200M+ satÄ±r destekler)
    if (connection.type === 'postgresql') {
      return await streamingPostgreSQLSync(dataset, connection, columnMapping, jobId);
    }
    
    // MSSQL iÃ§in (Azure SQL dahil)
    if (connection.type === 'mssql') {
      return await mssqlSync(dataset, connection, columnMapping, jobId);
    }
    
    // MySQL iÃ§in
    if (connection.type === 'mysql') {
      return await mysqlSync(dataset, connection, columnMapping, jobId);
    }
    
    // API ve diÄŸer kaynaklar iÃ§in (belleÄŸe alarak iÅŸle)
    let rows: any[] = [];
    
    if (connection.type === 'api') {
      // API baÄŸlantÄ±sÄ± iÃ§in fetch yap
      logger.info('Fetching from API', { host: connection.host });
      
      // source_query JSON formatÄ±nda endpoint bilgisi iÃ§eriyor olabilir
      let apiConfig: any = connection.api_config || {};
      
      // source_query JSON ise parse et
      if (dataset.source_query && typeof dataset.source_query === 'string') {
        try {
          const queryConfig = JSON.parse(dataset.source_query);
          apiConfig = { ...apiConfig, ...queryConfig };
        } catch (e) {
          // JSON deÄŸilse endpoint olarak kullan
          apiConfig.endpoint = dataset.source_query;
        }
      }
      
      // URL oluÅŸtur - Trim ile boÅŸluklarÄ± temizle
      let url = connection.host.trim().replace(/\/$/, '');
      
      // HTTP -> HTTPS otomatik dÃ¶nÃ¼ÅŸÃ¼m (301 redirect'i Ã¶nlemek iÃ§in)
      if (url.startsWith('http://')) {
        const httpsUrl = url.replace('http://', 'https://');
        logger.info('Trying HTTPS first', { httpsUrl: httpsUrl.substring(0, 100) });
        
        try {
          const testRes = await fetch(httpsUrl, { method: 'HEAD' });
          if (testRes.ok || testRes.status < 400) {
            url = httpsUrl;
            logger.info('Using HTTPS', { url: url.substring(0, 100) });
          }
        } catch (e) {
          logger.debug('HTTPS not available, using HTTP', { url });
        }
      }
      
      if (apiConfig.endpoint) {
        url += '/' + apiConfig.endpoint.replace(/^\//, '');
      }
      if (apiConfig.queryParams) {
        url += (url.includes('?') ? '&' : '?') + apiConfig.queryParams;
      }
      
      logger.info('API URL', { url: url.substring(0, 100), method: apiConfig.method || 'GET' });
      
      // Headers oluÅŸtur
      const fetchHeaders: Record<string, string> = {
        'Content-Type': 'application/json',
        'Accept': 'application/json',
        ...(apiConfig.headers || {})
      };
      
      // API Key varsa custom header'a ekle (connection.api_config'den)
      const connApiConfig = typeof connection.api_config === 'string' 
        ? JSON.parse(connection.api_config) 
        : (connection.api_config || {});
      
      if (connApiConfig.apiKey) {
        const headerName = connApiConfig.headerName || 'Authorization';
        if (headerName === 'Authorization') {
          fetchHeaders['Authorization'] = `Bearer ${connApiConfig.apiKey}`;
        } else {
          fetchHeaders[headerName] = connApiConfig.apiKey;
        }
        logger.info('API Key header set', { headerName });
      }
      
      // Fetch options
      const fetchOptions: RequestInit = {
        method: apiConfig.method || 'GET',
        headers: fetchHeaders,
        redirect: 'follow'
      };
      
      // POST/PUT iÃ§in body ekle
      if ((apiConfig.method === 'POST' || apiConfig.method === 'PUT') && apiConfig.requestBody) {
        try {
          fetchOptions.body = typeof apiConfig.requestBody === 'string' 
            ? apiConfig.requestBody 
            : JSON.stringify(apiConfig.requestBody);
          logger.info('Request body set', { bodyLength: fetchOptions.body?.length });
        } catch (e) {
          logger.warn('Could not set request body', { error: (e as Error).message });
        }
      }
      
      const response = await fetch(url, fetchOptions);
      
      if (!response.ok) {
        throw new Error(`API error: ${response.status} ${response.statusText}`);
      }
      
      let data: any = await response.json();
      
      // responsePath ile nested data Ã§Ä±kar
      if (apiConfig.responsePath) {
        const paths = apiConfig.responsePath.split('.');
        for (const p of paths) {
          if (data && (data as any)[p] !== undefined) {
            data = (data as any)[p];
          }
        }
      }
      
      rows = Array.isArray(data) ? data : [data];
      logger.info('Fetched rows from API', { count: rows.length });
    }
    
    if (rows.length === 0) {
      logger.warn('No data fetched from source');
      return 0;
    }
    
    // Column mapping yoksa otomatik oluÅŸtur (API verileri iÃ§in)
    if (columnMapping.length === 0 && rows.length > 0) {
      logger.info('No column mapping, creating auto mapping from first row');
      const firstRow = rows[0];
      for (const key of Object.keys(firstRow)) {
        const value = firstRow[key];
        let clickhouseType = 'String';
        if (typeof value === 'number') {
          clickhouseType = Number.isInteger(value) ? 'Int64' : 'Float64';
        } else if (typeof value === 'boolean') {
          clickhouseType = 'UInt8';
        }
        columnMapping.push({
          source: key,
          target: key.replace(/[^a-zA-Z0-9_]/g, '_'),
          clickhouseType
        });
      }
    }
    
    // 2. ClickHouse tablosunu truncate et
    try {
      await clickhouse.execute(`TRUNCATE TABLE clixer_analytics.${dataset.clickhouse_table}`);
      logger.info('Truncated ClickHouse table', { table: dataset.clickhouse_table });
    } catch (truncErr: any) {
      logger.warn('Could not truncate, trying ALTER DELETE', { error: truncErr.message });
      await clickhouse.execute(`ALTER TABLE clixer_analytics.${dataset.clickhouse_table} DELETE WHERE 1=1`);
    }
    
    // 3. Column mapping'e gÃ¶re veriyi dÃ¶nÃ¼ÅŸtÃ¼r
    const transformedRows = rows.map(row => {
      const transformed: any = {};
      for (const mapping of columnMapping) {
        const sourceCol = mapping.source || mapping.sourceName;
        const targetCol = mapping.target || mapping.targetName;
        let value = row[sourceCol];
        const chType = mapping.clickhouseType || 'String';
        
        // Tip dÃ¶nÃ¼ÅŸÃ¼mÃ¼ - boÅŸ string de null gibi iÅŸlenir
        const isNumericType = chType.includes('Int') || chType.includes('Float') || chType.includes('Decimal');
        
        if (value === null || value === undefined || (value === '' && isNumericType)) {
          // ClickHouse tiplerine gÃ¶re varsayÄ±lan deÄŸer
          if (isNumericType) {
            value = 0;
          } else if (chType === 'Date') {
            value = '1970-01-01';
          } else if (chType === 'DateTime') {
            value = '1970-01-01 00:00:00';
          } else {
            value = '';
          }
        } else if (isNumericType && typeof value === 'string') {
          // String ama sayÄ±sal tip - sayÄ±ya dÃ¶nÃ¼ÅŸtÃ¼r
          const numVal = parseFloat(value);
          value = isNaN(numVal) ? 0 : numVal;
        }
        
        transformed[targetCol] = value;
      }
      return transformed;
    });
    
    // 4. ClickHouse'a insert et (batch olarak - MEMORY OPTÄ°MÄ°ZE)
    // BATCH_SIZE global sabiti kullanÄ±lÄ±r (5000 row = ~50MB RAM)
    let insertedCount = 0;
    let batchNumber = 0;
    const totalBatches = Math.ceil(transformedRows.length / BATCH_SIZE);
    const columns = columnMapping.map((m: any) => m.target || m.targetName);
    
    logger.info('Starting batch insert', { 
      totalRows: transformedRows.length, 
      batchSize: BATCH_SIZE, 
      totalBatches 
    });
    
    for (let i = 0; i < transformedRows.length; i += BATCH_SIZE) {
      batchNumber++;
      
      // âš ï¸ KILL CHECK: Job iptal edilmiÅŸ mi?
      if (jobId && await isJobCancelled(jobId)) {
        logger.info('Job cancelled during batch processing', { 
          jobId, 
          batchNumber, 
          processedSoFar: insertedCount 
        });
        return insertedCount; // Mevcut durumu dÃ¶ndÃ¼r
      }
      
      // Memory kontrolÃ¼ - limit aÅŸÄ±lÄ±rsa GC tetikle
      const memCheck = checkMemory();
      if (!memCheck.ok) {
        logger.warn('Memory pressure, forcing GC', { usedMB: memCheck.usedMB });
        forceGC();
        // KÄ±sa bir bekleme
        await new Promise(r => setTimeout(r, 100));
      }
      
      // Batch'i iÅŸle
      const batch = transformedRows.slice(i, i + BATCH_SIZE);
      
      // VALUES oluÅŸtur - AkÄ±llÄ± tarih dÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ ile
      const values = batch.map(row => {
        const vals = columns.map((col: string) => {
          const val = row[col];
          if (val === null || val === undefined) {
            return 'NULL';
          }
          
          // Tarih/DateTime kontrolÃ¼ - akÄ±llÄ± dÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼
          if (val instanceof Date || (typeof val === 'string' && val.match(/\d{4}[-\/]\d{2}[-\/]\d{2}|\d{2}[-\/]\d{2}[-\/]\d{4}/))) {
            const converted = toClickHouseDateTime(val);
            if (converted) return `'${converted}'`;
            return 'NULL';
          }
          
          if (typeof val === 'string') {
            // String escape - SQL injection korumasÄ±
            return `'${val.replace(/\\/g, '\\\\').replace(/'/g, "''")}'`;
          }
          return val;
        });
        return `(${vals.join(', ')})`;
      }).join(',\n');
      
      const insertSql = `INSERT INTO clixer_analytics.${dataset.clickhouse_table} (${columns.join(', ')}) VALUES ${values}`;
      
      try {
        await clickhouse.execute(insertSql);
        insertedCount += batch.length;
        
        // Progress log (her 10 batch'te bir)
        if (batchNumber % 10 === 0 || batchNumber === totalBatches) {
          const progress = Math.round((batchNumber / totalBatches) * 100);
          const memUsed = Math.round(process.memoryUsage().heapUsed / 1024 / 1024);
          logger.info('Batch progress', { 
            batch: `${batchNumber}/${totalBatches}`,
            progress: `${progress}%`,
            inserted: insertedCount,
            memoryMB: memUsed
          });
        }
        
        // Her GC_INTERVAL batch'te bir GC tetikle (memory basÄ±ncÄ±nÄ± azalt)
        if (batchNumber % (GC_INTERVAL / BATCH_SIZE) === 0) {
          forceGC();
        }
        
      } catch (insertErr: any) {
        logger.error('Insert error', { 
          error: insertErr.message, 
          batch: batchNumber,
          rowRange: `${i}-${i + batch.length}`
        });
        // Devam et, diÄŸer batch'leri dene
      }
    }
    
    // Son GC
    forceGC();
    
    // ============================================
    // VERÄ° TUTARLILIK KONTROLÃœ VE DUPLICATE TEMÄ°ZLÄ°ÄÄ°
    // ============================================
    const validation = await validateDataConsistency(dataset, null, transformedRows.length);
    
    if (!validation.isConsistent) {
      logger.warn('Data consistency warning', {
        datasetId: dataset.id,
        expected: transformedRows.length,
        actual: validation.targetCount,
        duplicatesRemoved: validation.duplicateCount
      });
    }
    
    const finalMem = Math.round(process.memoryUsage().heapUsed / 1024 / 1024);
    logger.info('Full refresh completed', { 
      datasetId: dataset.id, 
      rowsInserted: insertedCount,
      finalRowCount: validation.targetCount,
      duplicatesRemoved: validation.duplicateCount,
      isConsistent: validation.isConsistent,
      finalMemoryMB: finalMem
    });
    
    return validation.targetCount; // OPTIMIZE sonrasÄ± gerÃ§ek sayÄ±
    
  } catch (error: any) {
    logger.error('Full refresh error', { datasetId: dataset.id, error: error.message });
    throw error;
  }
}

// ============================================
// PENDING JOBS PROCESSOR
// ============================================

/**
 * VeritabanÄ±ndaki pending job'larÄ± iÅŸle
 * Bu, ETL baÅŸlat butonuna basÄ±ldÄ±ÄŸÄ±nda oluÅŸturulan job'larÄ± iÅŸler
 * 
 * âš ï¸ KRÄ°TÄ°K KURAL: AynÄ± dataset iÃ§in aynÄ± anda sadece bir job Ã§alÄ±ÅŸabilir!
 * Bu kural duplicate job'larÄ± Ã¶nler ve birbirini bloklamayÄ± engeller.
 */
async function processPendingJobs(): Promise<void> {
  try {
    // Pending durumundaki job'larÄ± al (en eski Ã¶nce)
    const pendingJobs = await db.queryAll(`
      SELECT 
        e.id as job_id, 
        e.dataset_id, 
        e.action, 
        e.tenant_id,
        d.name as dataset_name,
        d.clickhouse_table
      FROM etl_jobs e
      JOIN datasets d ON e.dataset_id = d.id
      WHERE e.status = 'pending'
      ORDER BY e.started_at ASC
      LIMIT 10
    `);

    if (pendingJobs.length === 0) {
      return; // Bekleyen job yok
    }

    logger.info('Processing pending jobs', { count: pendingJobs.length });

    for (const job of pendingJobs) {
      // âš ï¸ DUPLICATE PREVENTION: Dataset lock kontrolÃ¼
      const lockAcquired = await acquireDatasetLock(job.dataset_id);
      
      if (!lockAcquired) {
        // Bu dataset iÃ§in zaten Ã§alÄ±ÅŸan bir job var
        logger.warn('Skipping job - dataset already has running job (duplicate prevention)', {
          jobId: job.job_id,
          datasetId: job.dataset_id,
          dataset: job.dataset_name
        });
        
        // Job'Ä± skip olarak iÅŸaretle (tekrar denenmemesi iÃ§in)
        await db.query(
          `UPDATE etl_jobs SET status = 'skipped', completed_at = NOW(), 
           error_message = 'AynÄ± dataset iÃ§in zaten Ã§alÄ±ÅŸan bir job var (duplicate prevention)' 
           WHERE id = $1`,
          [job.job_id]
        );
        continue;
      }
      
      try {
        // Job'Ä± running olarak iÅŸaretle
        await db.query(
          `UPDATE etl_jobs SET status = 'running', started_at = NOW() WHERE id = $1`,
          [job.job_id]
        );
        
        // Aktif job bilgisini Redis'e kaydet (kill iÃ§in)
        await cache.set(`etl:active:${job.job_id}`, JSON.stringify({
          datasetId: job.dataset_id,
          datasetName: job.dataset_name,
          startedAt: new Date().toISOString(),
          pid: process.pid
        }), LOCK_TTL);

        logger.info('Starting pending job', { 
          jobId: job.job_id, 
          dataset: job.dataset_name, 
          action: job.action 
        });

        // Dataset bilgisini al
        const dataset = await db.queryOne(
          'SELECT * FROM datasets WHERE id = $1',
          [job.dataset_id]
        );

        if (!dataset) {
          throw new Error(`Dataset not found: ${job.dataset_id}`);
        }

        // Connection bilgisini al
        const connection = await db.queryOne(
          'SELECT * FROM data_connections WHERE id = $1',
          [dataset.connection_id]
        );

        if (!connection) {
          throw new Error(`Connection not found: ${dataset.connection_id}`);
        }

        const startTime = Date.now();
        let rowsProcessed = 0;

        // âœ… Ã–nce Ã–ZEL ACTION'larÄ± kontrol et (new_records_sync, missing_sync, partial_refresh)
        if (job.action === 'new_records_sync') {
          // ğŸš€ Sadece yeni kayÄ±tlarÄ± Ã§ek (max ID'den sonrasÄ±)
          logger.info('ğŸš€ NEW RECORDS SYNC starting from pending job', { jobId: job.job_id });
          
          // ClickHouse'daki max ID'yi al
          const pkColumn = dataset.reference_column || 'id';
          const chStats = await clickhouse.query(`SELECT max(${pkColumn}) as max_id FROM clixer_analytics.${dataset.clickhouse_table}`);
          const chMaxId = Number(chStats[0]?.max_id) || 0;
          
          rowsProcessed = await syncNewRecordsAfterMaxId(dataset, connection, job.job_id, pkColumn, chMaxId);
        } 
        else if (job.action === 'partial_refresh') {
          // KÄ±smi yenileme (son X gÃ¼n)
          const days = dataset.delete_days || 7;
          rowsProcessed = await syncByDateDeleteInsert(dataset, connection, job.job_id);
        }
        else {
          // Normal sync - stratejisine gÃ¶re veri Ã§ek
          switch (dataset.sync_strategy) {
            case 'timestamp':
              rowsProcessed = await syncByTimestamp(dataset, connection, job.job_id);
              break;
            case 'id':
              rowsProcessed = await syncById(dataset, connection, job.job_id);
              break;
            case 'date_partition':
              rowsProcessed = await syncByDatePartition(dataset, connection, job.job_id);
              break;
            case 'date_delete_insert':
              rowsProcessed = await syncByDateDeleteInsert(dataset, connection, job.job_id);
              break;
            case 'full_refresh':
            default:
              rowsProcessed = await fullRefresh(dataset, connection, job.job_id);
          }
        }
        
        // Kill check - job iptal edilmiÅŸ olabilir
        if (await isJobCancelled(job.job_id)) {
          logger.info('Job was cancelled', { jobId: job.job_id });
          await db.query(
            `UPDATE etl_jobs SET status = 'cancelled', completed_at = NOW() WHERE id = $1`,
            [job.job_id]
          );
        } else {
          // Job baÅŸarÄ±lÄ±
          await db.query(
            `UPDATE etl_jobs SET status = 'completed', completed_at = NOW(), rows_processed = $1 WHERE id = $2`,
            [rowsProcessed, job.job_id]
          );

          // Dataset gÃ¼ncelle
          let totalRows = rowsProcessed;
          try {
            if (dataset.clickhouse_table) {
              const countResult = await clickhouse.query(`SELECT count() as cnt FROM clixer_analytics.${dataset.clickhouse_table}`);
              totalRows = countResult[0]?.cnt || rowsProcessed;
            }
          } catch (e) {
            logger.warn('Could not get total rows from ClickHouse', { error: e });
          }
          
          await db.query(
            `UPDATE datasets SET last_sync_at = NOW(), last_sync_rows = $1, total_rows = $2, status = 'active' WHERE id = $3`,
            [rowsProcessed, totalRows, job.dataset_id]
          );

          // Cache invalidate
          await cache.invalidate(`kpi:${dataset.clickhouse_table}:*`, 'etl');

          logger.info('Pending job completed', {
            jobId: job.job_id,
            dataset: job.dataset_name,
            rowsProcessed,
            duration: `${Date.now() - startTime}ms`
          });
        }

      } catch (jobError: any) {
        // Job baÅŸarÄ±sÄ±z
        logger.error('Pending job failed', { 
          jobId: job.job_id, 
          dataset: job.dataset_name, 
          error: jobError.message 
        });

        await db.query(
          `UPDATE etl_jobs SET status = 'failed', completed_at = NOW(), error_message = $1 WHERE id = $2`,
          [jobError.message, job.job_id]
        );
      } finally {
        // âš ï¸ Ã–NEMLÄ°: Lock'Ä± her zaman serbest bÄ±rak!
        await releaseDatasetLock(job.dataset_id);
        // Aktif job kaydÄ±nÄ± temizle
        await cache.del(`etl:active:${job.job_id}`);
      }
    }
  } catch (error) {
    logger.error('Pending jobs processor error', { error });
  }
}

// ============================================
// SCHEDULER
// ============================================

async function checkScheduledJobs(): Promise<void> {
  try {
    // etl_schedules tablosundan zamanÄ± gelen schedule'larÄ± bul
    const schedules = await db.queryAll(
      `SELECT s.*, d.name as dataset_name, d.clickhouse_table
       FROM etl_schedules s
       JOIN datasets d ON s.dataset_id = d.id
       WHERE s.is_active = true
       AND d.status = 'active'
       AND (s.next_run_at IS NULL OR s.next_run_at <= NOW())`
    );

    logger.info('Checking scheduled jobs', { count: schedules.length });

    for (const schedule of schedules) {
      logger.info('Running scheduled sync', { dataset: schedule.dataset_name, cron: schedule.cron_expression });
      
      try {
        await processETLJob({
          datasetId: schedule.dataset_id,
          action: 'incremental_sync'
        });

        // Bir sonraki Ã§alÄ±ÅŸma zamanÄ±nÄ± hesapla (basit: cron'a gÃ¶re)
        let nextRun = new Date();
        const cron = schedule.cron_expression;
        
        // Basit cron parser
        if (cron === '* * * * *') nextRun.setMinutes(nextRun.getMinutes() + 1);
        else if (cron === '*/5 * * * *') nextRun.setMinutes(nextRun.getMinutes() + 5);
        else if (cron === '*/15 * * * *') nextRun.setMinutes(nextRun.getMinutes() + 15);
        else if (cron === '*/30 * * * *') nextRun.setMinutes(nextRun.getMinutes() + 30);
        else if (cron === '0 * * * *') nextRun.setHours(nextRun.getHours() + 1);
        else if (cron === '0 */6 * * *') nextRun.setHours(nextRun.getHours() + 6);
        else if (cron === '0 */12 * * *') nextRun.setHours(nextRun.getHours() + 12);
        else if (cron === '0 0 * * *') nextRun.setDate(nextRun.getDate() + 1);
        else nextRun.setMinutes(nextRun.getMinutes() + 1); // default: 1 dakika

        // Schedule'Ä± gÃ¼ncelle
        await db.query(
          `UPDATE etl_schedules SET next_run_at = $1, last_run_at = NOW() WHERE id = $2`,
          [nextRun, schedule.id]
        );
      } catch (jobError: any) {
        logger.error('Scheduled job failed', { dataset: schedule.dataset_name, error: jobError.message });
      }
    }
  } catch (error) {
    logger.error('Scheduler error', { error });
  }
}

// ============================================
// START WORKER
// ============================================

async function start() {
  try {
    db.createPool();
    await db.checkHealth();
    logger.info('PostgreSQL connected');

    clickhouse.createClickHouseClient();
    await clickhouse.checkHealth();
    logger.info('ClickHouse connected');

    cache.createRedisClient();
    await cache.checkHealth();
    logger.info('Redis connected');

    // ETL trigger event'lerini dinle
    cache.subscribe('etl:trigger', async (message: ETLJob) => {
      try {
        // Active jobs sayÄ±sÄ±nÄ± artÄ±r
        await cache.set('etl:worker:active_jobs', '1', 3600);
        await processETLJob(message);
        await cache.set('etl:worker:active_jobs', '0', 3600);
      } catch (error) {
        logger.error('ETL job processing failed', { message, error });
        await cache.set('etl:worker:active_jobs', '0', 3600);
      }
    });

    // Ä°lk baÅŸlangÄ±Ã§ta bekleyen job'larÄ± kontrol et
    logger.info('Checking for pending jobs on startup...');
    await processPendingJobs();

    // Pending job'larÄ± her 5 saniyede bir kontrol et
    setInterval(async () => {
      try {
        await processPendingJobs();
      } catch (e) {
        logger.error('Pending jobs check failed', { error: e });
      }
    }, 5 * 1000);

    // Scheduler - her dakika kontrol et
    setInterval(checkScheduledJobs, 60 * 1000);

    // Heartbeat - her 10 saniye
    const sendHeartbeat = async () => {
      try {
        await cache.set('etl:worker:heartbeat', Date.now().toString(), 60);
        await cache.set('etl:worker:status', JSON.stringify({
          startedAt: new Date().toISOString(),
          pid: process.pid,
          uptime: process.uptime()
        }), 60);
      } catch (e) {
        logger.error('Heartbeat failed', { error: e });
      }
    };
    
    // Ä°lk heartbeat
    await sendHeartbeat();
    // Periyodik heartbeat
    setInterval(sendHeartbeat, 10 * 1000);

    logger.info('âš™ï¸ ETL Worker started');

  } catch (error) {
    logger.error('Failed to start etl-worker', { error });
    process.exit(1);
  }
}

start();
